@inproceedings{7320414,
abstract = {App stores like Google Play and Apple AppStore have over 3 Million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. This paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and ratings. For this we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. We conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. We found that metadata alone results in a poor classification accuracy. When combined with natural language processing, the classification precision got between 70-95{\%} while the recall between 80-90{\%}. Multiple binary classifiers outperformed single multiclass classifiers. Our results impact the design of review analytics tools which help app vendors, developers, and users to deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders.},
author = {Maalej, Walid and Nabil, Hadeer},
booktitle = {2015 IEEE 23rd International Requirements Engineering Conference, RE 2015 - Proceedings},
doi = {10.1109/RE.2015.7320414},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/maalej Bug report, feature request, or simply praise On automatically classifying app reviews.pdf:pdf},
isbn = {9781467369053},
issn = {2332-6441},
keywords = {data mining,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,pattern cl,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {116--125},
title = {{Bug report, feature request, or simply praise? On automatically classifying app reviews}},
year = {2015}
}
@conference{Blohm2019442,
abstract = {Natural language processing in combination with advances in artificial intelligence is on the rise. However, compliance constraints while handling personal data in many types of documents hinder various application scenarios. We describe the challenges of working with personal and particularly sensitive data in practice with three different use cases. We present the anonymization bootstrap challenge in creating a prototype in a cloud environment. Finally, we outline an architecture for privacy compliant AI cloud applications and an anonymization tool. With these preliminary results, we describe future work in bridging privacy and AI.},
annote = {cited By 1},
author = {Blohm, Matthias and Dukino, Claudia and Kintz, Maximilien and Kochanowski, Monika and Koetter, Falko and Renner, Thomas},
booktitle = {ICEIS 2019 - Proceedings of the 21st International Conference on Enterprise Information Systems},
doi = {10.5220/0007746204540461},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ICEIS{\_}2019{\_}148.pdf:pdf},
isbn = {9789897583728},
keywords = {Anonymization,Artificial intelligence,Cloud platform,Compliance,GDPR,Natural language processing,scopus{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {442--449},
title = {{Towards a privacy compliant cloud architecture for natural language processing platforms}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067424797{\&}partnerID=40{\&}md5=bcd9edf4ed37ac95089ecae21073a1c2},
volume = {1},
year = {2019}
}
@incollection{Schneider2018,
abstract = {Finding suitable ways to handle personal data in conformance with the law is challenging. The European General Data Protection Regulation (GDPR), enforced since May 2018, makes it mandatory to citizens and companies to comply with the privacy requirements set in the regulation. For existing systems the challenge is to be able to show evidence that they are already complying with the GDPR, or otherwise to work towards compliance by modifying their systems and procedures, or alternatively reprogramming their systems in order to pass the eventual controls. For those starting new projects the advice is to take privacy into consideration since the very beginning, already at design time. This has been known as Privacy by Design (PbD). The main question is how much privacy can you effectively achieve by using PbD, and in particular whether it is possible to achieve Privacy by Construction. In this paper I give my personal opinion on issues related to the ambition of achieving Privacy by Construction.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Schneider, Gerardo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-03418-4_28},
editor = {Margaria, Tiziana and Steffen, Bernhard},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/schneider Is privacy by construction possible:},
isbn = {9783030034177},
issn = {16113349},
keywords = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {471--485},
publisher = {Springer International Publishing},
title = {{Is privacy by construction possible?}},
url = {http://link.springer.com/10.1007/978-3-030-03418-4{\_}28},
volume = {11244 LNCS},
year = {2018}
}
@incollection{biagioni_automated_2019,
abstract = {Along with the popularity of mobile devices, people share a growing amount of personal data to a variety of mobile applications for personalized services. In most cases, users can learn their data usage from the privacy policy along with the application. However, current privacy policies are always too long and obscure to provide readability and comprehensibility to users. To address this issue, we propose an automated privacy policy extraction system considering users' personal privacy concerns under different contexts. The system is implemented on Android smartphones and evaluated feedbacks from a group of users ({\&}{\#}x0024;{\&}{\#}x0024;n=96{\&}{\#}x0024;{\&}{\#}x0024;) as a field study. Experiments are conducted on both our dataset, which is the first user privacy concern profile dataset to the best of our knowledge, and a public dataset containing 115 privacy policies with 23K data practices. We achieve 0.94 precision for privacy category classification and 0.81 accuracy for policy segment extraction, which attests to the significance of our work as a direction towards meeting the transparency requirement of the General Data Protection Regulation (GDPR).},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Chang, Cheng and Li, Huaxin and Zhang, Yichi and Du, Suguo and Cao, Hui and Zhu, Haojin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-23597-0_4},
editor = {Biagioni, Edoardo S and Zheng, Yao and Cheng, Siyao},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Chang, Cheng and Li, Huaxin and Zhang, Yichi and Du, Suguo and Cao, Hui and Zhu, Haojin{\_} {\_}{\_}Automated and Personalized Privacy Policy Extraction Under GDPR Consideration{\_}{\_} (2019).pdf:pdf},
isbn = {9783030235963},
issn = {16113349},
keywords = {GDPR,Mobile application privacy,Privacy policy extraction,springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {43--54},
publisher = {Springer International Publishing},
title = {{Automated and Personalized Privacy Policy Extraction Under GDPR Consideration}},
url = {http://link.springer.com/10.1007/978-3-030-23597-0{\_}4},
volume = {11604 LNCS},
year = {2019}
}
@article{salnitri_modelling_2020,
abstract = {Personal data have become a central asset for multiple enterprise applications and online services offered by private companies, public organisations or a combination of both. The sensitivity of such data and the continuously growing legislation that accompanies their management dictate the development of methods that allow the development of more secure, trustworthy software systems with focus on privacy protection. The contribution of this paper is the definition of a novel requirements engineering method that supports both early and late requirements specification, giving emphasis on security, privacy and trust. The novelty of our work is that it provides the means for software designers and security experts to analyse the system-to-be from multiple aspects, starting from identifying high-level goals to the definition of business process composition, and elicitation of mechanisms to fortify the system from external threats. The method is supported by two CASE tools. To demonstrate the applicability and usefulness of our work, the paper shows its applications to a real-world case study.},
author = {Salnitri, Mattia and Angelopoulos, Konstantinos and Pavlidis, Michalis and Diamantopoulou, Vasiliki and Mouratidis, Haralambos and Giorgini, Paolo},
doi = {10.1007/s10270-019-00744-x},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/SALNIT{\~{}}1.PDF:PDF},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {CASE tools,Privacy,Security,Sociotechnical systems,Trust,springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
number = {2},
pages = {467--491},
shorttitle = {Modelling the interplay of security, privacy and t},
title = {{Modelling the interplay of security, privacy and trust in sociotechnical systems: a computer-aided design approach}},
url = {http://link.springer.com/10.1007/s10270-019-00744-x},
volume = {19},
year = {2020}
}
@inproceedings{song_auditing_2019,
abstract = {To help enforce data-protection regulations such as GDPR and detect unauthorized uses of personal data, we develop a new model auditing technique that helps users check if their data was used to train a machine learning model. We focus on auditing deep-learning models that generate natural-language text, including word prediction and dialog generation. These models are at the core of popular online services and are often trained on personal data such as users' messages, searches, chats, and comments. We design and evaluate a black-box auditing method that can detect, with very few queries to a model, if a particular user's texts were used to train it (among thousands of other users). We empirically show that our method can successfully audit well-generalized models that are not overfitted to the training data. We also analyze how text-generation models memorize word sequences and explain why this memorization makes them amenable to auditing.},
address = {New York, NY, USA},
annote = {event-place: Anchorage, AK, USA},
archivePrefix = {arXiv},
arxivId = {1811.00513},
author = {Song, Congzheng and Shmatikov, Vitaly},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330885},
eprint = {1811.00513},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Song, Congzheng and Shmatikov, Vitaly{\_} {\_}{\_}Auditing data provenance in text-generation models{\_}{\_} (2019).pdf:pdf},
isbn = {9781450362016},
keywords = {Auditing,Machine learning,Membership inference,Text generation,acm{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {acm{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {196--206},
publisher = {Association for Computing Machinery},
series = {{\{}KDD{\}} '19},
title = {{Auditing data provenance in text-generation models}},
url = {https://doi.org/10.1145/3292500.3330885},
year = {2019}
}
@inproceedings{6912257,
abstract = {App stores allow users to submit feedback for downloaded apps in form of star ratings and text reviews. Recent studies analyzed this feedback and found that it includes information useful for app developers, such as user requirements, ideas for improvements, user sentiments about specific features, and descriptions of experiences with these features. However, for many apps, the amount of reviews is too large to be processed manually and their quality varies largely. The star ratings are given to the whole app and developers do not have a mean to analyze the feedback for the single features. In this paper we propose an automated approach that helps developers filter, aggregate, and analyze user reviews. We use natural language processing techniques to identify fine-grained app features in the reviews. We then extract the user sentiments about the identified features and give them a general score across all reviews. Finally, we use topic modeling techniques to group fine-grained features into more meaningful high-level features. We evaluated our approach with 7 apps from the Apple App Store and Google Play Store and compared its results with a manually, peer-conducted analysis of the reviews. On average, our approach has a precision of 0.59 and a recall of 0.51. The extracted features were coherent and relevant to requirements evolution tasks. Our approach can help app developers to systematically analyze user opinions about single features and filter irrelevant reviews.},
author = {Guzman, Emitza and Maalej, Walid},
booktitle = {2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings},
doi = {10.1109/RE.2014.6912257},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Guzman, Emitza and Maalej, Walid{\_} {\_}{\_}How do users like this feature{\_} A fine grained sentiment analysis of App reviews{\_}{\_} (2014).pdf:pdf},
isbn = {9781479930333},
issn = {2332-6441},
keywords = {feature extraction,formal specification,ieee{\_}inc{\_}nlp{\_}x{\_}re,informatio},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {153--162},
title = {{How do users like this feature? A fine grained sentiment analysis of App reviews}},
year = {2014}
}
@incollection{piattini_gdpr-based_2019,
abstract = {Because of GDPR's principle of “data protection by design and by default”, organizations who wish to stay lawful have to re-think their data practices. Access Control (AC) can be a technical solution for them to protect access to “personal data by design”, and thus to gain legal compliance, but this requires to have Access Control Policies (ACPs) expressing requirements aligned with GDPR's provisions. Provisions are however pieces of law and are not written to be immediately interpreted as technical requirements; the task is thus not straightforward. The Agile software development methodology can help untangle the problem. It has dedicated tools to describe requirements and one of such them, User Stories, seems up to task. Stories are concise yet informal descriptions telling who, what and why something is required by users; they are prioritized in lists, called backlogs. Inspired by these Agile tools this paper advances the notion of Data Protection backlogs, which are lists of User Stories about GDPR provisions told as technical requirements. For each User Story we build a corresponding ACP, so enabling the implementation of GDPR compliant AC systems.},
address = {Cham},
annote = {Series Title: Communications in Computer and Information Science},
author = {Bartolini, Cesare and Daoudagh, Said and Lenzini, Gabriele and Marchetti, Eda},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-030-29238-6_1},
editor = {Piattini, Mario and da Cunha, Paulo and de Guzm{\'{a}}n, Ignacio and P{\'{e}}rez-Castillo, Ricardo},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bartolini, Cesare and Daoudagh, Said and Lenzini, Gabriele and Marchetti, Eda{\_} {\_}{\_}GDPR-Based User Stories in the Access Control Perspective{\_}{\_} (2019).pdf:pdf},
isbn = {9783030292379},
issn = {18650937},
keywords = {Access Control Policy (ACP),General Data Protection Regulation (GDPR),User Story,springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {3--17},
publisher = {Springer International Publishing},
title = {{GDPR-Based User Stories in the Access Control Perspective}},
url = {http://link.springer.com/10.1007/978-3-030-29238-6{\_}1},
volume = {1010},
year = {2019}
}
@conference{Zinsmaier2020473,
abstract = {We propose and apply a requirements engineering approach that focuses on security and privacy properties and takes into account various stakeholder interests. The proposed methodology facilitates the integration of security and privacy by design into the requirements engineering process. Thus, specific, detailed security and privacy requirements can be implemented from the very beginning of a software project. The method is applied to an exemplary application scenario in the logistics industry. The approach includes the application of threat and risk rating methodologies, a technique to derive technical requirements from legal texts, as well as a matching process to avoid duplication and accumulate all essential requirements.},
annote = {cited By 0},
author = {Zinsmaier, Sandra Domenique and Langweg, Hanno and Waldvogel, Marcel},
booktitle = {ICISSP 2020 - Proceedings of the 6th International Conference on Information Systems Security and Privacy},
doi = {10.5220/0008960604730480},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/zinsmaier A practical approach to stakeholder-driven determination of security requirements based on the GDPR and common criteria.pdf:pdf},
isbn = {9789897583995},
keywords = {Common criteria,GDPR,Privacy by design,Requirements engineering,Security by design,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {473--480},
title = {{A practical approach to stakeholder-driven determination of security requirements based on the GDPR and common criteria}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083037726{\&}partnerID=40{\&}md5=a2e78310aed8810e27df77c6eb99f11c},
year = {2020}
}
@article{Ringmann2018258,
abstract = {We identify 74 generic, reusable technical requirements based on the GDPR that can be applied to software products which process personal data. The requirements can be traced to corresponding articles and recitals of the GDPR and fulfill the key principles of lawfulness and transparency. Therefore, we present an approach to requirements engineering with regard to developing legally compliant software that satisfies the principles of privacy by design, privacy by default as well as security by design.},
annote = {cited By 2},
author = {Ringmann, Sandra Domenique and Langweg, Hanno and Waldvogel, Marcel},
doi = {10.1007/978-3-030-02671-4_15},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ringmann2018{\_}Chapter{\_}RequirementsForLegallyComplian.pdf:pdf},
isbn = {9783030026707},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Compliant software,GDPR,Privacy by design and by default,Requirements engineering,Security by design,scopus{\_}inc{\_}gdpr{\_}x{\_}re,springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}re,springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {258--276},
title = {{Requirements for legally compliant software based on the GDPR}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055921505{\&}doi=10.1007{\%}2F978-3-030-02671-4{\_}15{\&}partnerID=40{\&}md5=a3080ec2d269eb5ff9311f56b3cec7ad},
volume = {11230 LNCS},
year = {2018}
}
@inproceedings{8501328,
abstract = {The General Data Protection Regulation (GDPR) encourages the use of Data Protection Impact Assessments (DPIAs) to integrate privacy into organisations' activities and practices from early design onwards. To date, however, there has been little prescription about how Security {\&} Privacy Requirements Engineering processes map to the necessary activities of a DPIA, and how these activities can be tool-supported. To address this problem, we present a tool-supported process for undertaking DPIAs using existing Requirements Engineering approaches and the CAIRIS platform. We illustrate this process using a real-world case study example where it was used to elicit privacy risks for a prototype medical application to support chemotherapy treatment.},
author = {Coles, Joshua and Faily, Shamal and Ki-Aries, Duncan},
booktitle = {Proceedings - 2018 5th International Workshop on Evolving Security and Privacy Requirements Engineering, ESPRE 2018},
doi = {10.1109/ESPRE.2018.00010},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Coles, Joshua and Faily, Shamal and Ki-Aries, Duncan{\_} {\_}{\_}Tool-supporting Data Protection Impact Assessments with CAIRIS{\_}{\_} (2018).pdf:pdf},
isbn = {9781538684207},
keywords = {CAIRIS,GDPR,Privacy,Requirements Engineering,Risk,ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
month = {aug},
pages = {21--27},
title = {{Tool-supporting Data Protection Impact Assessments with CAIRIS}},
year = {2018}
}
@article{robol2017toward,
abstract = {Privacy is a key aspect for the European Union (EU), where it is regulated by a specific law, the General Data Protection Regulation (GDPR). Compliance to the GDPR is a problem for organizations, it imposes strict constraints whenever they deal with personal data and, in case of infringement, it specifies severe consequences such as legal and monetary penalties. Such organizations frequently are complex systems, where personal data is processed by humans and technical services. Therefore, it becomes fundamental to consider privacy from the social perspective when designing such system, i.e., when relations between different components are specified. This is, indeed, also specified in the GDPR, which encourages to apply privacy-by-design principles. This paper proposes a method to support the design of GDPR compliant systems, based on a socio-technical approach composed of a modeling language and a reasoning framework.},
annote = {cited By 8},
author = {Robol, Marco and Salnitri, Mattia and Giorgini, Paolo},
doi = {10.1007/978-3-319-70241-4_16},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/6Y7HPISC/Robol e.a. - 2017 - Toward GDPR-Compliant Socio-Technical Systems Mod.pdf:pdf},
isbn = {9783319702407},
issn = {18651348},
journal = {Lecture Notes in Business Information Processing},
keywords = {Automated reasoning,Modeling languages,Privacy,Requirement engineering,Socio-technical systems,scopus{\_}inc{\_}gdpr{\_}x{\_}re,springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}re,springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {236--250},
title = {{Toward GDPR-compliant socio-technical systems: Modeling language and reasoning framework}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035062424{\&}doi=10.1007{\%}2F978-3-319-70241-4{\_}16{\&}partnerID=40{\&}md5=6e9c34d89dfa11e6ee7b96a022592513},
volume = {305},
year = {2017}
}
@inproceedings{10.1145/3266237.3266270,
abstract = {Context: Privacy of personal data is a growing concern regarding users of software systems. In this sense, the literature reports that in order to avoid privacy breaches, there must be systematic approaches to specify privacy requirements from the early activities of software development. Objective: Motivated by this situation, this paper presents a framework of privacy modeling capabilities that must be addressed by requirements modeling languages to better support privacy specification. The capabilities will be used to compare three goal-oriented modeling languages (i∗, NFR-Framework and Secure-Tropos). Method: The framework was created with basis on a conceptual foundation and a conceptual model of privacy built from an analysis of a standard, a regulation, guidelines and other bibliographical sources related to privacy. A health care example is used to illustrate how the framework can be used to compare the chosen modeling languages. Results: Fourteen privacy modeling capabilities were defined in the framework and it was observed that the analyzed modeling languages do not fully support them. Conclusions: The proposed framework contributes towards the consolidation of a privacy conceptual foundation that can be used to evaluate modeling languages for privacy in Requirements Engineering. The comparison performed by using this framework indicates Secure-Tropos as the most complete language to model privacy among the analyzed goal-oriented modeling languages.},
address = {New York, NY, USA},
author = {Peixoto, Mariana Maia and Silva, Carla},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3266237.3266270},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/peixoto Specifying privacy requirements with goal-oriented modeling languages.pdf:pdf},
isbn = {9781450365031},
keywords = {Goal-oriented languages,Privacy,Requirements engineering,Requirements modeling,acm{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {112--121},
publisher = {Association for Computing Machinery},
series = {SBES '18},
title = {{Specifying privacy requirements with goal-oriented modeling languages}},
url = {https://doi.org/10.1145/3266237.3266270},
year = {2018}
}
@conference{Dias20192551,
abstract = {The process of sensitive data preservation is a manual and a semi-automatic procedure. Sensitive data preservation suffers various problems, in particular, affect the handling of confidential, sensitive and personal information, such as the identification of sensitive data in documents requiring human intervention that is costly and propense to generate error, and the identification of sensitive data in large-scale documents does not allow an approach that depends on human expertise for their identification and relationship. DataSense will be highly exportable software that will enable organizations to identify and understand the sensitive data in their possession in unstructured textual information (digital documents) in order to comply with legal, compliance and security purposes. The goal is to identify and classify sensitive data (Personal Data) present in large-scale structured and non-structured information in a way that allows entities and/or organizations to understand it without calling into question security or confidentiality issues. The DataSense project will be based on European-Portuguese text documents with different approaches of NLP (Natural Language Processing) technologies and the advances in machine learning, such as Named Entity Recognition, Disambiguation, Co-referencing (ARE) and Automatic Learning and Human Feedback. It will also be characterized by the ability to assist organizations in complying with standards such as the GDPR (General Data Protection Regulation), which regulate data protection in the European Union.},
annote = {cited By 0},
author = {Dias, Mariana and Ferreira, Jo{\~{A}}{\pounds}o C. and Maia, Rui and Santos, Pedro and Ribeiro, Ricardo},
booktitle = {Proceedings of the 33rd International Business Information Management Association Conference, IBIMA 2019: Education Excellence and Innovation Management through Vision 2020},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/dias 1573829346-paper-313-Privacy-in-Text-Documents.pdf:pdf},
isbn = {9780999855126},
keywords = {Named Entities Recognition,Natural Language Processing,Sensitive Data,Text Mining,scopus{\_}inc{\_}gdpr{\_}x{\_}nlp,wos{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}nlp,wos{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {2551--2560},
title = {{Privacy in text documents}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074083548{\&}partnerID=40{\&}md5=0dda63052746ad033b0cbe3d1c510df6},
year = {2019}
}
@article{nazir2017applications,
abstract = {Natural Language Processing (NLP) is a well-known technique of artificial intelligence to extract the elements of concerns from raw plain text information. It can be utilized to process the early software requirements in order to achieve the goals like requirement prioritization and classification (functional and non-functional). To the best of our knowledge, no research work is available yet to examine and summarize the utilization of NLP in the domain of Software Requirement Engineering (SRE). Therefore, in this paper, we investigate the applications of NLP in the context of SRE. A Systematic Literature Review (SLR) is carried out to select 27 studies published during 2002–2016. Consequently, 6 NLP techniques and 14 existing tools are identified. Furthermore, 9 tools and 2 algorithms, proposed by the researchers, are presented. It has been concluded that the NLP techniques and tools are highly supportive to accelerate the SRE process. However, some manual operations are still required on initial plain text software requirements before applying the desired NLP techniques.},
annote = {cited By 6},
author = {Nazir, Farhana and Butt, Wasi Haider and Anwar, Muhammad Waseem and {Khan Khattak}, Muazzam A.},
doi = {10.1007/978-981-10-4154-9_56},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/NAZIR{\_}{\~{}}1.PDF:PDF},
isbn = {9789811041532},
issn = {18761119},
journal = {Lecture Notes in Electrical Engineering},
keywords = {NLP,NLP tools,SRE,Software requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {485--493},
title = {{The applications of natural language processing (NLP) for software requirement engineering - A systematic literature review}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016110229{\&}doi=10.1007{\%}2F978-981-10-4154-9{\_}56{\&}partnerID=40{\&}md5=8d023ab2bfef65cc9b202fba103f4fd9},
volume = {424},
year = {2017}
}
@inproceedings{Rabinia8501495,
abstract = {In recent years, several goal modeling approaches have been used and extended to capture the complexity of legal requirements and help modeling them in notations familiar to the requirements engineers and analysts. Legal-GRL, which is an extension of the Goal-oriented Requirements Language (GRL), is used for modeling and analyzing legal requirements. However, creating Legal-GRL models is still a manual process, which limits its effectiveness and scalability. In this paper, we propose a new goal modeling framework based on GRL to facilitate the automation of the legal requirements modeling process. Our FOL-based Legal-GRL (FLG) framework uses a legal ontology, which entails a modal theory and First-order Logic (FOL) approach, for the purpose of extraction, refinement, and representation of legal requirements. Our FLG framework consists of a database design and a set of methods for automating the modeling process. We evaluate our work by modeling several statements from HIPAA, PHIPA, the EU GDPR and EU-US Privacy Shield.},
author = {Rabinia, Amin and Ghanavati, Sepideh},
booktitle = {Proceedings - 2018 8th International Model-Driven Requirements Engineering Workshop, MoDRE 2018},
doi = {10.1109/MoDRE.2018.00014},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rabinia, Amin and Ghanavati, Sepideh{\_} {\_}{\_}The FOL-based legal-GRL (FLG) framework{\_} Towards an automated goal modeling approach for regulations{\_}{\_} (2018).pdf:pdf},
isbn = {9781538684061},
keywords = {First order logic,GRL,Goal modeling,Legal requirements,ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {58--67},
title = {{The FOL-based legal-GRL (FLG) framework: Towards an automated goal modeling approach for regulations}},
year = {2018}
}
@inproceedings{silva_using_2020,
abstract = {As information systems deal with contracts and documents in essential services, there is a lack of mechanisms to help organizations in protecting the involved data subjects. In this paper, we evaluate the use of named entity recognition as a way to identify, monitor and validate personally identifiable information. In our experiments, we use three of the most well-known Natural Language Processing tools (NLTK, Stanford CoreNLP, and spaCy). First, the effectiveness of the tools is evaluated in a generic dataset. Then, the tools are applied in datasets built based on contracts that contain personally identifiable information. The results show that models' performance was highly positive in accurately classifying both the generic and the contracts' data. Furthermore, we discuss how our proposal can effectively act as a Privacy Enhancing Technology.},
address = {New York, NY, USA},
annote = {event-place: Brno, Czech Republic},
author = {Silva, Paulo and Gon{\c{c}}alves, Carolina and Godinho, Carolina and Antunes, Nuno and Curado, Marilia},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3341105.3375774},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/SILVA{\_}{\~{}}3.PDF:PDF},
isbn = {9781450368667},
keywords = {Named entity recognition,Natural language processing,Online contracts,Personally identifiable information,Privacy violations,acm{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {acm{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {1305--1307},
publisher = {Association for Computing Machinery},
series = {{\{}SAC{\}} '20},
title = {{Using natural language processing to detect privacy violations in online contracts}},
url = {https://doi.org/10.1145/3341105.3375774},
year = {2020}
}
@incollection{camarinha-matos_interactive_2019,
abstract = {This case study focuses on an experiment analysing textual conversation data using machine learning algorithms and shows that sharing data across organisational boundaries requires anonymisation that decreases that data's information richness. Additionally, sharing data between organisations, conducting data analytics and collaborating to create new business insight requires inter-organisational collaboration. This study shows that analysing highly anonymised and professional conversation data challenges the capabilities of artificial intelligence. Machine learning algorithms alone cannot learn the internal connections and meanings of information cues. This experiment is therefore in line with prior research in interactive machine learning where data scientists, specialists and computational agents interact. This study reveals that, alongside humans, computational agents will be important actors in collaborative networks. Thus, humans are needed in several phases of the machine learning process for facilitating and training. This calls for collaborative working in multi-disciplinary teams of data scientists and substance experts interacting with computational agents.},
address = {Cham},
annote = {Series Title: IFIP Advances in Information and Communication Technology},
author = {Alam{\"{a}}ki, Ari and Aunimo, Lili and Ketamo, Harri and Parvinen, Lasse},
booktitle = {IFIP Advances in Information and Communication Technology},
doi = {10.1007/978-3-030-28464-0_16},
editor = {Camarinha-Matos, Luis M and Afsarmanesh, Hamideh and Antonelli, Dario},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Alamkietal.2019InteractiveMachineLearning{\_}ManagingInformationRichnessinHighlyAnonymizedConversationData.pdf:pdf},
isbn = {9783030284633},
issn = {1868422X},
keywords = {Anonymisation,Big data,Collaboration,Information richness,Interactive machine learning,Privacy,Unstructured text,springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {173--184},
publisher = {Springer International Publishing},
shorttitle = {Interactive {\{}Machine{\}} {\{}Learning{\}}},
title = {{Interactive Machine Learning: Managing Information Richness in Highly Anonymized Conversation Data}},
url = {http://link.springer.com/10.1007/978-3-030-28464-0{\_}16},
volume = {568},
year = {2019}
}
@inproceedings{Hjerppe8920529,
abstract = {The General Data Protection Regulation (GDPR) in the European Union is the most famous recently enacted privacy regulation. Despite of the regulation's legal, political, and technological ramifications, relatively little research has been carried out for better understanding the GDPR's practical implications for requirements engineering and software architectures. Building on a grounded theory approach with close ties to the Finnish software industry, this paper contributes to the sealing of this gap in previous research. Three questions are asked and answered in the context of software development organizations. First, the paper elaborates nine practical constraints under which many small and medium-sized enterprises (SMEs) often operate when implementing solutions that address the new regulatory demands. Second, the paper elicits nine regulatory requirements from the GDPR for software architectures. Third, the paper presents an implementation for a software architecture that complies both with the requirements elicited and the constraints elaborated.},
archivePrefix = {arXiv},
arxivId = {1907.07498},
author = {Hjerppe, Kalle and Ruohonen, Jukka and Lepp{\"{a}}nen, Ville},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00036},
eprint = {1907.07498},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Hjerppe, Kalle and Ruohonen, Jukka and Lepp{\_}{\_}{\_}{\_}a{\_}{\_}nen, Ville{\_} {\_}{\_}The general data protection regulation{\_} Requirements, architectures, and constraints{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Data protection,GDPR,Law,Privacy,Regulation,Requirements engineering,SMEs,SOA,Software architectures,ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {265--275},
title = {{The general data protection regulation: Requirements, architectures, and constraints}},
volume = {2019-Septe},
year = {2019}
}
@article{7100933,
abstract = {Templates are effective tools for increasing the precision of natural language requirements and for avoiding ambiguities that may arise from the use of unrestricted natural language. When templates are applied, it is important to verify that the requirements are indeed written according to the templates. If done manually, checking conformance to templates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to changes in the requirements. In this article, using techniques from natural language processing (NLP), we develop an automated approach for checking conformance to templates. Specifically, we present a generalizable method for casting templates into NLP pattern matchers and reflect on our practical experience implementing automated checkers for two well-known templates in the requirements engineering community. We report on the application of our approach to four case studies. Our results indicate that: (1) our approach provides a robust and accurate basis for checking conformance to templates; and (2) the effectiveness of our approach is not compromised even when the requirements glossary terms are unknown. This makes our work particularly relevant to practice, as many industrial requirements documents have incomplete glossaries.},
author = {Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank},
doi = {10.1109/TSE.2015.2428709},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank{\_} {\_}{\_}Automated checking of conformance to requirements templates using natural language processing{\_}{\_} (2015).pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Case Study Research,Natural Language Processing (NLP),Requirements Templates,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {10},
pages = {944--968},
title = {{Automated checking of conformance to requirements templates using natural language processing}},
volume = {41},
year = {2015}
}
@article{binkhonain2019review,
abstract = {Context: Recent developments in requirements engineering (RE) methods have seen a surge in using machine-learning (ML) algorithms to solve some difficult RE problems. One such problem is identification and classification of non-functional requirements (NFRs) in requirements documents. ML-based approaches to this problem have shown to produce promising results, better than those produced by traditional natural language processing (NLP) approaches. Yet, a systematic understanding of these ML approaches is still lacking. Method: This article reports on a systematic review of 24 ML-based approaches for identifying and classifying NFRs. Directed by three research questions, this article aims to understand what ML algorithms are used in these approaches, how these algorithms work and how they are evaluated. Results: (1) 16 different ML algorithms are found in these approaches; of which supervised learning algorithms are most popular. (2) All 24 approaches have followed a standard process in identifying and classifying NFRs. (3) Precision and recall are the most used matrices to measure the performance of these approaches. Finding: The review finds that while ML-based approaches have the potential in the classification and identification of NFRs, they face some open challenges that will affect their performance and practical application. Impact: The review calls for the close collaboration between RE and ML researchers, to address open challenges facing the development of real-world ML systems. Significance: The use of ML in RE opens up exciting opportunities to develop novel expert and intelligent systems to support RE tasks and processes. This implies that RE is being transformed into an application of modern expert systems.},
annote = {cited By 9},
author = {Binkhonain, Manal and Zhao, Liping},
doi = {10.1016/j.eswax.2019.100001},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Binkhonain, Manal and Zhao, Liping{\_} {\_}{\_}A review of machine learning algorithms for identification and classification of non-functional requirements{\_}{\_} (2019).pdf:pdf},
issn = {25901885},
journal = {Expert Systems with Applications: X},
keywords = {Machine learning,Non-functional requirements,Requirements documents,Requirements engineering,Requirements identification Requirements classific,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{A review of machine learning algorithms for identification and classification of non-functional requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070485369{\&}doi=10.1016{\%}2Fj.eswax.2019.100001{\&}partnerID=40{\&}md5=677fac38e2d3f2499ceca7984fe200f0},
volume = {1},
year = {2019}
}
@incollection{Robol2018,
abstract = {Since the origin of the web, up to social networks, and now to the internet of things, the quantity of personal information produced and shared is uncontrollably increasing. Privacy regulations protect our right to have the control on our personal data. According to the recent General Data Protection Regulation (GDPR), entered into force in May 2018, infringements can be very costly to organizations, ranging from 10s to 100s of thousands of Euros. In order to ensure compliance with such regulations, privacy should be taken into consideration as early as at requirements time, so to avoid expensive after-the-fact fixes. Modeling frameworks have been proposed to support the analysis of requirements in complex socio-technical systems, however, even if a primary role is given to security, for privacy more work need to be done. In this paper, starting from the social concept of consent, we propose a modeling language and define the formal framework for the analysis of privacy-consent requirements. We report on our experience in the analysis of privacy in the medical domain, in the context of a research project with the Trentino health-care provider (APSS).},
address = {Cham},
annote = {Series Title: Lecture Notes in Business Information Processing},
author = {Robol, Marco and Paja, Elda and Salnitri, Mattia and Giorgini, Paolo},
booktitle = {Lecture Notes in Business Information Processing},
doi = {10.1007/978-3-030-02302-7_15},
editor = {Buchmann, Robert Andrei and Karagiannis, Dimitris and Kirikova, Marite},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Robol2018{\_}Chapter{\_}ModelingAndReasoningAboutPriva.pdf:pdf},
isbn = {9783030023010},
issn = {18651348},
keywords = {Automated reasoning,Consent,Modeling,Privacy,Regulations,Requirements,Socio-technical systems,springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {238--254},
publisher = {Springer International Publishing},
title = {{Modeling and reasoning about privacy-consent requirements}},
url = {http://link.springer.com/10.1007/978-3-030-02302-7{\_}15},
volume = {335},
year = {2018}
}
@incollection{nicosia_data_2019,
abstract = {The increase of data leaks, attacks, and other ransom-ware in the last few years have pointed out concerns about data security and privacy. All this has negatively affected the sharing and publication of data. To address these many limitations, innovative techniques are needed for protecting data. Especially, when used in machine learning based-data models. In this context, differential privacy is one of the most effective approaches to preserve privacy. However, the scope of differential privacy applications is very limited (e. g. numerical and structured data). Therefore, in this study, we aim to investigate the behavior of differential privacy applied to textual data and time series. The proposed approach was evaluated by comparing two Principal Component Analysis based differential privacy algorithms. The effectiveness was demonstrated through the application of three machine learning models to both anonymized and primary data. Their performances were thoroughly evaluated in terms of confidentiality, utility, scalability, and computational efficiency. The PPCA method provides a high anonymization quality at the expense of a high time-consuming, while the DPCA method preserves more utility and faster time computing. We show the possibility to combine a neural network text representation approach with differential privacy methods. We also highlighted that it is well within reach to anonymize real-world measurements data from satellites sensors for an anomaly detection task. We believe that our study will significantly motivate the use of differential privacy techniques, which can lead to more data sharing and privacy preserving.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Jaidan, David Nizar and Carrere, Maxime and Chemli, Zakaria and Poisvert, R{\'{e}}mi},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-37599-7_60},
editor = {Nicosia, Giuseppe and Pardalos, Panos and Umeton, Renato and Giuffrida, Giovanni and Sciacca, Vincenzo},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Jaidan, David Nizar and Carrere, Maxime and Chemli, Zakaria and Poisvert, R{\_}{\_}'{\_}e{\_}{\_}mi{\_} {\_}{\_}Data Anonymization for Privacy Aware Machine Learning{\_}{\_} (2019).pdf:pdf},
isbn = {9783030375980},
issn = {16113349},
keywords = {Anomaly detection,Anonymization,Machine learning,Natural language processing,Privacy,Text encoding,Time series,springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {725--737},
publisher = {Springer International Publishing},
title = {{Data Anonymization for Privacy Aware Machine Learning}},
url = {http://link.springer.com/10.1007/978-3-030-37599-7{\_}60},
volume = {11943 LNCS},
year = {2019}
}
@inproceedings{Kumar2011,
abstract = {Going from requirements analysis to design phase is considered as one of the most complex and difficult activities in software development. Errors caused during this activity can be quite expensive to fix in later phases of software development. One main reason for such potential problems is due to the specification of software requirements in Natural Language format. To overcome some of these defects we have proposed a technique, which aims to provide semi-automated assistance for developers to generate UML models from normalized natural language requirements using Natural Language Processing techniques. This technique initially focuses on generating use-case diagram and analysis class model (conceptual model) followed by collaboration model generation for each use-case. Then it generates a consolidated design class model from which code model can also be generated. It also provides requirement traceability both at design and code levels by using Key-Word-In-Context and Concept Location techniques respectively to identify inconsistencies in requirements. Finally, this technique generates XML Metadata Interchange (XMI) files for visualizing generated models in any UML modeling tool having XMI import feature. This paper is an extension to our existing work by enhancing its complete usage with the help of Qualification Verification System as a case study.},
address = {New York, NY, USA},
author = {Deeptimahanti, Deva Kumar and Sanyal, Ratna},
booktitle = {Proceedings of the 4th India Software Engineering Conference 2011, ISEC'11},
doi = {10.1145/1953355.1953378},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Deeptimahanti, Deva Kumar and Sanyal, Ratna{\_} {\_}{\_}Semi-automatic generation of UML models from natural language requirements{\_}{\_} (2011).pdf:pdf},
isbn = {9781450305594},
keywords = {Natural language processing,Requirement engineering,Unified modeling language,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {165--174},
publisher = {Association for Computing Machinery},
series = {ISEC '11},
title = {{Semi-automatic generation of UML models from natural language requirements}},
url = {https://doi.org/10.1145/1953355.1953378},
year = {2011}
}
@article{yang_analysing_2011,
abstract = {Many requirements documents are written in natural language (NL). However, with the flexibility of NL comes the risk of introducing unwanted ambiguities in the requirements and misunderstandings between stakeholders. In this paper, we describe an automated approach to identify potentially nocuous ambiguity, which occurs when text is interpreted differently by different readers. We concentrate on anaphoric ambiguity, which occurs when readers may disagree on how pronouns should be interpreted. We describe a number of heuristics, each of which captures information that may lead a reader to favor a particular interpretation of the text. We use these heuristics to build a classifier, which in turn predicts the degree to which particular interpretations are preferred. We collected multiple human judgements on the interpretation of requirements exhibiting anaphoric ambiguity and showed how the distribution of these judgements can be used to assess whether a particular instance of ambiguity is nocuous. Given a requirements document written in natural language, our approach can identify sentences that contain anaphoric ambiguity, and use the classifier to alert the requirements writer of text that runs the risk of misinterpretation. We report on a series of experiments that we conducted to evaluate the performance of the automated system we developed to support our approach. The results show that the system achieves high recall with a consistent improvement on baseline precision subject to some ambiguity tolerance levels, allowing us to explore and highlight realistic and potentially problematic ambiguities in actual requirements documents. {\textcopyright} Springer-Verlag London Limited 2011.},
author = {Yang, Hui and de Roeck, Anne and Gervasi, Vincenzo and Willis, Alistair and Nuseibeh, Bashar},
doi = {10.1007/s00766-011-0119-y},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/JBP9DHMW/Yang e.a. - 2011 - Analysing anaphoric ambiguity in natural language .pdf:pdf},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Anaphoric ambiguity,Antecedent preference heuristics,Human judgements,Machine learning,Natural language,Nocuous ambiguity,Noun-phrase coreference resolution,Requirements,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {sep},
number = {3},
pages = {163--169},
title = {{Analysing anaphoric ambiguity in natural language requirements}},
url = {http://link.springer.com/10.1007/s00766-011-0119-y},
volume = {16},
year = {2011}
}
@incollection{benczur_towards_2018,
abstract = {The requirements imposed by the new European personal Data Protection Regulation (GDPR) are applicable for all European citizens data processed anywhere in the world. The GDPR requires companies to identify, protect and make compliant their processing of personal data collected from European citizens. Identifying personal data in a non- structured set of big data can be a painful and cost-ineffective operation. In a static search or in a big data streaming mode, this kind of operation requires a huge human effort and/or computing resources, if done manually. The strain becomes event harder when after detection, one has to pseudo- or anonymize some pieces of information according to their category (name, address, age, etc.). The current approaches to identify personal data to anonymize are mainly based on text identification executed via regular expression scripts that are not dynamic enough to identify different formats of personal information. In this paper, we propose a solution based on a supervised machine learning system that identifies personal information in large datasets. Once the different parts of personal information are identified and tagged (also in real time), a pseudo- or anonymization technique is applied as example of processing. A preliminary proof-of-concept implementation is presented.},
address = {Cham},
annote = {Series Title: Communications in Computer and Information Science},
author = {{Di Cerbo}, Francesco and Trabelsi, Slim},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-030-00063-9_13},
editor = {Bencz{\'{u}}r, Andr{\'{a}}s and Thalheim, Bernhard and Horv{\'{a}}th, Tom{\'{a}}{\v{s}} and Chiusano, Silvia and Cerquitelli, Tania and Sidl{\'{o}}, Csaba and Revesz, Peter Z},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/dicerbotelerise2018{\_}preprint.pdf:pdf},
isbn = {9783030000622},
issn = {18650929},
keywords = {Machine learning,Personal data protection,Privacy,springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}nlp},
pages = {118--126},
publisher = {Springer International Publishing},
title = {{Towards personal data identification and anonymization using machine learning techniques}},
url = {http://link.springer.com/10.1007/978-3-030-00063-9{\_}13},
volume = {909},
year = {2018}
}
@article{6582404,
abstract = {Domain analysis is a labor-intensive task in which related software systems are analyzed to discover their common and variable parts. Many software projects include extensive domain analysis activities, intended to jumpstart the requirements process through identifying potential features. In this paper, we present a recommender system that is designed to reduce the human effort of performing domain analysis. Our approach relies on data mining techniques to discover common features across products as well as relationships among those features. We use a novel incremental diffusive algorithm to extract features from online product descriptions, and then employ association rule mining and the (k)-nearest neighbor machine learning method to make feature recommendations during the domain analysis process. Our feature mining and feature recommendation algorithms are quantitatively evaluated and the results are presented. Also, the performance of the recommender system is illustrated and evaluated within the context of a case study for an enterprise-level collaborative software suite. The results clearly highlight the benefits and limitations of our approach, as well as the necessary preconditions for its success. {\textcopyright} 1976-2012 IEEE.},
author = {Hariri, Negar and Castro-Herrera, Carlos and Mirakhorli, Mehdi and Cleland-Huang, Jane and Mobasher, Bamshad},
doi = {10.1109/TSE.2013.39},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/HARIRI{\~{}}1.PDF:PDF},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Domain analysis,association rule mining,clustering,ieee{\_}inc{\_}nlp{\_}x{\_}re,k-nearest neighbor,recommender systems},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
number = {12},
pages = {1736--1752},
title = {{Supporting domain analysis through mining and recommending features from online product listings}},
volume = {39},
year = {2013}
}
@conference{Fernandes2018398,
abstract = {The European Union establishes in the Regulation 2016/679, or GDPR (General Data Protection Regulation), a set of legal dispositions to achieve the protection of natural persons in what personal data processing and the free movement of such data is concerned. When those dispositions are considered in the development of information systems, the later become attainable for legal approval within that scope. This paper presents the methodology we are following to elaborate a reusable catalogue of personal data protection requirements aligned with the GDPR. Following a separation-of-concerns approach, the catalogue shall serve the purpose of constructing information systems able to communicate with those that process individuals' personal data, to materialize the regulatory data protection capabilities disposed in the GDPR. In that context, the elicitation of system requirements demands for the interpretation of a legal document by business analysts, which consists of a scientifically relevant challenge. This research is contextualized by the RSLingo initiative, a model-driven requirements engineering approach for the rigorous specification of system requirements. In particular this paper discusses the GDPR's requirements defined as a catalogue of both business goals and system goals.},
annote = {cited By 2},
author = {Fernandes, M{\`{a}}rio and {Da Silva}, Alberto Rodrigues and Gon{\c{c}}alves, Ant{\'{o}}nio},
booktitle = {ICEIS 2018 - Proceedings of the 20th International Conference on Enterprise Information Systems},
doi = {10.5220/0006810603980405},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/fernandes c145-mf-ICEIS2018{\_}GDPR{\_}CR{\_}vFinal.pdf:pdf},
isbn = {9789897582981},
keywords = {GDPR,Personal Data Protection,Regulation (EU) 2016/679,Requirements Specification,Rslingo,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {398--405},
title = {{Specification of personal data protection requirements: Analysis of legal requirements from the GDPR regulation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047745820{\&}partnerID=40{\&}md5=18ed86b5d55ed3a4834c66f7a0d02746},
volume = {2},
year = {2018}
}
@article{6112783,
abstract = {Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP). {\textcopyright} 1976-2012 IEEE.},
author = {Falessi, Davide and Cantone, Giovanni and Canfora, Gerardo},
doi = {10.1109/TSE.2011.122},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/FALESS{\~{}}2.PDF:PDF},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Empirical software engineering,equivalent requirements,ieee{\_}inc{\_}nlp{\_}x{\_}re,metrics and measurement,natural language processing,traceability recovery},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {jan},
number = {1},
pages = {18--44},
title = {{Empirical principles and an industrial case study in retrieving equivalent requirements via natural language processing techniques}},
volume = {39},
year = {2013}
}
@article{lucassen_improving_2016,
abstract = {User stories are a widely adopted requirements notation in agile development. Yet, user stories are too often poorly written in practice and exhibit inherent quality defects. Triggered by this observation, we propose the Quality User Story (QUS) framework, a set of 13 quality criteria that user story writers should strive to conform to. Based on QUS, we present the Automatic Quality User Story Artisan (AQUSA) software tool. Relying on natural language processing (NLP) techniques, AQUSA detects quality defects and suggest possible remedies. We describe the architecture of AQUSA, its implementation, and we report on an evaluation that analyzes 1023 user stories obtained from 18 software companies. Our tool does not yet reach the ambitious 100 {\%} recall that Daniel Berry and colleagues require NLP tools for RE to achieve. However, we obtain promising results and we identify some improvements that will substantially improve recall and precision.},
author = {Lucassen, Garm and Dalpiaz, Fabiano and van der Werf, Jan Martijn E.M. and Brinkkemper, Sjaak},
doi = {10.1007/s00766-016-0250-x},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/F9LAF2TC/Lucassen e.a. - 2016 - Improving agile requirements the Quality User Sto.pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {AQUSA,Multi-case study,Natural language processing,QUS framework,Requirements quality,User stories,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {sep},
number = {3},
pages = {383--403},
shorttitle = {Improving agile requirements},
title = {{Improving agile requirements: the Quality User Story framework and tool}},
url = {http://link.springer.com/10.1007/s00766-016-0250-x},
volume = {21},
year = {2016}
}
@article{Meis2017,
abstract = {Privacy as a software quality is becoming more important these days and should not be underestimated during the development of software that processes personal data. The privacy goal of intervenability, in contrast to unlinkability (including anonymity and pseudonymity), has so far received little attention in research. Intervenability aims for the empowerment of end-users by keeping their personal data and how it is processed by the software system under their control. Several surveys have pointed out that the lack of intervenability options is a central privacy concern of end-users. In this paper, we systematically assess the privacy goal of intervenability and set up a software requirements taxonomy that relates the identified intervenability requirements with a taxonomy of transparency requirements. Furthermore, we provide a tool-supported method to identify intervenability requirements from the functional requirements of a software system. This tool-supported method provides the means to elicit and validate intervenability requirements in a computer-aided way. Our combined taxonomy of intervenability and transparency requirements gives a detailed view on the privacy goal of intervenability and its relation to transparency. We validated the completeness of our taxonomy by comparing it to the relevant literature that we derived based on a systematic literature review. The proposed method for the identification of intervenability requirements shall support requirements engineers to elicit and document intervenability requirements in compliance with the EU General Data Protection Regulation.},
annote = {cited By 7},
author = {Meis, Rene and Heisel, Maritta},
doi = {10.3390/info8010030},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Meis, Rene and Heisel, Maritta{\_} {\_}{\_}Computer-aided identification and validation of intervenability requirements{\_}{\_} (2017).pdf:pdf},
issn = {20782489},
journal = {Information (Switzerland)},
keywords = {Computer-aided software engineering,Intervenability,Privacy,Privacy analysis,Privacy requirements,Requirements engineering,scopus{\_}inc{\_}gdpr{\_}x{\_}re,wos{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}re,wos{\_}inc{\_}gdpr{\_}x{\_}re},
number = {1},
title = {{Computer-aided identification and validation of intervenability requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016939828{\&}doi=10.3390{\%}2Finfo8010030{\&}partnerID=40{\&}md5=746936d0cb705b6dd074644b677775fa},
volume = {8},
year = {2017}
}
@inproceedings{Stach3297432,
abstract = {Due to the Internet of Things (IoT), a giant leap towards a quantified self is made, i. e., more and more aspects of our lives are being captured, processed, and analyzed. This has many positive implications, e. g., Smart Health services help to relieve patients as well as physicians and reduce treatment costs. However, the price for such services is the disclosure of a lot of private data. For this reason, Smart Health services were particularly considered by the European General Data Protection Regulation (GDPR): a data subject's explicit consent is required when such a service processes his or her data. However, the elicitation of privacy requirements is a shortcoming in most IoT privacy systems. Either the user is overwhelmed by too many options or s/he is not sufficiently involved in the decision process. For this reason, we introduce EPICUREAN, a recommender-based privacy requirements elicitation approach. EPICUREAN uses modeling and data mining techniques to determine and recommend appropriate privacy settings to the user. The user is thus considerably supported but remains in full control over his or her private data.},
address = {New York, NY, USA},
annote = {NLP wordt gebruikt, maar het is geen "approach". Het is een klein onderdeel van de aanpak.},
author = {Stach, Christoph and Steimle, Frank},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3297280.3297432},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Stach, Christoph and Steimle, Frank{\_} {\_}{\_}Recommender-based privacy requirements elicitation - EPICUREAN{\_}{\_} (2019).pdf:pdf},
isbn = {9781450359337},
keywords = {Association rules,Clustering,EHealth,IoT,Knowledge modeling,Privacy requirements elicitation,Privacy system,Recommender system,acm{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {1500--1507},
publisher = {Association for Computing Machinery},
series = {SAC '19},
title = {{Recommender-based privacy requirements elicitation - EPICUREAN}},
url = {https://doi.org/10.1145/3297280.3297432},
volume = {Part F1477}
}
@inproceedings{Xiao2012,
abstract = {Access Control Policies (ACP) specify which principals such as users have access to which resources. Ensuring the correctness and consistency of ACPs is crucial to prevent security vulnerabilities. However, in practice, ACPs are commonly written in Natural Language (NL) and buried in large documents such as requirements documents, not amenable for automated techniques to check for correctness and consistency. It is tedious to manually extract ACPs from these NL documents and validate NL functional requirements such as use cases against ACPs for detecting inconsistencies. To address these issues, we propose an approach, called Text2Policy, to automatically extract ACPs from NL software documents and resource-access information from NL scenario-based functional requirements. We conducted three evaluations on the collected ACP sentences from publicly available sources along with use cases from both open source and proprietary projects. The results show that Text2Policy effectively identifies ACP sentences with the precision of 88.7{\%} and the recall of 89.4{\%}, extracts ACP rules with the accuracy of 86.3{\%}, and extracts action steps with the accuracy of 81.9{\%}. {\textcopyright} 2012 ACM.},
address = {New York, NY, USA},
author = {Xiao, Xusheng and Paradkar, Amit and Thummalapenta, Suresh and Xie, Tao},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012},
doi = {10.1145/2393596.2393608},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Xiao, Xusheng and Paradkar, Amit and Thummalapenta, Suresh and Xie, Tao{\_} {\_}{\_}Automated extraction of security policies from natural-language software documents{\_}{\_} (2012).pdf:pdf},
isbn = {9781450316149},
keywords = {access control,acm{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,requirements analysis},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {FSE '12},
title = {{Automated extraction of security policies from natural-language software documents}},
url = {https://doi.org/10.1145/2393596.2393608},
year = {2012}
}
@incollection{piras_defend_2019,
abstract = {The advent of the European General Data Protection Regulation (GDPR) imposes organizations to cope with radical changes concerning user data protection paradigms. GDPR, by promoting a Privacy by Design approach, obliges organizations to drastically change their methods regarding user data acquisition, management, processing, as well as data breaches monitoring, notification and preparation of prevention plans. This enforces data subjects (e.g., citizens, customers) rights by enabling them to have more information regarding usage of their data, and to take decisions (e.g., revoking usage permissions). Moreover, organizations are required to trace precisely their activities on user data, enabling authorities to monitor and sanction more easily. Indeed, since GDPR has been introduced, authorities have heavily sanctioned companies found as not GDPR compliant. GDPR is difficult to apply also for its length, complexity, covering many aspects, and not providing details concerning technical and organizational security measures to apply. This calls for tools and methods able to support organizations in achieving GDPR compliance. From the industry and the literature, there are many tools and prototypes fulfilling specific/isolated GDPR aspects, however there is not a comprehensive platform able to support organizations in being compliant regarding all GDPR requirements. In this paper, we propose the design of an architecture for such a platform, able to reuse and integrate peculiarities of those heterogeneous tools, and to support organizations in achieving GDPR compliance. We describe the architecture, designed within the DEFeND EU project, and discuss challenges and preliminary benefits in applying it to the healthcare and energy domains.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Piras, Luca and Al-Obeidallah, Mohammed Ghazi and Praitano, Andrea and Tsohou, Aggeliki and Mouratidis, Haralambos and {Gallego-Nicasio Crespo}, Beatriz and Bernard, Jean Baptiste and Fiorani, Marco and Magkos, Emmanouil and Sanz, Andr{\`{e}}s Castillo and Pavlidis, Michalis and D'Addario, Roberto and Zorzino, Giuseppe Giovanni},
booktitle = {16th International Conference, TrustBus 2019, Linz, Austria, August 26–29, 2019},
doi = {10.1007/978-3-030-27813-7_6},
editor = {Gritzalis, Stefanos and Weippl, Edgar R and Katsikas, Sokratis K and Anderst-Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Mavroeidi, Aikaterini Georgia and Kitsiou, Angeliki and Kalloniatis, Christos{\_} {\_}N{\_}A{\_} (N{\_}A).pdf:pdf},
isbn = {9783030278120},
issn = {16113349},
keywords = {Data protection,GDPR,Privacy by design,Privacy engineering,Security engineering,springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {78--93},
publisher = {Springer International Publishing},
shorttitle = {{\{}DEFeND{\}} {\{}Architecture{\}}},
title = {{DEFeND Architecture: A Privacy by Design Platform for GDPR Compliance}},
url = {http://link.springer.com/10.1007/978-3-030-27813-7{\_}6},
volume = {11711 LNCS},
year = {2019}
}
@article{Maalej2016311,
abstract = {App stores like Google Play and Apple AppStore have over 3 million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. The majority of the reviews, however, is rather non-informative just praising the app and repeating to the star ratings in words. This paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and text ratings. For this, we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. We conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. We found that metadata alone results in a poor classification accuracy. When combined with simple text classification and natural language preprocessing of the text—particularly with bigrams and lemmatization—the classification precision for all review types got up to 88–92 {\%} and the recall up to 90–99 {\%}. Multiple binary classifiers outperformed single multiclass classifiers. Our results inspired the design of a review analytics tool, which should help app vendors and developers deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders. We describe the tool main features and summarize nine interviews with practitioners on how review analytics tools including ours could be used in practice.},
annote = {cited By 61},
author = {Maalej, Walid and Kurtanovi{\'{c}}, Zijad and Nabil, Hadeer and Stanik, Christoph},
doi = {10.1007/s00766-016-0251-9},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Maalej, Walid and Kurtanovi{\_}{\_}'{\_}c{\_}{\_}, Zijad and Nabil, Hadeer and Stanik, Christoph{\_} {\_}{\_}On the automatic classification of app reviews{\_}{\_} (2016).pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Data-driven requirements engineering,Machine learning,Natural language processing,Review analytics,Software analytics,User feedback,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {311--331},
title = {{On the automatic classification of app reviews}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968538213{\&}doi=10.1007{\%}2Fs00766-016-0251-9{\&}partnerID=40{\&}md5=f44f36d563f85f8592ade371cd02ec42},
volume = {21},
year = {2016}
}
@inproceedings{Groen8933723,
abstract = {In 2018, the General Data Protection Regulation (GDPR) came into force, imposing strict laws aimed to protect the privacy of natural persons in member states of the European Union. However, the implications of the GDPR with respect to gathering, storing, and analyzing online user feedback - which is an important source of information for Crowd-based Requirements Engineering (CrowdRE) - have not been assessed yet. User feedback has been found to contain personal data, so the GDPR applies. It may be used for CrowdRE if conditions regarding data storage and handling are met and if, when used commercially, the duty to inform is carried out and the data subjects' rights and freedoms are respected. This can be a burden on the application of CrowdRE and might even inhibit its adoption. We propose a heuristic-based solution to anonymize the most prevalent types of personal data while crawling user feedback so that the data processing is no longer subject to GDPR.},
author = {Groen, Eduard C. and Ochs, Michael},
booktitle = {Proceedings - 2019 IEEE 27th International Requirements Engineering Conference Workshops, REW 2019},
doi = {10.1109/REW.2019.00038},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/GROEN{\_}{\~{}}2.PDF:PDF},
isbn = {9781728151656},
keywords = {Crowd-based requirements engineering,Data privacy,GDPR,Online user feedback,ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {180--185},
title = {{CrowdRE, user feedback and GDPR: Towards tackling GDPR implications with adequate technical and organizational measures in an effort-minimal way}},
year = {2019}
}
@inproceedings{Martin8406568,
abstract = {In this position paper we posit that, for Privacy by Design to be viable, engineers must be effectively involved and endowed with methodological and technological tools closer to their mindset, and which integrate within software and systems engineering methods and tools, realizing in fact the definition of Privacy Engineering. This position will be applied in the soon-to-start PDP4E project, where privacy will be introduced into existent general-purpose software engineering tools and methods, dealing with (risk management, requirements engineering, model-driven design, and software/systems assurance).},
author = {Martin, Yod Samuel and Kung, Antonio},
booktitle = {Proceedings - 3rd IEEE European Symposium on Security and Privacy Workshops, EURO S and PW 2018},
doi = {10.1109/EuroSPW.2018.00021},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Martin, Yod Samuel and Kung, Antonio{\_} {\_}{\_}Methods and Tools for GDPR Compliance Through Privacy and Data Protection Engineering{\_}{\_} (2018).pdf:pdf},
isbn = {9781538654453},
keywords = {GDPR,Model-driven engineering,PDP4E,Privacy Impact Assessment,Privacy by Design,Privacy engineering,Requirements engineering,Risk management,Software and systems assurance,ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re,wos{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re,wos{\_}inc{\_}gdpr{\_}x{\_}re},
month = {apr},
pages = {108--111},
title = {{Methods and Tools for GDPR Compliance Through Privacy and Data Protection Engineering}},
year = {2018}
}
@article{Cortina2019136,
abstract = {Companies are facing more and more regulations nowadays, including the General Data Protection Regulation (GDPR). They must then take appropriate technical and organisational measures related to GDPR and effectively implement them. In order to support this and the demonstration of compliance, a process assessment model based on the GDPR is proposed. This paper relates how the process model has been engineered, from semantic annotations based on the GDPR, to identification of rights and obligations for eliciting processes and describing their main components (purpose and outcomes). The support of additional document sources enabled to formulate process assessment indicators.},
annote = {cited By 0},
author = {Cortina, St{\'{e}}phane and Valoggia, Philippe and Barafort, B{\'{e}}atrix and Renault, Alain},
doi = {10.1007/978-3-030-28005-5_11},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Cortina2019{\_}Chapter{\_}DesigningADataProtectionProces.pdf:pdf},
isbn = {9783030280048},
issn = {18650937},
journal = {Communications in Computer and Information Science},
keywords = {Data protection,GDPR,Privacy,Process assessment model,Process model engineering,Requirements engineering,Transformation process,scopus{\_}inc{\_}gdpr{\_}x{\_}re,springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}re,springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {136--148},
title = {{Designing a Data Protection Process Assessment Model Based on the GDPR}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072959280{\&}doi=10.1007{\%}2F978-3-030-28005-5{\_}11{\&}partnerID=40{\&}md5=57df2ef42d17ade79b7a551749880573},
volume = {1060},
year = {2019}
}
@inproceedings{8948740,
abstract = {Due to introduction of General Data Protection Regulation (GDPR) in EU; all the cloud hosted applications (span across multi geo location) wherein captures the personal data need to first identify data privacy protection (DPP) entities and handle it as per EU norms and regulations. The company's legal contracts or transactions in tie up with other parties (customers, partners, suppliers, etc.) are usually stored in to repository; on termination of the contracts either by agreement or mutual consent then the other parties' information are usually archived for historical reasons. The other parties are usually interested in knowing what all documents or transactions they were participated earlier and expects the data to be pruned on need basis. As these documents are unstructured in nature, this paper proposes a solution in identifying all DPP entities with in legal contract documents, index the corpus level accumulated knowledgebase, apply customized ranking algorithm for the retrieved legal contract documents based on DPP search query, derive DPP entities specific legal contract document dependency relation graph for which the parties are participating by using techniques from Information Retrieval, Information Extraction, Natural Language Processing (NLP) and Ontology.},
author = {Nayak, Shiva Prasad and Pasumarthi, Suresh},
booktitle = {Proceedings - 2019 1st International Conference on Digital Data Processing, DDP 2019},
doi = {10.1109/DDP.2019.00023},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Nayak, Shiva Prasad and Pasumarthi, Suresh{\_} {\_}{\_}Automatic Detection and Analysis of DPP Entities in Legal Contract Documents{\_}{\_} (2019).pdf:pdf},
isbn = {9781728153636},
keywords = {DPP,GDPR,Information Extraction,Information Retrieval,Legal Contract Documents,Natural Language Processing,Ontology,ieee{\_}inc{\_}gdpr{\_}x{\_}nlp,scopus{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {ieee{\_}inc{\_}gdpr{\_}x{\_}nlp,scopus{\_}inc{\_}gdpr{\_}x{\_}nlp},
month = {nov},
pages = {70--75},
title = {{Automatic Detection and Analysis of DPP Entities in Legal Contract Documents}},
year = {2019}
}
@incollection{Alshammari2017,
abstract = {Privacy by Design has emerged as a proactive approach for embedding privacy into the early stages of the design of information and communication technologies, but it is no ‘silver bullet'. Challenges involved in engineering Privacy by Design include a lack of holistic and systematic methodologies that address the complexity and variability of privacy issues and support the translation of its principles into engineering activities. A consequence is that its principles are given at a high level of abstraction without accompanying tools and guidelines to address these challenges. We analyse three privacy requirements engineering methods from which we derive a set of criteria that aid in identifying data-processing activities that may lead to privacy violations and harms and also aid in specifying appropriate design decisions. We also present principles for engineering Privacy by Design that can be developed upon these criteria. Based on these, we outline some preliminary thoughts on the form of a principled framework that addresses the plurality and contextuality of privacy issues and supports the translation of the principles of Privacy by Design into engineering activities.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Alshammari, Majed and Simpson, Andrew},
booktitle = {5th Annual Privacy Forum, APF 2017, Vienna, Austria},
doi = {10.1007/978-3-319-67280-9_9},
editor = {Schweighofer, Erich and Leitold, Herbert and Mitrakas, Andreas and Rannenberg, Kai},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Alshammari, Majed and Simpson, Andrew{\_} {\_}{\_}Towards a principled approach for engineering privacy by design{\_}{\_} (2017).pdf:pdf},
isbn = {9783319672793},
issn = {16113349},
keywords = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}gdpr{\_}x{\_}re},
pages = {161--177},
publisher = {Springer International Publishing},
title = {{Towards a principled approach for engineering privacy by design}},
url = {http://link.springer.com/10.1007/978-3-319-67280-9{\_}9},
volume = {10518 LNCS},
year = {2017}
}
@inproceedings{ayala2018grace,
abstract = {The General Data Protection Regulation (GDPR) aims to protect personal data of EU residents and can impose severe sanctions for non-compliance. Organizations are currently implementing various measures to ensure their software systems fulfill GDPR obligations such as identifying a legal basis for data processing or enforcing data anonymization. However, as regulations are formulated vaguely, it is difficult for practitioners to extract and operationalize legal requirements from the GDPR. This paper aims to help organizations understand the data protection obligations imposed by the GDPR and identify measures to ensure compliance. To achieve this goal, we propose GuideMe, a 6-step systematic approach that supports elicitation of solution requirements that link GDPR data protection obligations with the privacy controls that fulfill these obligations and that should be implemented in an organization's software system. We illustrate and evaluate our approach using an example of a university information system. Our results demonstrate that the solution requirements elicited using our approach are aligned with the recommendations of privacy experts and are expressed correctly.},
author = {Ayala-Rivera, Vanessa and Pasquale, Liliana},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00023},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ayala-Rivera, Vanessa and Pasquale, Liliana{\_} {\_}{\_}The grace period has ended{\_} An approach to operationalize GDPR requirements{\_}{\_} (2018).pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {Compliance,GDPR,Privacy,Requirements,ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}gdpr{\_}x{\_}re,scopus{\_}inc{\_}gdpr{\_}x{\_}re},
month = {aug},
pages = {136--146},
title = {{The grace period has ended: An approach to operationalize GDPR requirements}},
year = {2018}
}
@inproceedings{bano2015addressing,
abstract = {Ambiguity in natural language requirements has long been recognized as an inevitable challenge in requirements engineering (RE). Various initiatives have been taken by RE researchers to address the challenges of ambiguity. In this paper the results of a mapping study are presented that focus on the application of Natural Language Processing (NLP) techniques for addressing ambiguity in requirements. Systematic review of the literature resulted in 174 studies on the subject published during 1995 to 2015, and out of these only 28 are empirically evaluated studies that were selected. From of the resulting set of papers, 81{\%} have focused on detecting ambiguity; whereas 4{\%} and 5{\%} are focusing on reducing and removing ambiguity respectively. Addressing syntactic, semantic, and lexical ambiguities has attracted more attention than other types. In spite of all the research efforts, there is a lack of empirical evaluation of NLP tools and techniques for addressing ambiguity in requirements. The results have pointed out some gaps in empirical results and have raised questions the designing of an analytical framework for research in this field.},
author = {Bano, Muneera},
booktitle = {5th International Workshop on Empirical Requirements Engineering, EmpiRE 2015 - Proceedings},
doi = {10.1109/EmpiRE.2015.7431303},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bano, Muneera{\_} {\_}{\_}Addressing the challenges of requirements ambiguity{\_} A review of empirical literature{\_}{\_} (2016).pdf:pdf},
isbn = {9781509001163},
issn = {2329-6356},
keywords = {Ambiguity,NLP,Requirements,Systematic Mapping Study,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {21--24},
title = {{Addressing the challenges of requirements ambiguity: A review of empirical literature}},
year = {2016}
}
@article{McManus2008,
abstract = {Research highlights that only one in eight information technology projects can be considered truly successful (failure being described as those projects that do not meet the original time, cost and (quality) requirements criteria). Despite such failures, huge sums continue to be invested in information systems projects and written off. For example the cost of project failure across the European Union was €142 billion in 2004. The research looked at 214 information systems (IS) projects at the same time, interviews were conducted with a selective number of project managers to follow up issues or clarify points of interest. The period of analysis covered 1998?2005 the number of information systems projects examined across the European Union.},
author = {McManus, J and Wood-Harper, T},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/InformationSystemsProjectFailure.pdf:pdf},
journal = {British Computer Society - The Chartered Institute for IT},
number = {June 2008},
pages = {1--4},
title = {{A study in project failure}},
url = {http://www.bcs.org/content/ConWebDoc/19584},
year = {2008}
}
@inproceedings{torre2019using,
abstract = {The General Data Protection Regulation (GDPR) harmonizes data privacy laws and regulations across Europe. Through the GDPR, individuals are able to better control their personal data in the face of new technological developments. While the GDPR is highly advantageous to individuals, complying with it poses major challenges for organizations that control or process personal data. Since no automated solution with broad industrial applicability currently exists for GDPR compliance checking, organizations have no choice but to perform costly manual audits to ensure compliance. In this paper, we share our experience building a UML representation of the GDPR as a first step towards the development of future automated methods for assessing compliance with the GDPR. Given that a concrete implementation of the GDPR is affected by the national laws of the EU member states, GDPR's expanding body of case law and other contextual information, we propose a two-tiered representation of the GDPR: a generic tier and a specialized tier. The generic tier captures the concepts and principles of the GDPR that apply to all contexts, whereas the specialized tier describes a specific tailoring of the generic tier to a given context, including the contextual variations that may impact the interpretation and application of the GDPR. We further present the challenges we faced in our modeling endeavor, the lessons we learned from it, and future directions for research.},
author = {Torre, Damiano and Soltana, Ghanem and Sabetzadeh, Mehrdad and Briand, Lionel C. and Auffinger, Yuri and Goes, Peter},
booktitle = {Proceedings - 2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems, MODELS 2019},
doi = {10.1109/MODELS.2019.00-20},
isbn = {9781728125350},
keywords = {General Data Protection Regulation,OCL,Regulatory Compliance,UML,ieee{\_}e2{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {ieee{\_}e2{\_}gdpr{\_}x{\_}nlp},
pages = {1--11},
title = {{Using Models to Enable Compliance Checking against the GDPR: An Experience Report}},
year = {2019}
}
@inproceedings{dalpiaz2018agile,
abstract = {90{\%} of agile practitioners employ user stories for capturing requirements. Of these, 70{\%} follow a simple template when creating user stories: As a {\textless}role{\textgreater} I want to {\textless}action{\textgreater}, [so that {\textless}benefit{\textgreater}]. User stories' popularity among practitioners and their simple yet strict structure make them ideal candidates for automatic reasoning based on natural language processing. In our research, we have found that circa 50{\%} of real-world user stories contain easily preventable errors that may endanger their potential. To alleviate this problem, we have created methods, theories and tools that support creating better user stories. This tutorial combines our previous work into a pipeline for working with user stories: (1) The basics of creating user stories, and their use in requirements engineering; (2) How to improve user story quality with the Quality User Story Framework and AQUSA tool; and (3) How to generate conceptual models from user stories using the Visual Narrator and the Interactive Narrator tools. Our toolset is demonstrated with results obtained from 20+ software companies employing user stories.},
author = {Dalpiaz, Fabiano and Brinkkemper, Sjaak},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00075},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/dalpiaz brinkkemper Agile requirements engineering with user stories.pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {Agile requirements engineering,Natural language processing,User stories,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {506--507},
title = {{Agile requirements engineering with user stories}},
year = {2018}
}
@article{brereton2007lessons,
abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
doi = {10.1016/j.jss.2006.07.009},
file = {:C$\backslash$:/Users/aaberkan/Downloads/1-s2.0-S016412120600197X-main.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Empirical software engineering,Systematic literature review},
number = {4},
pages = {571--583},
publisher = {Elsevier Inc.},
title = {{Lessons from applying the systematic literature review process within the software engineering domain}},
url = {http://dx.doi.org/10.1016/j.jss.2006.07.009},
volume = {80},
year = {2007}
}
@article{nadkarni2011natural,
abstract = {Objectives: To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design. Target audience: This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the current state of the art. Scope: We describe the historical evolution of NLP, and summarize common NLP sub-problems in this extensive field. We then provide a synopsis of selected highlights of medical NLP efforts. After providing a brief description of common machine-learning approaches that are being used for diverse NLP sub-problems, we discuss how modern NLP architectures are designed, with a summary of the Apache Foundation's Unstructured Information Management Architecture. We finally consider possible future directions for NLP, and reflect on the possible impact of IBM Watson on the medical field.},
author = {Nadkarni, Prakash M. and Ohno-Machado, Lucila and Chapman, Wendy W.},
doi = {10.1136/amiajnl-2011-000464},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/18-5-544.pdf:pdf},
issn = {10675027},
journal = {Journal of the American Medical Informatics Association},
number = {5},
pages = {544--551},
pmid = {21846786},
title = {{Natural language processing: An introduction}},
volume = {18},
year = {2011}
}
@article{zhao2020natural,
abstract = {Context: NLP4RE - Natural language processing (NLP) supported requirements engineering (RE) - is an area of research and development that seeks to apply NLP techniques, tools and resources to a variety of requirements documents or artifacts to support a range of linguistic analysis tasks performed at various RE phases. Such tasks include detecting language issues, identifying key domain concepts and establishing traceability links between requirements. Objective: This article surveys the landscape of NLP4RE research to understand the state of the art and identify open problems. Method: The systematic mapping study approach is used to conduct this survey, which identified 404 relevant primary studies and reviewed them according to five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of the practice, and the NLP technologies used. Results: (i) NLP4RE is an active and thriving research area in RE that has amassed a large number of publications and attracted widespread attention from diverse communities; (ii) most NLP4RE studies are solution proposals having only been evaluated using a laboratory experiment or an example application; (iii) most NLP4RE studies have focused on the analysis phase, with detection as their central linguistic analysis task and requirements specification as their commonly processed document type; (iv) 130 new tools have been proposed by the selected studies to support a range of linguistic analysis tasks, but there is little evidence of adoption in the long term, although some industrial applications have been published; (v) 140 NLP techniques (e.g., POS tagging and tokenization), 66 NLP tools (e.g., Stanford CoreNLP and GATE) and 25 NLP resources (WordNet and British National Corpus) are extracted from the selected studies, but most of them - particularly those novel NLP techniques and specialized tools - are not in frequent use; by contrast, frequently used NLP technologies are syntactic analysis techniques, general-purpose tools and generic language lexicons. Conclusion: There is a huge discrepancy between the state of the art and the state of the practice in current NLP4RE research, indicated by insufficient industrial validation of NLP4RE research, little evidence of industrial adoption of the proposed tools, the lack of shared RE-specific language resources, and the lack of NLP expertise in NLP4RE research to advise on the choice of NLP technologies.},
archivePrefix = {arXiv},
arxivId = {2004.01099},
author = {Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio and Letsholo, Keletso J. and Ajagbe, Muideen A. and Chioasca, Erol Valeriu and Batista-Navarro, Riza T.},
eprint = {2004.01099},
file = {:C$\backslash$:/Users/aaberkan/Desktop/zhao2020.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {NLP,Natural language processing,Requirements engineering,Software engineering,Systematic mapping study,Systematic review},
number = {v},
title = {{Natural Language Processing (NLP) for requirements engineering: A systematic mapping study}},
year = {2020}
}
@article{budgen2008using,
abstract = {Background: A mapping study provides a systematic and objective procedure for identifying the nature and extent of the empirical study data that is available to answer a particular research question. Such studies can also form a useful preliminary step for PhD study. Aim: We set out to assess how effective such studies have been when used for software engineering topics, and to identify the specific challenges that they present. Method: We have conducted an informal review of a number of mapping studies in software engineering, describing their main characteristics and the forms of analysis employed. Results: We examine the experiences and outcomes from six mapping studies, of which four are published. From these we note a recurring theme about the problems of classification and a preponderance of ‘gaps ' in the set of empirical studies. Conclusions: We identify our challenges as improving classification guidelines, encouraging better reporting of primary studies, and argue for identifying some 'empirical grand challenges ' for software engineering as a focus for the community 1. 1},
author = {Budgen, David and Turner, Mark and Brereton, Pearl and Kitchenham, Barbara},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/10.1.1.222.9091.pdf:pdf},
journal = {Ppig},
pages = {195--204},
title = {{Using Mapping Studies in Software Engineering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.222.9091},
volume = {8},
year = {2008}
}
@article{petersen2008systematic,
abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
doi = {10.14236/ewic/ease2008.8},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/Research/Research methodology/Systematic Mapping Studies in SE Petersen et al.pdf:pdf},
journal = {12th International Conference on Evaluation and Assessment in Software Engineering, EASE 2008},
keywords = {Evidence based software engineering,Systematic mapping studies,Systematic reviews},
pages = {1--10},
title = {{Systematic mapping studies in software engineering}},
year = {2008}
}
@article{meth2013state,
abstract = {Context In large software development projects a huge number of unstructured text documents from various stakeholders becomes available and needs to be analyzed and transformed into structured requirements. This elicitation process is known to be time-consuming and error-prone when performed manually by a requirements engineer. Consequently, substantial research has been done to automate the process through a plethora of tools and technologies. Objective This paper aims to capture the current state of automated requirements elicitation and derive future research directions by identifying gaps in the existing body of knowledge and through relating existing works to each other. More specifically, we are investigating the following research question: What is the state of the art in research covering tool support for automated requirements elicitation from natural language documents? Method A systematic review of the literature in automated requirements elicitation is performed. Identified works are categorized using an analysis framework comprising tool categories, technological concepts and evaluation approaches. Furthermore, the identified papers are related to each other through citation analysis to trace the development of the research field. Results We identified, categorized and related 36 relevant publications. Summarizing the observations we made, we propose future research to (1) investigate alternative elicitation paradigms going beyond a pure automation approach (2) compare the effects of different types of knowledge on elicitation results (3) apply comparative evaluation methods and multi-dimensional evaluation measures and (4) strive for a closer integration of research activities across the sub-fields of automatic requirements elicitation. Conclusion Through the results of our paper, we intend to contribute to the Requirements Engineering body of knowledge by (1) conceptualizing an analysis framework for works in the area of automated requirements elicitation, going beyond former classifications (2) providing an extensive overview and categorization of existing works in this area (3) formulating concise directions for future research. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Meth, Hendrik and Brhel, Manuel and Maedche, Alexander},
doi = {10.1016/j.infsof.2013.03.008},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/1-s2.0-S0950584913000827-main.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Automation,Requirements Elicitation,Requirements Engineering,Requirements Reuse,Systematic Review},
number = {10},
pages = {1695--1709},
publisher = {Elsevier B.V.},
title = {{The state of the art in automated requirements elicitation}},
url = {http://dx.doi.org/10.1016/j.infsof.2013.03.008},
volume = {55},
year = {2013}
}
@misc{gdpr2016,
author = {{European Commission}},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/Data protection in the EU {\_} European Commission.pdf:pdf},
number = {May 2018},
pages = {1--5},
title = {{Data protection in the EU}},
url = {https://ec.europa.eu/info/law/law-topic/data-protection/data-protection-eu{\_}en},
urldate = {2021-02-07},
year = {2021}
}
@article{keele2007guidelines,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
archivePrefix = {arXiv},
arxivId = {1304.1186},
author = {Kitchenham, Barbara and Charters, Stuart},
doi = {10.1145/1134285.1134500},
eprint = {1304.1186},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/10.1.1.117.471.pdf:pdf},
isbn = {1595933751},
issn = {00010782},
pmid = {10853839},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
year = {2007}
}
@incollection{liddy2001natural,
abstract = {Natural Language Processing (NLP) is the computerized approach to analyzing text that is based on both a set of theories and a set of technologies. And, being a very active area of research and development, there is not a single agreed-upon definition that would satisfy everyone, but there are some aspects, which would be part of any knowledgeable person's definition.},
address = {New York, NY, USA},
author = {Liddy, Elizabeth D.},
booktitle = {Encyclopedia of Library and Information Science},
doi = {10.1145/234173.234180},
edition = {2nd Ed.},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/215711011.pdf:pdf},
issn = {15577317},
keywords = {NLP,natural language processing},
mendeley-tags = {NLP,natural language processing},
publisher = {Marcel Decker, Inc.},
title = {{Natural Language Processing}},
url = {https://surface.syr.edu/istpub/63/},
year = {2001}
}
@article{huth2019appropriate,
abstract = {The General Data Protection Regulation requires, inter alia, the establishment of technical and organizational measures to ensure privacy properties. Software developers face the challenge of identifying these properties and suitable privacy enhancing techniques (PET). We conduct a literature study and identify eight privacy engineering approaches, which we analyze for their coverage of the GDPR privacy properties and for their support in software development phases. We conclude that recent privacy engineering approaches have the conceptual background to cover the GDPR, but advocate research on the integration of privacy concerns in software development processes.},
author = {Huth, Dominik and Matthes, Florian},
file = {:C$\backslash$:/Users/aaberkan/Downloads/Huth AMCIS2019 (8).pdf:pdf},
journal = {25th Americas Conference on Information Systems, AMCIS 2019},
keywords = {GDPR,Literature review,Privacy Engineering,Privacy properties},
pages = {1--10},
title = {{“Appropriate technical and organizational measures”: Identifying privacy engineering approaches to meet GDPR requirements}},
year = {2019}
}
@article{albrecht2016gdpr,
author = {{Jan Philipp Albrecht}},
file = {:C$\backslash$:/Users/aaberkan/Downloads/2EurDataProtLRev287 (1).pdf:pdf},
journal = {European Data Protection Law Review},
number = {3},
pages = {287 -- 289},
title = {{How the GDPR Will Change the World}},
volume = {2},
year = {2016}
}
@article{kassab2014state,
abstract = {Little contemporary data exists that documents software requirements elicitation, requirements specification, document development, and specification validation practices. An exploratory survey of more than 3,000 software professionals was conducted and nearly 250 responses were obtained. Survey data obtained includes characteristics of projects, practices, organizations, and practitioners related to requirements engineering. Selected results are presented along with interpretations of this data.},
author = {Kassab, Mohamad and Neill, Colin and Laplante, Phillip},
doi = {10.1007/s11334-014-0232-4},
file = {:C$\backslash$:/Users/aaberkan/Downloads/Kassab2014{\_}Article{\_}StateOfPracticeInRequirementsE.pdf:pdf},
issn = {16145054},
journal = {Innovations in Systems and Software Engineering},
keywords = {Common practices,Requirements elicitation,Requirements engineering,Requirements specification,Software development industry,Software professionals},
number = {4},
pages = {235--241},
title = {{State of practice in requirements engineering: contemporary data}},
volume = {10},
year = {2014}
}
@book{van2009requirements,
abstract = {The paper presents an acoustic pyrometry method for the reconstruction of temperature maps inside power plant boilers. It is based on measuring times-of-flight of acoustic waves along a number of straight paths in a cross-section of the boiler; via an integral relationship, these times depend on the temperature of the gaseous medium along the paths. On this basis, 2D temperature maps can be reconstructed using suitable inversion techniques. The structure of a particular system for the measurement of the times-of-flight is described, and two classes of reconstruction algorithms are presented. The algorithms proposed have been applied to both simulated and experimental data measured in power plants of the Italian National Electricity Board (ENEL). The results obtained appear fairly satisfactory, considering the small data sets that it was possible to acquire in the tested boilers},
author = {Lamsweerde, A Van},
booktitle = {Change},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/Zelfontwikkeling/Boeken/Axel van Lamsweerde/Requirements Engineering{\_} From Syste (20)/Requirements Engineering{\_} From - Axel van Lamsweerde.pdf:pdf},
isbn = {978-0-470-01270-3},
issn = {0018-9456},
pages = {30--34},
title = {{From System Goals to UML Models to Software Specifications}},
year = {2009}
}
@inproceedings{yin2015study,
abstract = {In the past decade, open innovation (OI) has become a major element of companies' innovation processes in almost all industries. Even though, there is still room for research on the potential of OI in Software Engineering (SE), especially, finding out how OI would be used to extract users' software requirements automatically from Internet resources. This paper describes an initial research plan that aims at developing an empirically validated methodology for automatic extraction of software requirements from Internet resources. The research plan describes the architecture of the study, major steps, and expected results.},
address = {New York, NY, USA},
author = {Yin, Huishi},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2785592.2795366},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/yin huishi A study plan Open innovation based on internet data mining in software engineering.pdf:pdf},
isbn = {9781450333467},
keywords = {Internet data mining,Open innovation,Software engineering,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {192--193},
publisher = {Association for Computing Machinery},
series = {ICSSP 2015},
title = {{A study plan: Open innovation based on internet data mining in software engineering}},
url = {https://doi.org/10.1145/2785592.2795366},
volume = {24-26-Augu},
year = {2015}
}
@article{li2019impact,
abstract = {The European Union's General Data Protection Regulation (GDPR) demands significant data protection safeguards and poses both new challenges and potential opportunities to organizations around the world. Most organizations are not yet adequately prepared for compliance with the GDPR. To minimize liability under the GDPR, organizations around the world need to make changes to be in compliance with the GDPR. This editorial preface discusses GDPR's impact on global technology development including both challenges and opportunities. Furthermore, we discuss how China and the U.S., the two leading global economic power, can better respond to the challenges and opportunities brought up by GDPR.},
author = {Li, He and Yu, Lu and He, Wu},
doi = {10.1080/1097198X.2019.1569186},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/The Impact of GDPR on Global Technology Development.pdf:pdf},
issn = {23336846},
journal = {Journal of Global Information Technology Management},
keywords = {General Data Protection Regulation (GDPR),cybersecurity,data governance,personal data,privacy compliance,risk management,technology development},
number = {1},
pages = {1--6},
title = {{The Impact of GDPR on Global Technology Development}},
volume = {22},
year = {2019}
}
@article{nuseibeh2000requirements,
abstract = {This paper presents an overview of the field of software systems requirements engineering (RE). It describes the main areas of RE practice, and highlights some key open research issues for the future.},
author = {Nuseibeh, Bashar and Easterbrook, Steve},
doi = {10.1145/336512.336523},
file = {:C$\backslash$:/Users/aaberkan/Dropbox/Studies/Msc Business Informatics/Thesis/first phase/literature/Nuseibeh {\&} Easterbrook - Requirements Engineering$\backslash$; A roadmap.pdf:pdf},
isbn = {1581132530},
journal = {Proceedings of the Conference on the Future of Software Engineering, ICSE 2000},
pages = {35--46},
title = {{Requirements engineering: A roadmap}},
year = {2000}
}
@misc{GDPRfactsheet,
abstract = {The General Data Protection Regulation (GDPR) regulates the way businesses process and manage personal data. With a single European law for the protection of personal data, your company now needs to conform primarily to one data protection law while offering goods and services anywhere in the EU. By simplifying the regulatory environment for businesses, the GDPR represents a new opportunity for your business to improve personal data management and subsequently increase consumer trust in your business. This brochure highlights the obligations your company has under the GDPR.},
author = {{European Commission}},
doi = {10.2838/97649},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/data-protection-factsheet-sme-obligations{\_}en.pdf:pdf},
title = {{The GDPR: new opportunities , new obligations needs to know about the EU's General Data Protection Regulation business}},
url = {https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-sme-obligations{\_}en.pdf},
urldate = {2020-01-26},
year = {2018}
}
@misc{InformationCommisionersOffice2021,
author = {{Information Commisioners Office}},
file = {:C$\backslash$:/Users/aaberkan/Documents/Information rights after the end of the transition period – Frequently asked questions {\_} ICO.pdf:pdf},
title = {{Information rights at the end of the transition period - Frequently Asked Questions}},
url = {https://ico.org.uk/for-organisations/dp-at-the-end-of-the-transition-period/transition-period-faqs/},
urldate = {2021-01-27}
}
@article{wieringa2006requirements,
author = {Wieringa, Roel and Maiden, Neil and Mead, Nancy and Rolland, Colette},
doi = {10.1007/s00766-005-0021-6},
file = {:C$\backslash$:/Users/aaberkan/Downloads/Wieringa2006{\_}Article{\_}RequirementsEngineeringPaperCl.pdf:pdf},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Paper classification,Paper evaluation criteria,Requirements engineering research,Research methods},
number = {1},
pages = {102--107},
title = {{Requirements engineering paper classification and evaluation criteria: A proposal and a discussion}},
volume = {11},
year = {2006}
}
@article{gregor2013positioning,
abstract = {Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications},
author = {Gregor, Shirley and Hevner, Alan R.},
doi = {10.25300/MISQ/2013/37.2.01},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/43825912.pdf:pdf},
issn = {02767783},
journal = {MIS Quarterly},
keywords = {icle},
month = {feb},
number = {2},
pages = {337--355},
title = {{Positioning and Presenting Design Science Research for Maximum Impact}},
url = {https://www.jstor.org/stable/43825912{\%}0A https://misq.org/positioning-and-presenting-design-science-research-for-maximum-impact.html},
volume = {37},
year = {2013}
}
@article{euGDPR2016,
author = {Union, The European Parliament and the Council of European},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/CELEX{\_}32016R0679{\_}EN{\_}TXT.pdf:pdf},
journal = {Official Journal of the European Union},
title = {{REGULATION (EU) 2016/679}},
year = {2016}
}
@article{tikkinen2018eu,
abstract = {The General Data Protection Regulation (GDPR) will come into force in the European Union (EU) in May 2018 to meet current challenges related to personal data protection and to harmonise data protection across the EU. Although the GDPR is anticipated to benefit companies by offering consistency in data protection activities and liabilities across the EU countries and by enabling more integrated EU-wide data protection policies, it poses new challenges to companies. They are not necessarily prepared for the changes and may lack awareness of the upcoming requirements and the GDPR's coercive measures. The implementation of the GDPR requirements demands substantial financial and human resources, as well as training of employees; hence, companies need guidance to support them in this transition. The purposes of this study were to compare the current Data Protection Directive 95/46/EC with the GDPR by systematically analysing their differences and to identify the GDPR's practical implications, specifically for companies that provide services based on personal data. This study aimed to identify and discuss the changes introduced by the GDPR that would have the most practical relevance to these companies and possibly affect their data management and usage practices. Therefore, a review and a thematic analysis and synthesis of the article-level changes were carried out. Through the analysis, the key practical implications of the changes were identified and classified. As a synthesis of the results, a framework was developed, presenting 12 aspects of these implications and the corresponding guidance on how to prepare for the new requirements. These aspects cover business strategies and practices, as well as organisational and technical measures.},
author = {Tikkinen-Piri, Christina and Rohunen, Anna and Markkula, Jouni},
doi = {10.1016/j.clsr.2017.05.015},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/1-s2.0-S0267364917301966-main.pdf:pdf},
issn = {02673649},
journal = {Computer Law and Security Review},
keywords = {Data Protection Directive,GDPR,General Data Protection Regulation,Personal data},
number = {1},
pages = {134--153},
publisher = {Elsevier Ltd},
title = {{EU General Data Protection Regulation: Changes and implications for personal data collecting companies}},
url = {https://doi.org/10.1016/j.clsr.2017.05.015},
volume = {34},
year = {2018}
}
@book{voigt2017eu,
abstract = {This book provides expert advice on the practical implementation of the European Union's General Data Protection Regulation (GDPR) and systematically analyses its various provisions. Examples, tables, a checklist etc. showcase the practical consequences of the new legislation. The handbook examines the GDPR's scope of application, the organizational and material requirements for data protection, the rights of data subjects, the role of the Supervisory Authorities, enforcement and fines under the GDPR, and national particularities. In addition, it supplies a brief outlook on the legal consequences for seminal data processing areas, such as Cloud Computing, Big Data and the Internet of Things. Adopted in 2016, the General Data Protection Regulation will come into force in May 2018. It provides for numerous new and intensified data protection obligations, as well as a significant increase in fines (up to 20 million euros). As a result, not only companies located within the European Union will have to change their approach to data security; due to the GDPR's broad, transnational scope of application, it will affect numerous companies worldwide.},
address = {Cham},
author = {Voigt, Paul and von dem Bussche, Axel},
booktitle = {Springer International Publishing AG},
doi = {10.1007/978-3-319-57959-7},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/978-3-319-57959-7.pdf:pdf},
isbn = {978-3-319-57958-0},
issn = {03323102},
number = {5},
pages = {747},
pmid = {30489707},
publisher = {Springer International Publishing},
title = {{The EU General Data Protection Regulation (GDPR)}},
url = {https://link.springer.com/book/10.1007/978-3-319-57959-7{\#}about},
volume = {111},
year = {2017}
}
@incollection{rayadurgam_arsenal_2016,
abstract = {Requirements are informal and semi-formal descriptions of the expected behavior of a complex system from the viewpoints of its stakeholders (customers, users, operators, designers, and engineers). However, for the purpose of design, testing, and verification for critical systems, we can transform requirements into formal models that can be analyzed automatically. ARSENAL is a framework and methodology for systematically transforming natural language (NL) requirements into analyzable formal models and logic specifications. These models can be analyzed for consistency and implementability. The ARSENAL methodology is specialized to individual domains, but the approach is general enough to be adapted to new domains.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
archivePrefix = {arXiv},
arxivId = {1403.3142},
author = {Ghosh, Shalini and Elenius, Daniel and Li, Wenchao and Lincoln, Patrick and Shankar, Natarajan and Steiner, Wilfried},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-40648-0_4},
editor = {Rayadurgam, Sanjai and Tkachuk, Oksana},
eprint = {1403.3142},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/RBE9HDE6/Ghosh e.a. - 2016 - ARSENAL Automatic Requirements Specification Extr.pdf:pdf},
isbn = {9783319406473},
issn = {16113349},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {41--46},
publisher = {Springer International Publishing},
shorttitle = {ARSENAL},
title = {{ARSENAL: Automatic requirements specification extraction from natural language}},
url = {http://link.springer.com/10.1007/978-3-319-40648-0{\_}4},
volume = {9690},
year = {2016}
}
@inproceedings{6602669,
abstract = {Natural Language is the general norm for representing requirements in industry. Such representation of requirements cannot be subjected to automated reasoning and is, often, ambiguous and inconsistent. Structuring the natural language requirements can significantly improve reasoning the requirements as well as reusing them in related future projects. We present a novel automated approach to utilize Grammatical Knowledge Patterns for structuring the natural language requirements in the form of Frames. {\textcopyright} 2013 IEEE.},
author = {Bhatia, Jaspreet and Sharma, Richa and Biswas, Kanad K. and Ghaisas, Smita},
booktitle = {2013 3rd International Workshop on Requirements Patterns, RePa 2013 - Proceedings},
doi = {10.1109/RePa.2013.6602669},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bhatia, Jaspreet and Sharma, Richa and Biswas, Kanad K. and Ghaisas, Smita{\_} {\_}{\_}Using Grammatical knowledge patterns for structuring requirements specifications{\_}{\_} (2013).pdf:pdf},
isbn = {9781479909483},
keywords = {Frames,Grammatical Knowledge Patterns,Natural Language Patterns,Natural Language Processing,Requirements Engineering,Reuse,Structuring Requirements,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {31--34},
title = {{Using Grammatical knowledge patterns for structuring requirements specifications}},
year = {2013}
}
@article{Li2020,
abstract = {Although academia has recognized the importance of explicitly specifying security requirements in early stages of system developments for years, in reality, many projects mix security requirements with other types of requirements. Thus, there is a strong need for precisely and efficiently classifying such security requirements from other requirements in requirement specifications. Existing studies leverage lexical evidence to build probabilistic classifiers, which are domain-dependent by design and cannot effectively classify security requirements from different application domains. In this paper, we propose an ontology-driven learning approach to automatically classify security requirements. Our approach consists of a conceptual layer and a linguistic layer, which understands security requirements based on not only lexical evidence but also conceptual domain knowledge. In particular, we apply a systematic approach to identify linguistic features of security requirements based on an extended security requirements ontology and linguistic knowledge, connecting the conceptual layer with the linguistic layer. Such linguistic features are then used to train domain-independent security requirements classifiers by using machine learning techniques. We have carried out a series of experiments to evaluate the performance and generalization ability of our proposal against existing approaches. The results of the experiments show that the proposed approach outperforms existing approaches with a significant increase of F1 score (0.63 VS. 0.44) when the training dataset and the testing dataset come from different application domains, i.e., the classifiers trained by our approach can be generalized to classify security requirements from different domains.},
annote = {cited By 0},
author = {Li, Tong and Chen, Zhishuai},
doi = {10.1016/j.jss.2020.110566},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Li, Tong and Chen, Zhishuai{\_} {\_}{\_}An ontology-based learning approach for automatically classifying security requirements{\_}{\_} (2020).pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {linguistic pattern,machine learning,natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,security requirements classification,security requirements ontology},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{An ontology-based learning approach for automatically classifying security requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081157908{\&}doi=10.1016{\%}2Fj.jss.2020.110566{\&}partnerID=40{\&}md5=816b60db31640a3b8165a4ac22bd00d5},
volume = {165},
year = {2020}
}
@inproceedings{8714682,
abstract = {Requirements Engineering is one of the most important phases of the software development lifecycle. The success of the whole software project depends upon the quality of the requirements. But as we know that mostly the software requirements are stated and documented in the natural language. The requirements written in natural language can be ambiguous and inconsistent. These ambiguities and inconsistencies can lead to misinterpretations and wrong implementations in design and development phase. To address these issues a number of approaches, tools and techniques have been proposed for the automatic detection of natural language ambiguities form software requirement documents. However, to the best of our knowledge, there is very little work done to compare and analyze the differences between these tools and techniques. In this paper, we presented a state of art survey of the currently available tools and techniques for the automatic detection of natural language ambiguities from software requirements. We also focused on figuring out the popularity of different tools and techniques on the basis of citations. This research mathbfwill help the practitioners and researchers to get the latest insights in the above-mentioned context.},
author = {Riaz, Muhammad Qasim and Butt, Wasi Haider and Rehman, Saad},
booktitle = {5th International Conference on Information Management, ICIM 2019},
doi = {10.1109/INFOMAN.2019.8714682},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Riaz, Muhammad Qasim and Butt, Wasi Haider and Rehman, Saad{\_} {\_}{\_}Automatic Detection of Ambiguous Software Requirements{\_} An Insight{\_}{\_} (2019).pdf:pdf},
isbn = {9781728134307},
keywords = {Ambiguity,Ambiguous software requirements,Natural language ambiguity,Natural language processing,Natural language requirement,Requirement engineering,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--6},
title = {{Automatic Detection of Ambiguous Software Requirements: An Insight}},
year = {2019}
}
@conference{Friesen2018,
abstract = {Natural language requirement descriptions are often unstructured, contradictory and incomplete and are therefore challenging for automatic processing. Although many of these deficits can be compensated by means of natural language processing, there still remain cases where interaction with end-users is necessary for clarification. In this vision paper, we present CORDULA, a system using chatbot technology to establish end-user communication in order to support the requirement elicitation and partial compensation of deficits in user requirements.},
annote = {cited By 3},
author = {Friesen, Edwin and B{\"{a}}umer, Frederik S. and Geierhos, Michaela},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/friesen CORDULA Software requirements extraction utilizing chatbot as communication interface.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{CORDULA: Software requirements extraction utilizing chatbot as communication interface}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045428517{\&}partnerID=40{\&}md5=d4b4abace7d7289b989dbb84feab5d57},
volume = {2075},
year = {2018}
}
@inproceedings{8920595,
abstract = {In the past, users were asked to express their needs and intentions by writing a structured requirements document in natural language. Due to the pervasive use of online forums and social media, user feedback is more accessible today. However, the information obtained is often fragmented, involving multipleperspectives from multiple parties on an on-going basis. In this paper, we propose a Crowd-based Requirements Engineering approach by Argumentation (CrowdRE-Arg), which analyses the conversations from user forum, identifies the arguments in favor or opposing of a given requirements related discussion topic. By generating the argumentation model of the involved user statements, we are able to recover the conflicting viewpoints, to reason about the winning arguments for informed requirements decisions. The proposed approach is illustrated with a data set of sample conversations about the design of a new Google-Map feature from Reddit. Also, we apply natural language processing techniques and machine learning algorithms to support the automated execution of the CrowdRE-Arg approach.},
author = {Khan, Javed Ali and Xie, Yuchen and Liu, Lin and Wen, Lijie},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00018},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Khan, Javed Ali and Xie, Yuchen and Liu, Lin and Wen, Lijie{\_} {\_}{\_}Analysis of requirements-related arguments in user forums{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Argumentation,Machine learning,Natural language processing,Requirements,User forum,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {63--74},
title = {{Analysis of requirements-related arguments in user forums}},
volume = {2019-Septe},
year = {2019}
}
@incollection{douligeris_automated_2019,
abstract = {Bridging the gap between natural language requirements (NLR) and precise formal specifications is a crucial task of knowledge engineering. Software system development has become more complex in recent years, and it includes many requirements in different domains that users need to understand. Many of these requirements are expressed in natural language, which may be incomplete and ambiguous. However, the formal language with its rigorous semantics may accurately represent certain temporal logic properties and allow for automatic validation analysis. It is difficult for software engineers to understand the formal temporal logic from numerous requirements. In this paper, we propose a novel method to automatically mine the linear temporal logic (LTL) from the natural language requirements and check the consistency among different formal properties. We use natural language processing (NLP) to parse requirement sentences and map syntactic dependencies to LTL formulas by using our extraction rules. Also, we apply the automata-based model checking to assess the correctness and consistency of the extracted properties. Through implementation and case studies, we demonstrate that our approach is well suited to deal with the temporal logic requirements upon which the natural language is based.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Pi, Xingxing and Shi, Jianqi and Huang, Yanhong and Wei, Hansheng},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-29563-9_8},
editor = {Douligeris, Christos and Karagiannis, Dimitris and Apostolou, Dimitris},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Pi, Xingxing and Shi, Jianqi and Huang, Yanhong and Wei, Hansheng{\_} {\_}{\_}Automated Mining and Checking of Formal Properties in Natural Language Requirements{\_}{\_} (2019).pdf:pdf},
isbn = {9783030295622},
issn = {16113349},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {75--87},
publisher = {Springer International Publishing},
title = {{Automated Mining and Checking of Formal Properties in Natural Language Requirements}},
url = {http://link.springer.com/10.1007/978-3-030-29563-9{\_}8},
volume = {11776 LNAI},
year = {2019}
}
@article{Rago2018801,
abstract = {Engineering activities often produce considerable documentation as a by-product of the development process. Due to their complexity, technical analysts can benefit from text processing techniques able to identify concepts of interest and analyze deficiencies of the documents in an automated fashion. In practice, text sentences from the documentation are usually transformed to a vector space model, which is suitable for traditional machine learning classifiers. However, such transformations suffer from problems of synonyms and ambiguity that cause classification mistakes. For alleviating these problems, there has been a growing interest in the semantic enrichment of text. Unfortunately, using general-purpose thesaurus and encyclopedias to enrich technical documents belonging to a given domain (e.g. requirements engineering) often introduces noise and does not improve classification. In this work, we aim at boosting text classification by exploiting information about semantic roles. We have explored this approach when building a multi-label classifier for identifying special concepts, called domain actions, in textual software requirements. After evaluating various combinations of semantic roles and text classification algorithms, we found that this kind of semantically-enriched data leads to improvements of up to 18{\%} in both precision and recall, when compared to non-enriched data. Our enrichment strategy based on semantic roles also allowed classifiers to reach acceptable accuracy levels with small training sets. Moreover, semantic roles outperformed Wikipedia- and WordNET-based enrichments, which failed to boost requirements classification with several techniques. These results drove the development of two requirements tools, which we successfully applied in the processing of textual use cases.},
annote = {cited By 3},
author = {Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andres},
doi = {10.1007/s10579-017-9406-7},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andres{\_} {\_}{\_}Using semantic roles to improve text classification in the requirements domain{\_}{\_} (2018).pdf:pdf},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Knowledge representation,Natural language processing,Semantic enrichment,Text classification,Use case specification,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {801--837},
title = {{Using semantic roles to improve text classification in the requirements domain}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033438575{\&}doi=10.1007{\%}2Fs10579-017-9406-7{\&}partnerID=40{\&}md5=38d5feda2f01d8e528ea94d90c116ae9},
volume = {52},
year = {2018}
}
@inproceedings{6649851,
abstract = {A software requirements specification (SRS) contains all the requirements for a system-to-be. These are typically separated into functional requirements (FR), which describe the features of the system under development, and the non-functional requirements (NFR), which include quality attributes, design constraints, among others. It is well known that NFRs have a large impact on the overall cost and time of the system development process, as they frequently describe cross-cutting concerns. In order to improve software development support, an automated analysis of SRS documents for different NFR types is required. Our work contains two significant contributions towards this goal: (1) A new gold standard corpus containing annotations for different NFR types, based on a requirements ontology, and (2) a Support Vector Machine (SVM) classifier to automatically categorize requirements sentences into different ontology classes. Results obtained from two different SRS corpora demonstrate the effectiveness of our approach. {\textcopyright} 2013 IEEE.},
author = {Rashwan, Abderahman and Ormandjieva, Olga and Witte, Rene},
booktitle = {Proceedings - International Computer Software and Applications Conference},
doi = {10.1109/COMPSAC.2013.64},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/RASHWA{\~{}}1.PDF:PDF},
isbn = {9780769549866},
issn = {07303157},
keywords = {Requirements Corpus Development,Requirements Ontology,SVM Classifier,Software Requirements Engineering,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {381--386},
title = {{Ontology-based classification of non-functional requirements in software specifications: A new corpus and SVM-based classifier}},
year = {2013}
}
@inproceedings{10.1145/2652524.2652530,
abstract = {Context. A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary serves as a helpful tool for mitigating ambiguities. Goal. A necessary step for building a glossary is to decide upon the glossary terms and to identify their related terms. Doing so manually is a laborious task. Our objective is to provide automated support for identifying candidate glossary terms and their related terms. Our work differs from existing work on term extraction mainly in that, instead of providing a flat list of candidate terms, our approach clusters the terms by relevance. Method. We use case study research as the basis for our empirical investigation. Results. We present an automated approach for identifying and clustering candidate glossary terms. We evaluate the approach through two industrial case studies; one study concerns a satellite software component, and the other - an evidence management tool for safety certification. Conclusions. Our results indicate that over requirements documents: (1) our approach is more accurate than other existing methods for identifying candidate glossary terms; this makes it less likely that our approach will miss important glossary terms. (2) Clustering provides an effective basis for grouping related terms; this makes clustering a useful support tool for selection of glossary terms and associating these terms with their related terms.},
address = {New York, NY, USA},
author = {Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1145/2652524.2652530},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank{\_} {\_}{\_}Improving requirements glossary construction via clustering{\_} Approach and industrial case studies{\_}{\_} (2014).pdf:pdf},
isbn = {9781450327749},
issn = {19493789},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,case study research,clustering,glossary,natural language processing (NLP),term extraction},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ESEM '14},
title = {{Improving requirements glossary construction via clustering: Approach and industrial case studies}},
url = {https://doi.org/10.1145/2652524.2652530},
year = {2014}
}
@inproceedings{10.1145/3106195.3106207,
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering {\&} machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more e.ort need to be invested for making such approaches applicable in practice.},
address = {New York, NY, USA},
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3106195.3106207},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Li, Yang and Schulze, Sandro and Saake, Gunter{\_} {\_}{\_}Reverse engineering variability from natural language documents{\_} A systematic literature review{\_}{\_} (2017).pdf:pdf},
isbn = {9781450352215},
keywords = {Feature identification,Natural language documents,Reverse engineering,Software product lines,Systematic literature review,Variability extraction,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {133--142},
publisher = {Association for Computing Machinery},
series = {SPLC '17},
title = {{Reverse engineering variability from natural language documents: A systematic literature review}},
url = {https://doi.org/10.1145/3106195.3106207},
volume = {1},
year = {2017}
}
@inproceedings{6046555,
abstract = {The capability to identify potential system hazards and operability problems, and to recommend appropriate mitigation mechanisms is vital to the development of safety critical embedded systems. Hazard and Operability (HAZOP) analysis which is mostly used to achieve these objectives is a complex and largely human-centred process, and increased tool support could reduce costs and improve quality. This work presents a framework and tool prototype that facilitates the early identification of potential system hazards from requirements and the reuse of previous experience for conducting HAZOP. The results from the preliminary evaluation of the tool suggest its potential viability for application in real industrial context. {\textcopyright} 2011 IEEE.},
author = {Daramola, Olawande and St{\aa}lhane, Tor and Sindre, Guttorm and Omoronyia, Inah},
booktitle = {2011 4th International Workshop on Managing Requirements Knowledge, MaRK'11 - Part of the 19th IEEE International Requirements Engineering Conference, RE'11},
doi = {10.1109/MARK.2011.6046555},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Daramola, O. and St{\_}{\_}aa{\_}lhane, T. and Omoronyia, I. and Sindre, G.{\_} {\_}{\_}Using Ontologies and Machine Learning for Hazard Identification and Safety Analysis{\_}{\_} (2013).pdf:pdf},
isbn = {9781457709388},
keywords = {HAZOP analysis,case-based reasoning,hazard identification,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,ontology,requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {3--11},
title = {{Enabling hazard identification from requirements and reuse-oriented HAZOP analysis}},
year = {2011}
}
@inproceedings{5783092,
abstract = {In times of ever-growing system complexity and thus increasing possibilities for errors, high-quality requirements are crucial to prevent design errors in later project phases and to facilitate design verification and validation. To ensure and improve the consistency, completeness and correctness of requirements, formal languages have been introduced as an alternative to using natural language (NL) requirement descriptions. However, in many cases existing NL requirements must be taken into account. The formalization of those requirements by now is a primarily manual task, which therefore is both cumbersome and error-prone. We introduce the tool DODT that semi-automatically transforms NL requirements into semi-formal boilerplate requirements. The transformation builds upon a domain ontology (DO) containing knowledge of the problem domain and upon natural language processing techniques. The tool strongly reduced the required manual effort for the transformation. In addition the quality of the requirements was improved. {\textcopyright} 2011 IEEE.},
author = {Farfeleder, Stefan and Moser, Thomas and Krall, Andreas and St{\aa}lhane, Tor and Zojer, Herbert and Panis, Christian},
booktitle = {Proceedings of the 2011 IEEE Symposium on Design and Diagnostics of Electronic Circuits and Systems, DDECS 2011},
doi = {10.1109/DDECS.2011.5783092},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/FARFEL{\~{}}1.PDF:PDF},
isbn = {9781424497560},
keywords = {electronic engineering computing,embedded systems,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
pages = {271--274},
title = {{DODT: Increasing requirements formalism using domain ontologies for improved embedded systems development}},
year = {2011}
}
@inproceedings{5693175,
abstract = {To elicit software requirements, we have to have knowledge about a problem domain, e.g., healthcare, shopping or banking where the software is applied. A description of domain knowledge such as a domain ontology helps requirements analysts to elicit requirements completely and correctly to some extent even if they do not have such knowledge sufficiently. Several requirements elicitation methods and tools using domain knowledge description have been thus proposed, but how to develop and to enhance such description is rarely discussed. Summarizing existing documents related to the domain is one of the typical ways to develop such description, and an interview to domain experts is another typical way. However, requirements cannot be elicited completely only with such domain-specific knowledge because a user of such knowledge, i.e., a requirements analyst is not a domain expert in general. Requirements could be also elicited more correctly with both specific and general knowledge because general knowledge sometimes improves understandings of analysts about domain-specific knowledge. In this paper, we propose a method and a tool to enhance an ontology of domain knowledge for requirements elicitation by using Web mining. In our method and our tool, a domain ontology consists of concepts and their relationships. Our method and tool helps an analyst with a domain ontology to mine general concepts necessary for his requirements elicitation from documents on Web and to add such concepts to the ontology. We confirmed enhanced ontologies contribute to improving the completeness and correctness of elicited requirements through a comparative experiment. {\textcopyright} 2010 IEEE.},
author = {Kaiya, Haruhiko and Shimizu, Yuutarou and Yasui, Hirotaka and Kaijiri, Kenji and Saeki, Motoshi},
booktitle = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
doi = {10.1109/APSEC.2010.11},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kaiya, Haruhiko and Shimizu, Yuutarou and Yasui, Hirotaka and Kaijiri, Kenji and Saeki, Motoshi{\_} {\_}{\_}Enhancing domain knowledge for requirements elicitation with Web mining{\_}{\_} (2010).pdf:pdf},
isbn = {9780769542669},
issn = {15301362},
keywords = {Domain knowledge,Ontology,Requirements elicitation,Web mining,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {nov},
pages = {3--12},
title = {{Enhancing domain knowledge for requirements elicitation with Web mining}},
year = {2010}
}
@inproceedings{8501308,
abstract = {During requirements elicitation, different stakeholders with diverse backgrounds and skills need to effectively communicate to reach a shared understanding of the problem at hand. Linguistic ambiguity due to terminological discrepancies may occur between stakeholders that belong to different technical domains. If not properly addressed, ambiguity can create frustration and distrust during requirements elicitation meetings, and lead to problems at later stages of development. This paper presents a natural language processing approach to identify ambiguous terms between different domains. The approach is based on building domain-specific language models, one for each stakeholders' domain. Word embeddings from each language model are compared in order to measure the differences of use of a word, thus estimating its potential ambiguity across the domains of interest. The proposed strategy can be useful to prepare lists of dangerous terms to take into account during requirements elicitation meetings, such as workshops, or focus groups, when these involve stakeholders from distant domains.},
author = {Ferrari, Alessio and Esuli, Andrea and Gnesi, Stefania},
booktitle = {Proceedings - 2018 5th International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2018},
doi = {10.1109/AIRE.2018.00011},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ferrari Identification of Cross-Domain Ambiguity with Language Models.pdf:pdf},
isbn = {9781538684047},
keywords = {ambiguity,ambiguity detection,domain specific ambiguity,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,nlp,requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,word embeddings,word2vec},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {31--38},
title = {{Identification of Cross-Domain Ambiguity with Language Models}},
year = {2018}
}
@article{Subha2013131,
abstract = {The software requirements are documented in natural language to make it easy for the users to understand the document. This flexibility of natural language comes with the risk of introducing unwanted ambiguities in the requirements thus leading to poor quality. In this paper, we propose and evaluate a framework that automatically analyses the ambiguities in a requirements document, summarizes the document and assess its quality. We analyse the ambiguities that can occur in natural language and present a method to automate ambiguity analysis and consistency and completeness verification that are usually carried out by human reviewers which is time consuming and ineffective. The Open Text Summarizer based system summarizes the document and provides an extract of it. We use a decision tree based quality evaluator that identifies the quality indicators in the requirements document and evaluates it. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
annote = {cited By 2},
author = {Subha, R. and Palaniswami, S.},
doi = {10.1007/978-3-642-36321-4-12},
isbn = {9783642363207},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Ambiguity,Natural language Processing,Quality factors,Software Requirements Document,Text summarization,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {131--146},
title = {{Quality factor assessment and text summarization of unambiguous natural language requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872424182{\&}doi=10.1007{\%}2F978-3-642-36321-4-12{\&}partnerID=40{\&}md5=37b2c9cb2adf28783b895bead43b73d1},
volume = {361 CCIS},
year = {2013}
}
@conference{Zichler201945,
abstract = {Automotive OEMs and suppliers negotiate different documents before they sign contracts for a product development. This includes the Component Requirements Specification (CRS), which is submitted by the OEM. The CRS describes the characteristics of the product to be developed in detail and is therefore the basis for the development effort estimation of a supplier. If the specified component is a successor of an already available product, the requirements specifications of both the successor and the predecessor products can be compared to estimate the development effort for the new component. This activity is called delta analysis. Due to a lack of sufficient tool support, the delta analysis is still a predominantly manual task. The main reason for this is, that the documents to be compared are structurally too different. In this work, we introduce a new method for an automated conversion of an OEM's unstructured or otherwise structured CRS into a structured language used by the supplier. The process uses established NLP tools to analyze CRS and then translates the OEM's requirements into supplier-specific boilerplates using a newly developed technique. The concept is implemented with the R2BC prototype, which demonstrates the feasibility of the approach and enables the processing of first real CRS.},
annote = {cited By 0},
author = {Zichler, Konstantin and Helke, Steffen},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/zichler R2bc Tool-based requirements preparation for delta analyses by conversion into boilerplates.pdf:pdf},
issn = {16130073},
keywords = {Boilerplates,Delta analysis,Natural language processing,Requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {45--52},
title = {{R2bc : Tool-based requirements preparation for delta analyses by conversion into boilerplates}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061804632{\&}partnerID=40{\&}md5=77c3da34c5ceddae9752d04e09b9c3c9},
volume = {2308},
year = {2019}
}
@inproceedings{7508197,
abstract = {The foremost problem that arises in the Software Development Cycle is during the Requirements specification and analysis. Errors that are encountered during the first phase of the cycle migrate to other phases too which in turn results in the most costly process than the original specified process. The reason is that the specifications of software requirements are termed in the Nature Language Format. One can easily transform the requirements specified into computer model using UML. To minimize the errors that arise in the existing system, we have proposed a new technique that enhances the generation of UML models through Natural Language requirements, which can easily provide automatic assistance to the developers. The main aim of our paper is to focus on the production of Activity Diagram and Sequence Diagram through Natural Language Specifications. Standard POS tagger and parser analyze the input i.e., requirements in English language given by the users and extract phrases, activities, etc. from the text specifies. The technique is beneficial as it reduces the gap between informal natural language and the formal modeling language. The input is the requirements laid down by the users in English language. Some stages like pre-processing, part of speech (POs), tagging, parsing, phrase identification and designing of UML diagrams occur along with the input. The application and its framework is developed in Java and it is tested on by implementing on a few technical documents.},
author = {Gulia, Sarita and Choudhury, Tanupriya},
booktitle = {Proceedings of the 2016 6th International Conference - Cloud System and Big Data Engineering, Confluence 2016},
doi = {10.1109/CONFLUENCE.2016.7508197},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Gulia, Sarita and Choudhury, Tanupriya{\_} {\_}{\_}An efficient automated design to generate UML diagram from Natural Language Specifications{\_}{\_} (2016).pdf:pdf},
isbn = {9781467382021},
keywords = {Activity Diagram,Natural language (NL),Part of speech (POS) Tagger,Sequence Diagram,Software Requirement Specification,Unified Modeling Language (UML),Verb phrase,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {jan},
pages = {641--648},
title = {{An efficient automated design to generate UML diagram from Natural Language Specifications}},
year = {2016}
}
@conference{Kifetew2019,
abstract = {In this short report paper, we introduce the Software Engineering research unit at Fondazione Bruno Kessler, and summarise the research carried out in the area of requirements engineering for which natural language processing techniques have been exploited to build tools at support of software engineers. Ongoing and longer term research objectives are briefly outlined.},
annote = {cited By 0},
author = {Kifetew, F.M. and Perini, A. and Susi, A.},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/kifetew Research on NLP for RE at the FBK-software engineering research line A report.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Research on NLP for RE at the FBK-software engineering research line: A report}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068034388{\&}partnerID=40{\&}md5=0ea263374f67d895dcb78495efaba7ea},
volume = {2376},
year = {2019}
}
@conference{Schlutter2018,
abstract = {Complex systems such as automotive software systems are usually broken down into subsystems that are specified and developed in isolation and afterwards integrated to provide the functionality of the desired system. This results in a large number of requirements documents for each subsystem written by different people and in different departments. Requirements engineers are challenged by comprehending the concepts mentioned in a requirement because coherent information is spread over several requirements documents. In this paper, we describe a natural language processing pipeline that we developed to transform a set of heterogeneous natural language requirements into a knowledge representation graph. The graph provides an orthogonal view onto the concepts and relations written in the requirements. We provide a first validation of the approach by applying it to two requirements documents including more than 7,000 requirements from industrial systems. We conclude the paper by stating open challenges and potential application of the knowledge representation graph.},
annote = {cited By 2},
author = {Schlutter, Aaron and Vogelsang, Andreas},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/schlutter{\_}vogelsang{\_}2018.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Knowledge representation of requirements documents using natural language processing}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045430054{\&}partnerID=40{\&}md5=e8c5626f03108bb01c1831965797e68c},
volume = {2075},
year = {2018}
}
@article{Dalpiaz20193,
abstract = {Context. Defects such as ambiguity and incompleteness are pervasive in software requirements, often due to the limited time that practitioners devote to writing good requirements. Objective.We study whether a synergy between humans' analytic capabilities and natural language processing is an effective approach for quickly identifying near-synonyms, a possible source of terminological ambiguity. Method.We propose a tool-supported approach that blends information visualization with two natural language processing techniques: conceptual model extraction and semantic similarity. We evaluate the precision and recall of our approach compared to a pen-and-paper manual inspection session through a controlled quasi-experiment that involves 57 participants organized into 28 groups, each group working on one real-world requirements data set. Results.The experimental results indicate that manual inspection delivers higher recall (statistically significant with p ≤ 0.01) and non-significantly higher precision. Based on qualitative observations, we analyze the quantitative results and suggest interpretations that explain the advantages and disadvantages of each approach. Conclusions.Our experiment confirms conventional wisdom in requirements engineering: identifying terminological ambiguities is time consuming, even when with tool support; and it is hard to determine whether a near-synonym may challenge the correct development of a software system. The results suggest that the most effective approach may be a combination of manual inspection with an improved version of our tool.},
annote = {cited By 3},
author = {Dalpiaz, Fabiano and van der Schalk, Ivor and Brinkkemper, Sjaak and Aydemir, Fatma Başak and Lucassen, Garm},
doi = {10.1016/j.infsof.2018.12.007},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/DALPIA{\~{}}2.PDF:PDF},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Ambiguity,Empirical software engineering,Natural language processing,Requirements engineering,User stories,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {3--16},
title = {{Detecting terminological ambiguity in user stories: Tool and experimentation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059318780{\&}doi=10.1016{\%}2Fj.infsof.2018.12.007{\&}partnerID=40{\&}md5=1e4a4b7a18d279579992fa420240092b},
volume = {110},
year = {2019}
}
@conference{Portugal2018102,
abstract = {A challenge in requirements elicitation is to identify quality requirements, i.e. non-functional requirements (hereafter, NFR). In general, stakeholders need NFRs, but these requirements are not always explicit, they can be part of the tacit knowledge. The usual strategy adopted by requirements engineers to elicit NFRs is to act proactively by asking stakeholders their interests in qualities based on lists or catalogs. NFRFinder is a semi-automated process strategy for mining keywords. The strategy uses the keywords to find possible NFRs in unstructured texts, e.g. the meeting minutes that occur during an elicitation task. The strategy relies on catalogs, according to the NFR Framework, as a supporting knowledge base. However, to gain more confidence on the NFRFinder, we have applied it to a set of structured texts. We report on the recall and precision of NFRFinder using a gold standard built from different actors, for requirements sentences. The results are promising, and we point out towards the evolution of NFRFinder.},
annote = {cited By 3},
author = {Portugal, Roxana L.Q. and Li, Tong and Silva, Lyrene and Almentero, Eduardo and {Do Prado Leite}, Julio Cesar S.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3266237.3266269},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/PORTUG{\~{}}1.PDF:PDF},
isbn = {9781450365031},
keywords = {Knowledge-based search,Natural language processing,Non-functional requirements,Unstructured texts,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {102--111},
title = {{NFRFinder: A knowledge based strategy for mining non-functional requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055777664{\&}doi=10.1145{\%}2F3266237.3266269{\&}partnerID=40{\&}md5=a277d4d79f9b7cd3d620c2a870c95c04},
year = {2018}
}
@article{Kamalrudin2017,
abstract = {Requirements captured by requirements engineers (REs) are commonly inconsistent with their client's intended requirements and are often error prone. There is limited tool support providing end-to-end support between the REs and their client for the validation and improvement of these requirements. We have developed an automated tool called MaramaAIC (Automated Inconsistency Checker) to address these problems. MaramaAIC provides automated requirements traceability and visual support to identify and highlight inconsistency, incorrectness and incompleteness in captured requirements. MaramaAIC provides an end-to-end rapid prototyping approach together with a patterns library that helps to capture requirements and check the consistency of requirements that have been expressed in textual natural language requirements and then extracted to semi-formal abstract interactions, essential use cases (EUCs) and user interface prototype models. It helps engineers to validate the correctness and completeness of the EUCs modelled requirements by comparing them to “best-practice” templates and generates an abstract prototype in the form of essential user interface prototype models and concrete User Interface views in the form of HTML. We describe its design and implementation together with results of evaluating our tool's efficacy and performance, and user perception of the tool's usability and its strengths and weaknesses via a substantial usability study. We also present a qualitative study on the effectiveness of the tool's end-to-end rapid prototyping approach in improving dialogue between the RE and the client as well as improving the quality of the requirements.},
annote = {cited By 7},
author = {Kamalrudin, Massila and Hosking, John and Grundy, John},
doi = {10.1007/s10515-016-0192-z},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kamalrudin, Massila and Hosking, John and Grundy, John{\_} {\_}{\_}MaramaAIC{\_} tool support for consistency management and validation of requirements{\_}{\_} (2017).pdf:pdf},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Consistency management,Essential use cases,Essential user interface,Requirements validation,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {1},
title = {{MaramaAIC: tool support for consistency management and validation of requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959348202{\&}doi=10.1007{\%}2Fs10515-016-0192-z{\&}partnerID=40{\&}md5=66985c2e995cc8fe4bd83dd18184c2ea},
volume = {24},
year = {2017}
}
@article{wang_automatic_2016,
abstract = {Nowadays, software requirements are still mainly analyzed manually, which has many drawbacks (such as a large amount of labor consumption, inefficiency, and even inaccuracy of the results). The problems are even worse in domain analysis scenarios because a large number of requirements from many users need to be analyzed. In this sense, automatic analysis of software requirements can bring benefits to software companies. For this purpose, we proposed an approach to automatically analyze software requirement specifications (SRSs) and extract the semantic information. In this approach, a machine learning and ontology based semantic role labeling (SRL) method was used. First of all, some common verbs were calculated from SRS documents in the E-commerce domain, and then semantic frames were designed for those verbs. Based on the frames, sentences from SRSs were selected and labeled manually, and the labeled sentences were used as training examples in the machine learning stage. Besides the training examples labeled with semantic roles, external ontology knowledge was used to relieve the data sparsity problem and obtain reliable results. Based on the SemCor and WordNet corpus, the senses of nouns and verbs were identified in a sequential manner through the K-nearest neighbor approach. Then the senses of the verbs were used to identify the frame types. After that, we trained the SRL labeling classifier with the maximum entropy method, in which we added some new features based on word sense, such as the hypernyms and hyponyms of the word senses in the ontology. Experimental results show that this new approach for automatic functional requirements analysis is effective.},
author = {Wang, Yinglin},
doi = {10.1007/s12204-016-1783-3},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Wang, Yinglin{\_} {\_}{\_}Automatic semantic analysis of software requirements through machine learning and ontology approach{\_}{\_} (2016).pdf:pdf},
issn = {19958188},
journal = {Journal of Shanghai Jiaotong University (Science)},
keywords = {machine learning,semantic role labelling,software requirement engineering,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
number = {6},
pages = {692--701},
title = {{Automatic semantic analysis of software requirements through machine learning and ontology approach}},
url = {http://link.springer.com/10.1007/s12204-016-1783-3},
volume = {21},
year = {2016}
}
@article{Li2018324,
abstract = {[Context and motivation] In the increasingly competitive software market, it is essential for software companies to have a comprehensive understanding of development progress and user preferences of their corresponding application domain. [Question/problem] However, given the huge number of existing software applications, it is impossible to gain such insights via manual inspection. [Principal ideas/results] In this paper, we present a research preview of automatic user preferences elicitation approach. Specifically, our approach first clusters software applications into different categories based on their descriptions, and then identifies features of each category. We then link such features to corresponding user reviews and automatically classify sentiments of each review In order to understand user preferences over such feature In addition, we have carefully planned evaluations that will be carried out to further polish our work. [Contributions] Our proposal aims to help software companies to identify features of applications in a particular domain, as well as user preferences with regard to those features. We argue such analysis is especially important for startup companies that have few knowledge about the domain.},
annote = {cited By 1},
author = {Li, Tong and Zhang, Fan and Wang, Dan},
doi = {10.1007/978-3-319-77243-1_21},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Li, Tong and Zhang, Fan and Wang, Dan{\_} {\_}{\_}Automatic user preferences elicitation{\_} A data-driven approach{\_}{\_} (2018).pdf:pdf},
isbn = {9783319772424},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Machine learning,Natural language processing,Sentiment analysis,Topic modeling,User preferences,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {324--331},
title = {{Automatic user preferences elicitation: A data-driven approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043389317{\&}doi=10.1007{\%}2F978-3-319-77243-1{\_}21{\&}partnerID=40{\&}md5=b824b973757a89b8ba168cf34946cd8f},
volume = {10753 LNCS},
year = {2018}
}
@incollection{satapathy_utilizing_2016,
abstract = {UML diagrams form an important part of the software design specification. The source of these diagrams is requirement specification which is created from the user's need and requirements. In our work, we identify that two important areas in computer science and engineering, software engineering (SE) and natural language processing (NLP), form the core of this development. An algorithm for undertaking study of this approach is also presented. Herein, we also list the main usage of our technique to handle a more generalized environment such as non-software engineering domain.},
address = {Singapore},
annote = {Series Title: Advances in Intelligent Systems and Computing},
author = {Yalla, Prasanth and Sharma, Nakul},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-981-10-0767-5_7},
editor = {Satapathy, Suresh Chandra and Bhatt, Yogesh Chandra and Joshi, Amit and Mishra, Durgesh Kumar},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Yalla, Prasanth and Sharma, Nakul{\_} {\_}{\_}Utilizing NL text for generating UML diagrams{\_}{\_} (2016).pdf:pdf},
isbn = {9789811007668},
issn = {21945357},
keywords = {Computational linguistics,Natural language processing,Software engineering,UML diagrams,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {55--62},
publisher = {Springer Singapore},
title = {{Utilizing NL text for generating UML diagrams}},
url = {http://link.springer.com/10.1007/978-981-10-0767-5{\_}7},
volume = {438},
year = {2016}
}
@inproceedings{7320408,
abstract = {Goal models allow efficient representation of stakeholder goals and alternative ways by which these can be satisfied. Preferences over goals in the goal model are then used to specify criteria for selecting alternatives that fit specific contexts, situations and strategies. Given such preferences, automated reasoning tools allow for efficient exploration of such alternatives. Nevertheless, to be amenable to such automated processing, goals and preferences need to be specified in a formal language, making automated processing inaccessible to the very bearers of goals and preferences, i.e., the stakeholders. We combine natural language processing techniques to allow specification of preferences through natural language statements. The natural language statement is first matched through regular expressions to distinguish between the preference component and the goal component. The former is then mapped to a preferential strength measure, while the latter is used to identify the relevant goal in the goal model through statistical semantic similarity techniques. The result constitutes a formal representation that can be used for alternatives analysis. In this way, stakeholders can access advanced goal reasoning techniques through simple natural language preference expressions, facilitating their decision making in various requirements analysis contexts. An experimental evaluation with human participants shows that the proposed system is of substantial precision and that a mapping from natural preferential verbalizations to predefined preferential strength labels is possible through sampling from crowds.},
author = {Alabdulkareem, Fatima and Cercone, Nick and Liaskos, Sotirios},
booktitle = {2015 IEEE 23rd International Requirements Engineering Conference, RE 2015 - Proceedings},
doi = {10.1109/RE.2015.7320408},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Alabdulkareem, Fatima and Cercone, Nick and Liaskos, Sotirios{\_} {\_}{\_}Goal and Preference Identification through natural language{\_}{\_} (2015).pdf:pdf},
isbn = {9781467369053},
issn = {2332-6441},
keywords = {decision analysis,goal modeling,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language,preference analysis,requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {56--65},
title = {{Goal and Preference Identification through natural language}},
year = {2015}
}
@inproceedings{9027371,
abstract = {eLearning is gaining more ranking nowadays; eLearning systems (eLS) are in continuous need for improvements to meet its stakeholders' requirements. Traditional requirements elicitation techniques can't satisfy the continuous requirements of eLearning stakeholders. Crowdsourcing is an emerging concept in the requirements elicitation, an approach of requirements elicitation based on the crowdsourcing concept for eLS is discussed. In this paper the approach is further evaluated using bi-gram topic modeling. This will assess the approach validity to better extract eLearning stakeholders' requirements and help in the requirements elicitation and evolution of eLS. The bi-gram evaluation was applied on three LMS products and the results were compared with the results of LDA algorithm extraction and with the manual extraction of the requirements. The average results of bigram model were 0.68 f-measure, 0.76 precision, and 0.61 recall. The extracted keywords using bi-gram were better than normal LDA algorithm, relevant and can help in requirements evolution of the eLS.},
author = {Rizk, Nancy M. and Nasr, Eman S. and Gheith, Mervat H.},
booktitle = {ICENCO 2019 - 2019 15th International Computer Engineering Conference: Utilizing Machine Intelligence for a Better World},
doi = {10.1109/ICENCO48310.2019.9027371},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/rizk Enhancing CREeLS the crowdsourcing based requirements elicitation approach for elearning systems using bi-gram evaluation.pdf:pdf},
isbn = {9781728151465},
issn = {2475-2320},
keywords = {Crowdsourcing,Requirements elicitation,Topic Modelling,eLearning,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {222--226},
title = {{Enhancing CREeLS the crowdsourcing based requirements elicitation approach for elearning systems using bi-gram evaluation}},
year = {2019}
}
@inproceedings{8486143,
abstract = {Requirements engineering plays an important role in quality assurance, which is especially important for complex, embedded, safety-related systems. Such systems are often subject to additional regulations regarding functional safety such as ISO 26262 norm for road vehicles or EN 50128 for railway industry. Verifying quality of the requirements is a first step both for validation and verification of the system under test. This paper presents a review of the existing methods of automatic detection of the ambiguity and automatic assessment of the requirements quality together with the possible, future fields of research.},
author = {Kocerka, Jerzy and Krzeslak, Michal and Galuszka, Adam},
booktitle = {2018 23rd International Conference on Methods and Models in Automation and Robotics, MMAR 2018},
doi = {10.1109/MMAR.2018.8486143},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/kocerka Analysing Quality of Textual Requirements Using Natural Language Processing A Literature Review.pdf:pdf},
isbn = {9781538643259},
keywords = {Natural Language Parsing,Requirements Engineering,Software Engineering,Software Testing,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {876--880},
title = {{Analysing Quality of Textual Requirements Using Natural Language Processing: A Literature Review}},
year = {2018}
}
@inproceedings{10.1145/2896995.2896998,
abstract = {This paper focuses on the problem of generating human interpretable clusters of semantically related plain-text requirements. Presented approach applies techniques from information retrieval, natural language processing, network analysis, and machine learning for identifying semantically central terms as themes and clustering requirements into semantically coherent groups together with meaningful explanatory themes associated with the clusters to assist in user comprehension of the clusters. Presented approach is generic in nature and can be used for other phases of SDLC (Software Development Life Cycle) including code-comprehension and architectural discovery. Suggested approach is particularly suitable for developing automated tool support for requirements management and analysis.},
address = {New York, NY, USA},
author = {Misra, Janardan and Sengupta, Shubhashis and Podder, Sanjay},
booktitle = {Proceedings - 5th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2016},
doi = {10.1145/2896995.2896998},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Misra, Janardan and Sengupta, Shubhashis and Podder, Sanjay{\_} {\_}{\_}Topic cohesion preserving requirements clustering{\_}{\_} (2016).pdf:pdf},
isbn = {9781450341653},
keywords = {Clustering Interpretation Problem,Latent Semantic Analysis,Network Centrality,Requirements Analysis,Requirements Clustering,Requirements Management,Theme Mining,acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {22--28},
publisher = {Association for Computing Machinery},
series = {RAISE '16},
title = {{Topic cohesion preserving requirements clustering}},
url = {https://doi.org/10.1145/2896995.2896998},
year = {2016}
}
@inproceedings{8491144,
abstract = {Requirements are usually 'hand-written' and suffers from several problems like redundancy and inconsistency. The problems of redundancy and inconsistency between requirements or sets of requirements impact negatively the success of final products. Manually processing these issues requires too much time and it is very costly. The main contribution of this paper is the use of k-means algorithm for a redundancy and inconsistency detection in a new context, which is Requirements Engineering context. Also, we introduce a filtering approach to eliminate 'noisy' requirements and a preprocessing step based on the Natural Language Processing (NLP) technique to see the impact of this latter on the k-means results. We use Part-Of-Speech (POS) tagging and noun chunking to detect technical business terms associated to the requirements documents that we analyze. We experiment this approach on real industrial datasets. The results show the efficiency of the k-means clustering algorithm, especially with the filtering and preprocessing steps. Our approach is using the software SEMIOS and will be integrated as a new functionality.},
author = {Mezghani, Manel and Kang, Juyeon and Sedes, Florence},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00037},
file = {:C$\backslash$:/Users/aaberkan/Documents/bibtex-fulltext-downloader/Mezghani, Manel and Kang, Juyeon and Sedes, Florence{\_} {\_}{\_}Industrial requirements classification for redundancy and inconsistency detection in SEMIOS{\_}{\_} (2018).pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {Clustering,Inconsistency,NLP,Redundancy,Requirements engineering,Technical documents,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {297--303},
title = {{Industrial requirements classification for redundancy and inconsistency detection in SEMIOS}},
year = {2018}
}
@inproceedings{8501304,
abstract = {Requirements elicitation requires extensive knowledge and deep understanding of the problem domain where the final system will be situated. However, in many software development projects, analysts are required to elicit the requirements from an unfamiliar domain, which often causes communication barriers between analysts and stakeholders. In this paper, we propose a requirements ELICitation Aid tool (ELICA) to help analysts better understand the target application domain by dynamic extraction and labeling of requirements-relevant knowledge. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural language processing tasks. In addition to the information conveyed through text, ELICA captures and processes non-linguistic information about the intention of speakers such as their confidence level, analytical tone, and emotions. The extracted information is made available to the analysts as a set of labeled snippets with highlighted relevant terms which can also be exported as an artifact of the Requirements Engineering (RE) process. The application and usefulness of ELICA are demonstrated through a case study. This study shows how pre-existing relevant information about the application domain and the information captured during an elicitation meeting, such as the conversation and stakeholders' intentions, can be captured and used to support analysts achieving their tasks.},
archivePrefix = {arXiv},
arxivId = {1808.05857},
author = {{Shakeri Hossein Abad}, Zahra and Gervasi, Vincenzo and Zowghi, Didar and Barker, Ken},
booktitle = {Proceedings - 2018 5th International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2018},
doi = {10.1109/AIRE.2018.00007},
eprint = {1808.05857},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/shakeri ELICA An Automated Tool for Dynamic Extraction of Requirements Relevant Information.pdf:pdf},
isbn = {9781538684047},
keywords = {Dynamic information extraction,Natural language processing,Requirements elicitation,Tool support,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {8--14},
title = {{ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant Information}},
year = {2018}
}
@inproceedings{10.1145/1858996.1859007,
abstract = {Natural language is prevalent in requirements documents. However, ambiguity is an intrinsic phenomenon of natural language, and is therefore present in all such documents. Ambiguity occurs when a sentence can be interpreted differently by different readers. In this paper, we describe an automated approach for characterizing and detecting so-called nocuous ambiguities, which carry a high risk of misunderstanding among different readers. Given a natural language requirements document, sentences that contain specific types of ambiguity are first extracted automatically from the text. A machine learning algorithm is then used to determine whether an ambiguous sentence is nocuous or innocuous, based on a set of heuristics that draw on human judgments, which we collected as training data. We implemented a prototype tool for Nocuous Ambiguity Identification (NAI), in order to illustrate and evaluate our approach. The tool focuses on coordination ambiguity. We report on the results of a set of experiments to assess the performance and usefulness of the approach.},
address = {New York, NY, USA},
author = {Yang, Hui and Willis, Alistair and {De Roeck}, Anne and Nuseibeh, Bashar},
booktitle = {Proceedings of the IEEE/ACM International Conference on Automated Software Engineering},
doi = {10.1145/1858996.1859007},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Yang, Hui and Willis, Alistair and {\_}De Roeck{\_}, Anne and Nuseibeh, Bashar{\_} {\_}{\_}Automatic Detection of Nocuous Coordination Ambiguities in Natural Language Requirements{\_}{\_} (2010).pdf:pdf},
isbn = {9781450301169},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,coordination ambiguity,human judgments,machine learning,natural language requirements,nocuous ambiguity},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {53--62},
publisher = {Association for Computing Machinery},
series = {ASE '10},
title = {{Automatic Detection of Nocuous Coordination Ambiguities in Natural Language Requirements}},
url = {https://doi.org/10.1145/1858996.1859007},
year = {2010}
}
@conference{Shah2016,
abstract = {When specifying user requirements, not only it is critical to ensure correct and unambiguous specification of functional requirements, but also that of non-functional requirements (NFRs). In fact, resolving ambiguities from user specified natural language NFRs and specifying the correct ones in a formal language have attracted significant attention. Our current research focuses on the issues pertaining the same. We observe that it is a usual practice for a user to narrate the NFRs in natural language and the requirement engineers manually try to express the same, using some semi-formal or formal language notations. However, inaccurate and the laborious manual approach may fail to detect all the NFRs and correctly remove the ambiguities in those detected. Hence, current research attempts have focused on automating the conversion of natural language NFRs to formal notations. In literature, there exist numerous approaches that take requirements as input and output the extended UML counterpart including NFRs. However, majority of the approaches do not support ambiguity resolution and verification of the extracted NFRs that are fairly essential. In this paper, we propose and discuss a hybrid approach viz. NFRs-Specifier, that attempts to resolve ambiguities, extract NFR's, perform verification and generate NFRs specification by means of the extended UML model.},
annote = {cited By 0},
author = {Shah, Unnati S. and Patel, Sankita J. and Jinwala, Devesh C.},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/shah Specification of non-functional requirements A hybrid approach.pdf:pdf},
issn = {16130073},
keywords = {Ambiguity,Natural language processing,Non-functional requirements,Ontology,Requirements classification,Requirements engineering,Unified modeling language,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Specification of non-functional requirements: A hybrid approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964575549{\&}partnerID=40{\&}md5=3551bbde45ba074bd30eaf0c1b1ac7a0},
volume = {1564},
year = {2016}
}
@article{Ferrari2019,
abstract = {During requirements elicitation, different stakeholders with diverse backgrounds and skills need to effectively communicate to reach a shared understanding of the problem at hand. Linguistic ambiguity due to terminological discrepancies may occur between stakeholders that belong to different technical domains. If not properly addressed, ambiguity can create frustration and distrust during requirements elicitation meetings, and lead to problems at later stages of development. This paper presents a natural language processing approach to identify ambiguous terms between different domains, and rank them by ambiguity score. The approach is based on building domain-specific language models, one for each stakeholders' domain. Word embeddings from each language model are compared in order to measure the differences of use of a term, thus estimating its potential ambiguity across the domains of interest. We evaluate the approach on seven potential elicitation scenarios involving five domains. In the evaluation, we compare the ambiguity rankings automatically produced with the ones manually obtained by the authors as well as by multiple annotators recruited through Amazon Mechanical Turk. The rankings produced by the approach lead to a maximum Kendall's Tau of 88{\%}. However, for several elicitation scenarios, the application of the approach was unsuccessful in terms of performance. Analysis of the agreement among annotators and of the observed inaccuracies offer hints for further research on the relationship between domain knowledge and natural language ambiguity.},
annote = {cited By 1},
author = {Ferrari, Alessio and Esuli, Andrea},
doi = {10.1007/s10515-019-00261-7},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ferrari, Alessio and Esuli, Andrea{\_} {\_}{\_}An NLP approach for cross-domain ambiguity detection in requirements engineering{\_}{\_} (2019).pdf:pdf},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Ambiguity,Domain knowledge,Language models,NLP,Natural language processing,Requirements engineering,Word embeddings,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
title = {{An NLP approach for cross-domain ambiguity detection in requirements engineering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067856455{\&}doi=10.1007{\%}2Fs10515-019-00261-7{\&}partnerID=40{\&}md5=47be8a4d0dbbaf4c4e6a70a862668a21},
year = {2019}
}
@conference{Khelifa2018,
abstract = {Requirements are the basis for all software projects. Thus, the requirements phase needs much more attention in order to specify the problems that the software system is intended to solve. However, identifying correctly and completely the software requirements encompasses many issues due mainly to their inconsistencies, ambiguities, incompleteness, and instability. In addition, requirements change requests are inevitable during the software life-cycle (SLC). Change request expressed in natural language format are hard to analyze since they may affect different types of software requirements. To provide an appropriate response to a change request, this paper aims to: (i) investigate how well machine learning techniques are used in the classification of software requirements as well as requirements change requests, and (ii) give an overview of our research that proposes to use the natural language processing and Support Vector Machine (SVM) classifier to automatically classify the requirements change requests into mainly two categories (functional change and technical change).},
annote = {cited By 0},
author = {Khelifa, Amani and Haoues, Mariem and Sellami, Asma},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/khelifa Towards a software requirements change classification using support vector machine.pdf:pdf},
issn = {16130073},
keywords = {Classification,Functional change,Machine learning,Natural language processing,Requirements change requests,Software requirements,Technical change,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Towards a software requirements change classification using support vector machine}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059822763{\&}partnerID=40{\&}md5=aa0b4508d3111ceae2ba24f1fa16946a},
volume = {2279},
year = {2018}
}
@article{Carlson201477,
abstract = {In the late 1990s the National Aeronautics and Space Administration (NASA) Software Assurance Technology Center (SATC) developed a tool to automatically analyze a requirements document and produce a detailed quality report. The report was based on statistical analysis of word frequencies at various structural levels of the document. The Automated Requirements Measurement (ARM) tool was further enhanced to include additional functionality such as custom definitions of quality indicators inputs for document analysis. By 2011 work on the ARM tool was discontinued. This paper describes the reverse-engineering and reproduction of the functionality of ARM. Recreating the functionality of this tool yielded valuable insight into certain quality metrics and provides a benchmark tool for future research. In addition to recreating and working with the ARM tool, this paper explores both existing and potential definitions of quality metrics in requirements specifications. Automated requirements analysis is a convergence of various fields of research, including text mining, quality analysis, and natural language processing. Informed by tangential areas of research in document understanding and data mining, recommendations are made for future areas of research and development in automated requirements analysis. {\textcopyright} 2013 Springer-Verlag London.},
annote = {cited By 17},
author = {Carlson, Nathan and Laplante, Phil},
doi = {10.1007/s11334-013-0225-8},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Carlson-Laplante2014{\_}Article{\_}TheNASAAutomatedRequirementsMe.pdf:pdf},
issn = {16145054},
journal = {Innovations in Systems and Software Engineering},
keywords = {Requirements engineering,Software quality,Text processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {2},
pages = {77--91},
title = {{The NASA automated requirements measurement tool: A reconstruction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900478117{\&}doi=10.1007{\%}2Fs11334-013-0225-8{\&}partnerID=40{\&}md5=6383792c56ec40a2ae076e5512457319},
volume = {10},
year = {2014}
}
@article{Ferrari201423,
abstract = {[Context and motivation] System requirements specifications are normally written in natural language. These documents are required to be complete with respect to the input documents of the requirements definition phase, such as preliminary specifications, transcripts of meetings with the customers, etc. In other terms, they shall include all the relevant concepts and all the relevant interactions among concepts expressed in the input documents. [Question/Problem] Means are required to measure and improve the completeness of the requirements with respect to the input documents. [Principal idea/results] To measure this completeness, we propose two metrics that take into account the relevant terms of the input documents, and the relevant relationships among terms. Furthermore, to improve the completeness, we present a natural language processing tool named Completeness Assistant for Requirements (CAR), which supports the definition of the requirements: the tool helps the requirements engineer in discovering relevant concepts and interactions. [Contribution] We have performed a pilot test with CAR, which shows that the tool can help improving the completeness of the requirements with respect to the input documents. The study has also shown that CAR is actually useful in the identification of specific/alternative system behaviours that might be overseen without the tool. {\textcopyright} 2014 Springer International Publishing Switzerland.},
annote = {cited By 26},
author = {Ferrari, Alessio and Dell'Orletta, Felice and Spagnolo, Giorgio Oronzo and Gnesi, Stefania},
doi = {10.1007/978-3-319-05843-6_3},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ferrari, Alessio and Dell'Orletta, Felice and Spagnolo, Giorgio Oronzo and Gnesi, Stefania{\_} {\_}{\_}Measuring and improving the completeness of natural language requirements{\_}{\_} (2014).pdf:pdf},
isbn = {9783319058429},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Requirements analysis,natural language processing,relation extraction,requirements completeness,requirements quality,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,terminology extraction},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {23--38},
title = {{Measuring and improving the completeness of natural language requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958531874{\&}doi=10.1007{\%}2F978-3-319-05843-6{\_}3{\&}partnerID=40{\&}md5=e12e5f0f8b0ac36214404f3b7927e0d5},
volume = {8396 LNCS},
year = {2014}
}
@inproceedings{10.1145/2664243.2664280,
abstract = {With over forty years of use and refinement, access control, often in the form of access control rules (ACRs), continues to be a significant control mechanism for information security. However, ACRs are typically either buried within existing natural language (NL) artifacts or elicited from subject matter experts. To address the first situation, our research goal is to aid developers who implement ACRs by inferring ACRs from NL artifacts. To aid in rule inference, we propose an approach that extracts relations (i.e., the relationship among two or more items) from NL artifacts such as requirements documents. Unlike existing approaches, our approach combines techniques from information extraction and machine learning. We develop an iterative algorithm to discover patterns that represent ACRs in sentences. We seed this algorithm with frequently occurring nouns matching a subject action resource pattern throughout a document. The algorithm then searches for additional combinations of those nouns to discover additional patterns. We evaluate our approach on documents from three systems in three domains: conference management, education, and healthcare. Our evaluation results show that ACRs exist in 47{\%} of the sentences, and our approach effectively identifies those ACR sentences with a precision of 81{\%} and recall of 65{\%}; our approach extracts ACRs from those identified ACR sentences with an average precision of 76{\%} and an average recall of 49{\%}.},
address = {New York, NY, USA},
author = {Slankas, John and Xiao, Xusheng and Williams, Laurie and Xie, Tao},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2664243.2664280},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Slankas, John and Xiao, Xusheng and Williams, Laurie and Xie, Tao{\_} {\_}{\_}Relation extraction for inferring access control rules from natural language artifacts{\_}{\_} (2014).pdf:pdf},
isbn = {9781450330053},
keywords = {Access control,Classification,Natural language parsing,Security,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
number = {December},
pages = {366--375},
publisher = {Association for Computing Machinery},
series = {ACSAC '14},
title = {{Relation extraction for inferring access control rules from natural language artifacts}},
url = {https://doi.org/10.1145/2664243.2664280},
volume = {2014-Decem},
year = {2014}
}
@inproceedings{8048909,
abstract = {Vagueness in software requirements documents can lead to several maintenance problems, especially when the customer and development team do not share the same language. Currently, companies rely on human translators to maintain communication and limit vagueness by translating the requirement documents by hand. In this paper, we describe two approaches that automatically identify vagueness in requirements documents in a multilingual environment. We perform two studies for calibration purposes under strict industrial limitations, and describe the tool that we ultimately deploy. In the first study, six participants, two native Portuguese speakers and four native Spanish speakers, evaluated both approaches. Then, we conducted a field study to test the performance of the best approach in real-world environments at two companies. We describe several lessons learned for research and industrial deployment.},
author = {Cruz, Breno Dantas and Jayaraman, Bargav and Dwarakanath, Anurag and McMillan, Collin},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.24},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/CRUZ{\_}B{\~{}}1.PDF:PDF},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {Multilingual,Natural Language Processing,User Study,Vagueness Detection,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {233--242},
title = {{Detecting Vague Words {\&} Phrases in Requirements Documents in a Multilingual Environment}},
year = {2017}
}
@conference{Motger2019,
abstract = {Requirements Engineering (RE) is one of the most critical phases in software development. Analyzing requirements data is a laborious task performed by expert stakeholders using manual processes, as there are no standard automatic tools to handle this issue in a more efficient way. The purpose of this paper is to summarize the approach of the OpenReq-DD dependency detection tool developed at the OpenReq project, which allows an automatic requirement dependency detection approach. The core of this proposal is based on an ontology which defines dependency relations between specific terminologies related to the domain of the requirements. Using this information, it is possible to apply Natural Language Processing techniques to extract meaning from these requirements and relations, and Machine Learning techniques to apply conceptual clustering, with the major purpose of classifying these requirements into the defined ontology.},
annote = {cited By 0},
author = {Motger, Quim and Borrull, Ricard and Palomares, Cristina and Marco, Jordi},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/motger OpenReq-DD A requirements dependency detection tool.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{OpenReq-DD: A requirements dependency detection tool}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068078945{\&}partnerID=40{\&}md5=ea47cd3a67c540710b007e2a897a4ff0},
volume = {2376},
year = {2019}
}
@inproceedings{8080002,
abstract = {The accomplishment of any software system success depends on how well it meets the requirements of the stakeholders. These requirements are elicited from the customers. Software requirements are unambiguous if and only if it has one meaning. The elicited requirements are documented in software requirements specification document and these requirements are written in natural languages. Natural languages are basically ambiguous which makes the requirements documented in software requirements specification document unclear. This unclear requirement causes that software developers develop software which is different from the expected software based on the customer needs. The objective of this paper is to propose a framework that are able to detect ambiguity in software requirements specification document automatically using parts of speech tagging technique. To validate the outcome of the proposed work, open source software requirements specification documents will be used and generated result of the proposed work will be evaluated and validated by making comparison between the proposed prototype results and human generated results to decide how the proposed prototype can solved the ambiguity problem.},
author = {ale Sabriye, Ali Olow Jim and Zainon, Wan Mohd Nazmee Wan},
booktitle = {ICIT 2017 - 8th International Conference on Information Technology, Proceedings},
doi = {10.1109/ICITECH.2017.8080002},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ale Sabriye, Ali Olow Jim and Zainon, Wan Mohd Nazmee Wan{\_} {\_}{\_}A framework for detecting ambiguity in software requirement specification{\_}{\_} (2017).pdf:pdf},
isbn = {9781509063321},
keywords = {ambiguity detection,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirements engineering,software requirements specification},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {209--213},
title = {{A framework for detecting ambiguity in software requirement specification}},
year = {2017}
}
@article{Körner201092,
abstract = {This paper proposes an approach which utilizes natural language processing (NLP) and ontology knowledge to automatically denote the implicit semantics of textual requirements. Requirements documents include the syntax of natural language but not the semantics. Semantics are usually interpreted by the human user. In earlier work Gelhausen and Tichy showed that SalEMX automatically creates UML domain models from (semantically) annotated textual specifications [1]. This manual annotation process is very time consuming and can only be carried out by annotation experts. We automate semantic annotation so that SalEMX can be completely automated. With our approach, the analyst receives the domain model of a requirements specification in a very fast and easy manner. Using these concepts is the first step into farther automation of requirements engineering and software development. {\textcopyright} 2010 Springer-Verlag.},
annote = {cited By 4},
author = {K{\"{o}}rner, Sven J. and Landh{\"{a}}u{\ss}er, Mathias},
doi = {10.1007/978-3-642-13881-2_9},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/korner Semantic{\_}Enriching{\_}of{\_}Natural{\_}Language{\_}T.pdf:pdf},
isbn = {3642138802},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {92--99},
title = {{Semantic enriching of natural language texts with automatic thematic role annotation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955461025{\&}doi=10.1007{\%}2F978-3-642-13881-2{\_}9{\&}partnerID=40{\&}md5=9351b8b7b57b5e2004267f5d4d106cd9},
volume = {6177 LNCS},
year = {2010}
}
@conference{Yildiz2014358,
abstract = {In the software engineering world, creating and maintaining relationships between byproducts generated during the software lifecycle is crucial. A typical relation is the one that exists between an item in the requirements document and a block in the subsequent system design, i.e. class in the source code. In many software engineering projects, the requirement documentation is prepared in the language of the developers, whereas developers prefer to use the English language in the software development process. In this paper, we use the vector space model to extract traceability links between the requirements written in one language (Turkish) and the implementations of classes in another language (English). The experiments show that, by using a generic translator such as Google translate, we can obtain promising results, which can also be improved by using comment info in the source code. Copyright {\textcopyright} 2014 SCITEPRESS.},
annote = {cited By 0},
author = {Yildiz, Olcay Taner and Okutan, Ahmet and Solak, Ercan},
booktitle = {ICPRAM 2014 - Proceedings of the 3rd International Conference on Pattern Recognition Applications and Methods},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/yildiz ICPRAMConferencePaper-BilingualSoftwareRequirementsTracingusingVectorSpaceModel.pdf:pdf},
isbn = {9789897580185},
keywords = {NLP,Requirements tracing,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {358--363},
title = {{Bilingual software requirements tracing using vector space model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902346459{\&}partnerID=40{\&}md5=f1cddaf4f376321b3b4e7022d40cad34},
year = {2014}
}
@inproceedings{8754006,
abstract = {Among the requirements elicitation activities, the stakeholder analysis is the main source of requirements. In this article, we propose a new model of data-driven stakeholder analysis, named SIG (Stakeholder Intention Graph), a semantic extension of property graph model that can represent the stakeholders' intentions and their relationships. To elicit the stakeholders' intentions from the speech data during meetings, we developed a system of structural analysis and SIG generation method from speech data. Based on the graph theory, we also propose an analysis methodology of stakeholders' intentions and their structure with both global and local graph analyses. We implemented a speech data-driven stakeholder analysis system on the graph database Neo4j. As the output, the analysis system automatically generates the stakeholder matrix from the speech data at the meetings. We applied the analysis method and system to the speech data of actual development meetings on the public service systems, and demonstrated the effectiveness of the proposed method.},
author = {Shirasaki, Yuta and Kobayashi, Yuya and Aoyama, Mikio},
booktitle = {Proceedings - International Computer Software and Applications Conference},
doi = {10.1109/COMPSAC.2019.10209},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Shirasaki, Yuta and Kobayashi, Yuya and Aoyama, Mikio{\_} {\_}{\_}A speech data-driven stakeholder analysis methodology based on the stakeholder graph models{\_}{\_} (2019).pdf:pdf},
isbn = {9781728126074},
issn = {07303157},
keywords = {Data-Driven,Graph Analysis,Graph Database,Requirements Elicitation,Semantic Graph Model,Stakeholder Analysis,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {jul},
pages = {213--220},
title = {{A speech data-driven stakeholder analysis methodology based on the stakeholder graph models}},
volume = {2},
year = {2019}
}
@article{10.1145/1921532.1921547,
abstract = {Requirement engineering document (IEEE-830: 1998) plays a very significant role in software development. The size and complexity of software systems are continuously increasing. As scale changes to more complex and larger systems, new problems occur that did not exist in smaller systems This leads to redefining priorities of the activities that go into developing software. As systems ges complex, it becomes evident that the goals of the entire system can't be easily comprehended. Hence the need of more rigorous requirement analysis arises. The requirement analyst has to identify the requirements by using various methods like interviews, brainstorming, FAST (facilitated application specification techniques), quality function deployment, use-case etc. To overcome these issues, this paper proposes object based semi-automated system that categorize the requirements and further identifies the component based on requirement engineering document in a component library and further analyses the complexity of components and its usage.},
address = {New York, NY, USA},
author = {Sharma, Ashish and Kushwaha, Dharmender Singh},
doi = {10.1145/1921532.1921547},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sharma, Ashish and Kushwaha, Dharmender Singh{\_} {\_}{\_}Natural language based component extraction from requirement engineering document and its complexity analysis{\_}{\_} (2011).pdf:pdf},
issn = {0163-5948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,input output complexity,interface complexity,personal complexity attributes,product complexity,requirement based complexity,requirement complexity,user location complexity},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
number = {1},
pages = {1--14},
publisher = {Association for Computing Machinery},
title = {{Natural language based component extraction from requirement engineering document and its complexity analysis}},
url = {https://doi.org/10.1145/1921532.1921547},
volume = {36},
year = {2011}
}
@article{8506379,
abstract = {To understand requirements traceability in practice, we contribute, in this paper, an automated approach to identifying questions from requirements repositories and examining their answering status. Applying our approach to 345 open-source projects results in 20 622 questions, among which 53{\%} and 15{\%} are classified as successfully and unsuccessfully answered, respectively. By constructing a novel requirements socio-technical graph, we explore the impact of stakeholder-artifact relationships on traceability. The number of people, surprisingly, has little influence compared to other graph-theoretic measures like the clustering coefficient. Based on the repository mining results, we formulate a set of novel hypotheses about traceability. A case study supports some hypotheses while offering new insights.},
author = {Niu, Nan and Wang, Wentao and Gupta, Arushi and Assarandarban, Mona and Xu, Li Da and Savolainen, Juha and Cheng, Jing Ru C.},
doi = {10.1109/TCSS.2018.2872059},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/niu Requirements Socio-Technical Graphs for Managing Practitioners' Traceability Questions.pdf:pdf},
issn = {2329924X},
journal = {IEEE Transactions on Computational Social Systems},
keywords = {Practitioner questions,Requirements socio-technical graph (RSTG),Requirements traceability,Social internet of things (SIoT),ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
number = {4},
pages = {1152--1162},
title = {{Requirements Socio-Technical Graphs for Managing Practitioners' Traceability Questions}},
volume = {5},
year = {2018}
}
@incollection{hammami_requirements_2012,
abstract = {Requirements Engineering is a discipline that has been promoted, implemented and deployed for more than 20 years through the impulsion of standardization agencies (IEEE, ISO, ECSS,⋯) and national / international organizations such as AFIS, GfSE, INCOSE. Ever since, despite an increasing maturity, the Requirements Engineering discipline remains unequally understood and implemented, even within one same organization. The challenges faced today by industry include: "How to explain and make understandable the fundamentals of Requirements Engineering", "How to be more effective in Requirements authoring", "How to reach a Lean Requirements Engineering, in particular with improved knowledge management and the extensive use of modeling techniques". This paper focuses on requirements verification practices in the Industry. It gives some results of a study made end of 2010 about Requirements Engineering practices in different industrial sectors. Twenty-two companies worldwide were involved in this study through interviews and questionnaires. Current requirements verification practices are presented. It gives also some feedbacks of the use of innovative requirements authoring and verification techniques and tools in the industry. In particular, it addresses the use of Natural Language Processing (NLP) at the lexical level for correctness verification (on the form, not on the substance) of requirements, the use of Requirements boilerplates controlled by NLP for guiding requirements writing and checking, the use of Ontologies with NLP to verify requirements consistency, and the application of Information Retrieval techniques for requirements overlapping. {\textcopyright} 2012 Springer Berlin Heidelberg.},
address = {Berlin, Heidelberg},
author = {Fanmuy, Gauthier and Fraga, Anabel and Llorens, Juan},
booktitle = {Proceedings of the 2nd International Conference on Complex Systems Design and Management, CSDM 2011},
doi = {10.1007/978-3-642-25203-7_10},
editor = {Hammami, Omar and Krob, Daniel and Voirin, Jean-Luc},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Fanmuy, Gauthier and Fraga, Anabel and Llorens, Juan{\_} {\_}{\_}Requirements verification in the industry{\_}{\_} (2011).pdf:pdf},
isbn = {9783642252020},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {145--160},
publisher = {Springer Berlin Heidelberg},
title = {{Requirements verification in the industry}},
url = {http://link.springer.com/10.1007/978-3-642-25203-7{\_}10},
year = {2011}
}
@inproceedings{ISI:000495359000017,
abstract = {Often, when requirements are written, parts of the domain knowledge are assumed by the domain experts and not formalized in writing, but nevertheless used to build software artifacts. This issue, known as tacit knowledge, affects the performance of Traceability Links Recovery. Through this work we propose LORE, a novel approach that uses Natural Language Processing techniques along with an Ontological Requirements Expansion process to minimize the impact of tacit knowledge on TLR over process models. We evaluated our approach through a real-world industrial case study, comparing its outcomes against those of a baseline. Results show that our approach retrieves improved results for all the measured performance indicators. We studied why this is the case, and identified some issues that affect LORE, leaving room for improvement opportunities. We make an open-source implementation of LORE publicly available in order to facilitate its adoption in future studies.},
address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
annote = {31st Int Conf Adv Informat Syst Engn / 7th Int Workshop Cognit Aspects
of Informat Syst Engn / 1st Int Workshop on Key Enabling Technologies
for Digital Factories / Joint Workshop on Blockchains for Inter-Org
Collaborat and Flexible Adv Informat Syst, Rome, ITALY, JUN 03-07, 2019},
author = {Lape{\&}{\#}x00F1;a, Ra{\&}{\#}x00FA;l and P{\&}{\#}x00E9;rez, Francisca and Cetina, Carlos and Pastor, Ascar},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-21290-2_17},
editor = {{Giorgini, P and Weber}, B},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/LAPE{\_}{\_}{\~{}}1.PDF:PDF},
isbn = {9783030212896},
issn = {16113349},
keywords = {Business Process Models,Requirements Engineering,Traceability Links Recovery,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {261--275},
publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
series = {Lecture Notes in Computer Science},
title = {{Improving Traceability Links Recovery in Process Models Through an Ontological Expansion of Requirements}},
type = {Proceedings Paper},
volume = {11483 LNCS},
year = {2019}
}
@article{Bajwa2011217,
abstract = {Requirements are typically specified in natural languages (NL) such as English and then analyzed by analysts and developers to generate formal software design/model. However, English is ambiguous and the requirements specified in English can result in erroneous and absurd software designs. We propose a semantically controlled representation based on SBVR for specifying requirements. The SBVR based controlled representation can not only result in accurate and consistent software models but also machine process able because SBVR has pure mathematical foundation. We also introduce a java based implementation of the presented approach that is a proof of concept. {\textcopyright} 2011 Springer-Verlag.},
annote = {cited By 7},
author = {Bajwa, Imran Sarwar and {Asif Naeem}, M.},
doi = {10.1007/978-3-642-22327-3_23},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/bajwa On specifying requirements using a semantically controlled representation.pdf:pdf},
isbn = {9783642223266},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Natural Language Processing,Requirement Specifications,SBVR,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {217--220},
title = {{On specifying requirements using a semantically controlled representation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959644824{\&}doi=10.1007{\%}2F978-3-642-22327-3{\_}23{\&}partnerID=40{\&}md5=01d07c349a4df340845e5056a2db83ed},
volume = {6716 LNCS},
year = {2011}
}
@inproceedings{6894849,
abstract = {This paper presents an approach for pragmatic ambiguity detection in natural language requirements. Pragmatic ambiguities depend on the context of a requirement, which includes the background knowledge of the reader: different backgrounds can lead to different interpretations. The presented approach employs a graph-based modelling of the background knowledge of different readers, and uses a shortest-path search algorithm to model the pragmatic interpretation of a require-ment. The comparison of different pragmatic interpretations is used to decide if a requirement is ambiguous or not. The paper also provides a case study on real-world requirements, where we have assessed the effectiveness of the approach.},
author = {Ferrari, Alessio and Lipari, Giuseppe and Gnesi, Stefania and Spagnolo, Giorgio O.},
booktitle = {2014 IEEE 1st International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2014 - Proceedings},
doi = {10.1109/AIRE.2014.6894849},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ferrari, Alessio and Lipari, Giuseppe and Gnesi, Stefania and Spagnolo, Giorgio O.{\_} {\_}{\_}Pragmatic ambiguity detection in natural language requirements{\_}{\_} (2014).pdf:pdf},
isbn = {9781479963553},
keywords = {graph theory,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,search pr},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {1--8},
title = {{Pragmatic ambiguity detection in natural language requirements}},
year = {2014}
}
@inproceedings{10.1145/2593812.2593817,
abstract = {Bad requirements quality can have expensive consequences during the software development lifecycle. Especially, if it- erations are long and feedback comes late - The faster a problem is found, the cheaper it is to fix. We propose to detect issues in requirements based on re- quirements (bad) smells by applying a light-weight static requirements analysis. This light-weight technique allows for instant checks as soon as a requirement is written down. In this paper, we derive a set of smells, including automatic smell detection, from the natural language criteria of the ISO/IEC/IEEE 29148 standard. We evaluated the approach with 336 requirements and 53 use cases from 9 specifications that were written by the car manufacturer Daimler AG and the chemical business companyWacker Chemie AG, and discussed the results with their requirements and domain experts. While not all problems can be detected, the case study shows that lightweight smell analysis can uncover many practically relevant requirements defects. Based on these results and the discussion with our industry partners, we conclude that requirements smells can serve as an effcient supplement to traditional reviews or team discussions, in order to create fast feedback on requirements quality.},
address = {New York, NY, USA},
author = {Femmer, Henning and Fern{\'{a}}ndez, Daniel M{\'{e}}ndez and Juergens, Elmar and Klose, Michael and Zimmer, Ilona and Zimmer, J{\"{o}}rg},
booktitle = {1st International Workshop on Rapid Continuous Software Engineering, RCoSE 2014 - Proceedings},
doi = {10.1145/2593812.2593817},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/FEMMER{\~{}}1.PDF:PDF},
isbn = {9781450328562},
keywords = {Analytical quality assurance,Requirements engineering,Requirements smells,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {10--19},
publisher = {Association for Computing Machinery},
series = {RCoSE 2014},
title = {{Rapid requirements checks with requirements smells: Two case studies}},
url = {https://doi.org/10.1145/2593812.2593817},
year = {2014}
}
@conference{Gilson201861,
abstract = {User stories are increasingly adopted as the basis of requirement engineering artefacts in Agile Software Development. Surveys have shown that user stories are perceived as being effective at describing the main goals of a system. But the continuous management of a product backlog may be particularly time-consuming and error-prone, especially when assessing the quality or scope of user stories and keeping an eye on the system's big picture. On the other hand, models have been recognised as effective tools for communication and analysis purposes. In this research, we propose a generative approach to create robustness diagrams, i.e. a form of semi-formal use case scenarios, from the automated analysis of user stories. Stories are transformed into diagrams, enabling requirement engineers and users to validate the main concepts and functional steps behind stories and discover malformed or redundant stories. Such models also open the door for automated systematic analysis.},
annote = {cited By 2},
author = {Gilson, Fabian and Irwin, Calum},
booktitle = {Proceedings - 25th Australasian Software Engineering Conference, ASWEC 2018},
doi = {10.1109/ASWEC.2018.00016},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Gilson, Fabian and Irwin, Calum{\_} {\_}{\_}From user stories to use case scenarios towards a generative approach{\_}{\_} (2018).pdf:pdf},
isbn = {9781728112411},
keywords = {Agile software development,Model-driven development,Natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {61--65},
title = {{From user stories to use case scenarios towards a generative approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061061819{\&}doi=10.1109{\%}2FASWEC.2018.00016{\&}partnerID=40{\&}md5=497e1b40befe7d31be336fcbced9f6c1},
year = {2018}
}
@inproceedings{10.1145/3340482.3342745,
abstract = {Non-Functional Requirements (NFR), a set of quality attributes, required for software architectural design. Which are usually scattered in SRS and must be extracted for quality software development to meet user expectations. Researchers show that functional and non-functional requirements are mixed together within the same SRS, which requires a mammoth effort for distinguishing them. Automatic NFR classification would be a feasible way to characterize those requirements, where several techniques have been recommended e.g. IR, linguistic knowledge, etc. However, conventional supervised machine learning methods suffered for word representation problem and usually required hand-crafted features, which will be overcome by proposed research using RNN variants to categories NFR. The NFR are interrelated and one task happens after another, which is the ideal situation for RNN. In this approach, requirements are processed to eliminate unnecessary contents, which are used to extract features using word2vec to fed as input of RNN variants LSTM and GRU. Performance has been evaluated using PROMISE dataset considering several statistical analysis. Among those models, precision, recall, and f1-score of LSTM validation are 0.973, 0.967 and 0.966 respectively, which is higher over CNN and GRU models. LSTM also correctly classified minimum 60{\%} and maximum 80{\%} unseen requirements. In addition, classification accuracy of LSTM is 6.1{\%} better than GRU, which concluded that RNN variants can lead to better classification results, and LSTM is more suitable for NFR classification from textual requirements.},
address = {New York, NY, USA},
author = {Rahman, M. Abdur and Haque, M. Ariful and Tawhid, M. Nurul Ahad and Siddik, M. Saeed},
booktitle = {MaLTeSQuE 2019 - Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with ESEC/FSE 2019},
doi = {10.1145/3340482.3342745},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/RAHMAN{\~{}}1.PDF:PDF},
isbn = {9781450368551},
keywords = {Deep Learning,NLP,Non-Functional Requirements,RNN,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {25--30},
publisher = {Association for Computing Machinery},
series = {MaLTeSQuE 2019},
title = {{Classifying non-functional requirements using RNN variants for quality software development}},
url = {https://doi.org/10.1145/3340482.3342745},
year = {2019}
}
@inproceedings{10.1145/2491411.2494591,
abstract = {Using requirement boilerplates is an effective way to mitigate many types of ambiguity in Natural Language (NL) requirements and to enable more automated transformation and analysis of these requirements. When requirements are expressed using boilerplates, one must check, as a first quality assurance measure, whether the requirements actually conform to the boilerplates. If done manually, boilerplate conformance checking can be laborious, particularly when requirements change frequently. We present RUBRIC (ReqUirements BoileRplate sanIty Checker), a flexible tool for automatically checking NL requirements against boilerplates for conformance. RUBRIC further provides a range of diagnostics to highlight potentially problematic syntactic constructs in NL requirement statements. RUBRIC is based on a Natural Language Processing (NLP) technique, known as text chunking. A key advantage of RUBRIC is that it yields highly accurate results even in early stages of requirements writing, where a requirements glossary may be unavailable or only partially specified. RUBRIC is scalable and can be applied repeatedly to large sets of requirements as they evolve. The tool has been validated through an industrial case study which we outline briefly in the paper. Copyright 2013 ACM.},
address = {New York, NY, USA},
author = {Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank and Gnaga, Raul},
booktitle = {2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings},
doi = {10.1145/2491411.2494591},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/ARORA{\_}{\~{}}2.PDF:PDF},
isbn = {9781450322379},
keywords = {Natural Language Processing (NLP) text chunking,Requirement boilerplates,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {599--602},
publisher = {Association for Computing Machinery},
series = {ESEC/FSE 2013},
title = {{RUBRIC: A flexible tool for automated checking of conformance to requirement boilerplates}},
url = {https://doi.org/10.1145/2491411.2494591},
year = {2013}
}
@inproceedings{9016907,
abstract = {Software Requirements (SR) are considered as the foundation for a supreme quality software development process and each step of the software development process is dependent and is related to the SR. Software requirements elicitation may be the most important area of requirements engineering and possibly of the entire software development process. There is a lot of human work required in the process of software requirements elicitation and software requirements classification and in cases where the requirements are huge in number, this requirements elicitation and classification process becomes tedious and is prone to errors. We propose a novel approach to automate Requirements Elicitation and Classification using an intelligent conversational chatbot. Utilizing Machine Learning and Artificial Intelligence, the chatbot converses with stakeholders in Natural Language and elicits formal system requirements from the interaction, and subsequently classifies the elicited requirements into Functional and Non-functional system requirements.},
author = {{Rajender Kumar Surana}, Chetan Surana and Shriya and Gupta, DIpesh B. and Shankar, Sahana P.},
booktitle = {2019 4th IEEE International Conference on Recent Trends on Electronics, Information, Communication and Technology, RTEICT 2019 - Proceedings},
doi = {10.1109/RTEICT46194.2019.9016907},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/rajender Intelligent Chatbot for Requirements Elicitation and Classification.pdf:pdf},
isbn = {9781728106304},
keywords = {Artificial Intelligence,Automation,Chatbot,Classifier,Machine Learning,Na{\"{i}}ve Bayes,Requirements Engineering,Software Requirements Classification,Software Requirements Elicitation,Support Vector Machine,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {866--870},
title = {{Intelligent Chatbot for Requirements Elicitation and Classification}},
year = {2019}
}
@inproceedings{8501348,
abstract = {Users of today's online software services are often diversified and distributed, whose needs are hard to elicit using conventional RE approaches. As a consequence, crowd-based, data intensive requirements engineering approaches are considered important. In this paper, we have conducted an experimental study on a dataset of 2,966 requirements statements to evaluate the performance of three text clustering algorithms. The purpose of the study is to aggregate similar requirement statements suggested by the crowd users, and also to identify domain objects and operations, as well as required features from the given requirements statements dataset. The experimental results are then cross-checked with original tags provided by data providers for validation.},
author = {{Ali Khan}, Javed and Liu, Lin and Jia, Yidi and Wen, Lijie},
booktitle = {Proceedings - 2018 7th Workshop on Empirical Requirements Engineering, EmpiRE 2018},
doi = {10.1109/EmpiRE.2018.00010},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ali khan Linguistic Analysis of Crowd Requirements An Experimental Study.pdf:pdf},
isbn = {9781538683590},
issn = {2329-6356},
keywords = {Crowd-based RE,Smart home,Summarization,experiment,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirement clustering},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {24--31},
title = {{Linguistic Analysis of Crowd Requirements: An Experimental Study}},
year = {2018}
}
@article{Rago2016579,
abstract = {Developing high-quality requirements specifications often demands a thoughtful analysis and an adequate level of expertise from analysts. Although requirements modeling techniques provide mechanisms for abstraction and clarity, fostering the reuse of shared functionality (e.g., via UML relationships for use cases), they are seldom employed in practice. A particular quality problem of textual requirements, such as use cases, is that of having duplicate pieces of functionality scattered across the specifications. Duplicate functionality can sometimes improve readability for end users, but hinders development-related tasks such as effort estimation, feature prioritization, and maintenance, among others. Unfortunately, inspecting textual requirements by hand in order to deal with redundant functionality can be an arduous, time-consuming, and error-prone activity for analysts. In this context, we introduce a novel approach called ReqAligner that aids analysts to spot signs of duplication in use cases in an automated fashion. To do so, ReqAligner combines several text processing techniques, such as a use case-aware classifier and a customized algorithm for sequence alignment. Essentially, the classifier converts the use cases into an abstract representation that consists of sequences of semantic actions, and then these sequences are compared pairwise in order to identify action matches, which become possible duplications. We have applied our technique to five real-world specifications, achieving promising results and identifying many sources of duplication in the use cases.},
annote = {cited By 10},
author = {Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andres},
doi = {10.1007/s10270-014-0431-3},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andres{\_} {\_}{\_}Identifying duplicate functionality in textual use cases by aligning semantic actions{\_}{\_} (2016).pdf:pdf},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Machine learning,Natural language processing,Requirements engineering,Sequence alignment,Use case modeling,Use case refactoring,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {2},
pages = {579--603},
title = {{Identifying duplicate functionality in textual use cases by aligning semantic actions}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027940424{\&}doi=10.1007{\%}2Fs10270-014-0431-3{\&}partnerID=40{\&}md5=7f9542868212503db04994a97e9ccb9c},
volume = {15},
year = {2016}
}
@incollection{grunbacher_semi-automatic_2017,
abstract = {Context and motivation: Mature software systems comprise a vast number of heterogeneous system capabilities which are usually requested by different groups of stakeholders and which evolve over time. Software features describe and bundle low level capabilities logically on an abstract level and thus provide a structured and comprehensive overview of the entire capabilities of a software system. Question/problem: Software features are often not explicitly managed. Quite the contrary, feature-relevant information is often spread across several software engineering artifacts (e.g., user manual, issue tracking systems). It requires huge manual effort to identify and extract feature-relevant information from these artifacts in order to make feature knowledge explicit. Principal ideas/results: Our semi-automatic approach allows to identify and extract atomic software feature-relevant information from natural language user manuals by means of a domain glossary, structural sentence information, and natural language processing techniques with a precision and recall of over 94{\%} and 96{\%} respectively. Contribution: We provide an implementation of the atomic software feature-relevant information extraction approach together with this paper as well as corresponding evaluations based on example sections of a user manual taken from industry.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Quirchmayr, Thomas and Paech, Barbara and Kohl, Roland and Karey, Hannes},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-54045-0_19},
editor = {Gr{\"{u}}nbacher, Paul and Perini, Anna},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/QUIRCH{\~{}}1.PDF:PDF},
isbn = {9783319540443},
issn = {16113349},
keywords = {Information extraction,Natural language processing,Software feature,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {255--272},
publisher = {Springer International Publishing},
title = {{Semi-automatic software feature-relevant information extraction from natural language user manuals: An approach and practical experience at roche diagnostics GmbH}},
url = {http://link.springer.com/10.1007/978-3-319-54045-0{\_}19},
volume = {10153 LNCS},
year = {2017}
}
@article{guldali_torc_2011,
abstract = {Acceptance testing is a time-consuming task for complex software systems that have to fulfill a large number of requirements. To reduce this effort, we have developed a widely automated method for deriving test plans from requirements that are expressed in natural language. It consists of three stages: annotation, clustering, and test plan specification. The general idea is to exploit redundancies and implicit relationships in requirements specifications. Multi-viewpoint techniques based on RM-ODP (Reference Model for Open Distributed Processing) are employed for specifying the requirements. We then use linguistic analysis techniques, requirements clustering algorithms, and pattern-based requirements collection to reduce the total effort of testing against the requirements specification. In particular, we use linguistic analysis for extracting and annotating the actor, process and object of a requirements statement. During clustering, a similarity function is computed as a measure for the overlap of requirements. In the test plan specification stage, our approach provides capabilities for semi-automatically deriving test plans and acceptance criteria from the clustered informal textual requirements. Two patterns are applied to compute a suitable order of test activities. The generated test plans consist of a sequence of test steps and asserts that are executed or checked in the given order. We also present the supporting prototype tool TORC, which is available open source. For the evaluation of the approach, we have conducted a case study in the field of acceptance testing of a national electronic identification system. In summary, we report on lessons learned how linguistic analysis and clustering techniques can help testers in understanding the relations between requirements and for improving test planning. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {G{\"{u}}ldali, Baris and Funke, Holger and Sauer, Stefan and Engels, Gregor},
doi = {10.1007/s11219-011-9149-4},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/G{\_}{\_}{\_}{\_}u{\_}{\_}ldali, Baris and Funke, Holger and Sauer, Stefan and Engels, Gregor{\_} {\_}{\_}TORC{\_} Test plan optimization by requirements clustering{\_}{\_} (2011).pdf:pdf},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Acceptance criteria,Acceptance testing,Linguistic analysis,Requirements clustering,Test planning,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
number = {4},
pages = {771--799},
shorttitle = {TORC},
title = {{TORC: Test plan optimization by requirements clustering}},
url = {http://link.springer.com/10.1007/s11219-011-9149-4},
volume = {19},
year = {2011}
}
@inproceedings{6468761,
abstract = {To help expedite the process of constructing use case diagrams, a widely used notation in software engineering, we attempt to develop a generator system that can extract use case diagrams automatically from the input of a software requirements specification. The use of natural language processing techniques can greatly assist this process, one of which is to use syntax-driven semantic analysis. Semantic analysis can provide output in the form of semantic representations that can be used to extract appropriate use case elements. A set of rules have been developed to extract information about the elements of use case diagrams contained in the semantic representation. Our tests show that the system is able to automatically construct use case diagrams for a wide variety of linguistic variations. In a test using real-world cases, an average precision of 0.7375 and recall of 0.691 is obtained. {\textcopyright} 2012 Universitas Indonesia.},
author = {Latifaah and Manurung, Ruli},
booktitle = {2012 International Conference on Advanced Computer Science and Information Systems, ICACSIS 2012 - Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/latifaah.pdf:pdf},
isbn = {9789791421157},
keywords = {automatic programming,computational linguistics,fo,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {149--154},
title = {{Syntax-driven semantic analysis for constructing use case diagrams from software requirement specifications in Indonesian}},
year = {2012}
}
@inproceedings{10.1145/3299771.3299775,
abstract = {Use cases are generally meant to describe the functional requirements of a software system. However, the use of some Natural Language (NL) text may inherently introduce language and interpretation related issues. Several tools and techniques have been proposed and available to assess the quality of use case specification, however, often performed manually. The precise and automated way of analyzing the quality of use cases in different aspects is a need due to volatile functionalities and rapid change in requirements. In this paper, we report the results of two separate experimental studies conducted, a replication of one another, to evaluate the significance and relevance of the use case quality assessment metrics. Our results revealed redundancies among the parameters associated with the quality measures and suggested modifications on the formulation of use case metrics which in turn make them complete, correct and consistent. Subsequently, we develop a tool support to automatically analyze the quality of use case specification on the basis of experimentally validated metrics.},
address = {New York, NY, USA},
author = {Usdadiya, Chirag and Tiwari, Saurabh and Banerjee, Asim},
booktitle = {Proceedings of the 12th Innovations on Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
doi = {10.1145/3299771.3299775},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Usdadiya, Chirag and Tiwari, Saurabh and Banerjee, Asim{\_} {\_}{\_}An Empirical Study on Assessing the Quality of Use Case Metrics{\_}{\_} (2019).pdf:pdf},
isbn = {9781450362153},
keywords = {Natural Language Processing (NLP),Use cases,acm{\_}inc{\_}nlp{\_}x{\_}re,experimental study,metrics,tool support},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ISEC'19},
title = {{An Empirical Study on Assessing the Quality of Use Case Metrics}},
url = {https://doi.org/10.1145/3299771.3299775},
year = {2019}
}
@inproceedings{10.1145/2593801.2593806,
abstract = {Coordinating conjunctions have been a major source of ambiguity in Natural Language statements and the concern has been a major research focus in English Linguistics. Natural Language is also the most common form of expressing the requirements for an envisioned software system. These requirement documents also suffer from similar concern of coordination ambiguity. Presence of nocuous coordination ambiguity is a major concern for the requirements analysts. In this paper, we explore the applicability of constituency test for identifying coordinating conjunction instances in the requirements documents. We show through our study how identification of nocuous and innocuous coordinating conjunctions can be improved using semantic similarity heuristics and machine learning. Our study indicates that Na{\"{i}}ve Bayes classifier outperforms other machine learning algorithms.},
address = {New York, NY, USA},
author = {Sharma, Richa and Bhatia, Jaspreet and Biswas, K. K.},
booktitle = {3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2014 - Proceedings},
doi = {10.1145/2593801.2593806},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sharma, Richa and Bhatia, Jaspreet and Biswas, K. K.{\_} {\_}{\_}Machine learning for constituency test of coordinating conjunctions in requirements specifications{\_}{\_} (2014).pdf:pdf},
isbn = {9781450328463},
keywords = {Conjunctions,Constituency test,Coordination ambiguity,Machine learning,Requirements specifications,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {25--31},
publisher = {Association for Computing Machinery},
series = {RAISE 2014},
title = {{Machine learning for constituency test of coordinating conjunctions in requirements specifications}},
url = {https://doi.org/10.1145/2593801.2593806},
year = {2014}
}
@incollection{hutchison_is_2013,
abstract = {In large IS development projects a huge number of unstructured text documents become available and need to be analyzed and transformed into structured requirements. This elicitation process is known to be time-consuming and error-prone when performed manually by a requirements engineer. Thus, previous works have proposed to automate the process through alternative algorithms using different forms of knowledge. While the effectiveness of different algorithms has been intensively researched, limited efforts have been paid to investigate how the algorithms' outcomes are determined by the utilized knowledge. Our work explores how the amount and type of knowledge affects requirements elicitation quality in two consecutive simulations. The study is based on a requirements elicitation system that has been developed as part of our previous work. We intend to contribute to the body of knowledge by outlining how the provided amount and type of knowledge determine the outcomes of automatic requirements elicitation. {\textcopyright} 2013 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Meth, Hendrik and Maedche, Alexander and Einoeder, Maximilian},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-38709-8_37},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard and Salinesi, Camille and Norrie, Moira C and Pastor, {\'{O}}scar},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Meth, Hendrik and Maedche, Alexander and Einoeder, Maximilian{\_} {\_}{\_}Is knowledge power{\_} The role of knowledge in automated requirements elicitation{\_}{\_} (2013).pdf:pdf},
isbn = {9783642387081},
issn = {03029743},
keywords = {Requirements elicitation,automation,knowledge,simulation,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {578--593},
publisher = {Springer Berlin Heidelberg},
shorttitle = {Is {\{}Knowledge{\}} {\{}Power{\}}?},
title = {{Is knowledge power? The role of knowledge in automated requirements elicitation}},
url = {http://link.springer.com/10.1007/978-3-642-38709-8{\_}37},
volume = {7908 LNCS},
year = {2013}
}
@conference{Shweta2018543,
abstract = {The software requirement specifications are usually documented either in unstructured, semi structured or structured format. The requirements specified in unstructured format are written in simple continuous paragraph and the structured format specifies requirements by means of diagrams. The semi-structured format represents requirements with the help of some keywords. Literature suggests that the rule based work has been the common choice for unstructured format of documenting. However, these rule based works do not yield satisfactory results for semi-structured format. Consequently, these rules need to re-framed in order to apply them for the semi-structured formatted documents. In this paper, we present an improvement on the existing rules considering the keywords present in the text. The technique involves automatic extraction of the class diagrams using NLP tools and techniques. Along with existing rules, the newly formulated rules have been tested for different case studies and suitable metrics have been devised to evaluate the obtained results. Results show that the automatically generated class diagram have 82{\%} average precision value and 94{\%} average recall value when compared to the class diagrams generated by the human experts.},
annote = {cited By 0},
author = {Shweta and Sanyal, Ratna and Ghoshal, Bibhas},
booktitle = {Proceedings - 17th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2018},
doi = {10.1109/ICIS.2018.8466406},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/shweta Automatic Extraction of Structural Model from Semi Structured Software Requirement Specification.pdf:pdf},
isbn = {9781538658925},
keywords = {Natural Language Processing,Software Requirement Specification,Unified Modeling Language (UML),Use Case Description,class diagram,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {543--548},
title = {{Automatic Extraction of Structural Model from Semi Structured Software Requirement Specification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055694820{\&}doi=10.1109{\%}2FICIS.2018.8466406{\&}partnerID=40{\&}md5=4f106d93f6eb6949c90f91b1c6453314},
year = {2018}
}
@inproceedings{10.1145/2791060.2791117,
abstract = {A business subject who wishes to enter an established technological market is required to accurately analyse the features of the products of the different competitors. Such features are normally accessible through natural language (NL) brochures, or NL Web pages, which describe the products to potential customers. Building a feature model that hierarchically summarises the different features available in competing products can bring relevant benefits in market analysis. A company can easily visualise existing features, and reason about aspects that are not covered by the available solutions. However, designing a feature model starting from publicly available documents of existing products is a time consuming and error-prone task. In this paper, we present two tools, namely Commonality Mining Tool (CMT) and Feature Diagram Editor (FDE), which can jointly support the feature model definition process. CMT allows mining common and variant features from NL descriptions of existing products, by leveraging a natural language processing (NLP) approach based on contrastive analysis, which allows identifying domain-relevant terms from NL documents. FDE takes the commonalities and variabilities extracted by CMT, and renders them in a visual form. Moreover, FDE allows the graphical design and refinement of the final feature model, by means of an intuitive GUI.},
address = {New York, NY, USA},
author = {Ferrari, Alessio and Spagnolo, Giorgio O. and Gnesi, Stefania and Dell'orletta, Felice},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2791060.2791117},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/FE2E5E{\~{}}1.PDF:PDF;:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Chwastek, Robert{\_} {\_}{\_}Cognitive systems in human resources{\_}{\_} (2017).pdf:pdf},
isbn = {9781450336130},
keywords = {Software product lines,Tools,Variability mining,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {402--410},
publisher = {Association for Computing Machinery},
series = {SPLC '15},
title = {{CMT and FDE: Tools to bridge the gap between natural language documents and feature diagrams}},
url = {https://doi.org/10.1145/2791060.2791117},
volume = {20-24-July},
year = {2015}
}
@article{Winkler2017468,
abstract = {Neural Networks have been utilized to solve various tasks such as image recognition, text classification, and machine translation and have achieved exceptional results in many of these tasks. However, understanding the inner workings of neural networks and explaining why a certain output is produced are no trivial tasks. Especially when dealing with text classification problems, an approach to explain network decisions may greatly increase the acceptance of neural network supported tools. In this paper, we present an approach to visualize reasons why a classification outcome is produced by convolutional neural networks by tracing back decisions made by the network. The approach is applied to various text classification problems, including our own requirements engineering related classification problem. We argue that by providing these explanations in neural network supported tools, users will use such tools with more confidence and also may allow the tool to do certain tasks automatically.},
annote = {cited By 7},
author = {Winkler, Jonas Paul and Vogelsang, Andreas},
doi = {10.1007/978-3-319-59569-6_55},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/GRSS8VSP/Winkler en Vogelsang - 2017 - “What Does My Classifier Learn” A Visual Approach.pdf:pdf},
isbn = {9783319595689},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Artificial intelligence,Explanations,Machine learning,Natural language processing,Neural networks,Requirements engineering,Visual feedback,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {468--479},
title = {{What does my classifier learn? A visual approach to understanding natural language text classifiers}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021732233{\&}doi=10.1007{\%}2F978-3-319-59569-6{\_}55{\&}partnerID=40{\&}md5=c10feb863a6adf1740a3251129c7b5ed},
volume = {10260 LNCS},
year = {2017}
}
@article{Alwadain201945,
abstract = {In software development projects, the process of requirements engineering (RE) is one in which requirements are elicited, analyzed, documented, and managed. Requirements are traditionally collected using manual approaches, including interviews, surveys, and workshops. Employing traditional RE methods to engage a large base of users has always been a challenge, especially when the process involves users beyond the organization's reach. Furthermore, emerging software paradigms, such as mobile computing, social networks, and cloud computing, require better automated or semi-automated approaches for requirements elicitation because of the growth in systems users, the accessibility to crowd-generated data, and the rapid change of users' requirements. This research proposes a methodology to capture and analyze crowd-generated data (e.g., user feedback and comments) to find potential requirements for a software system in use. It semi-automates some requirements-elicitation tasks using data retrieval and natural language processing (NLP) techniques to extract potential requirements. It supports requirements engineers' efforts to gather potential requirements from crowd-generated data on social networks (e.g., Twitter). It is an assistive approach that taps into unused knowledge and experiences emphasizing continuous requirements elicitation during systems use.},
annote = {cited By 0},
author = {Alwadain, Ayed and Alshargi, Mishari},
doi = {10.14569/ijacsa.2019.0100907},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/alwadain Crowd-generated data mining for continuous requirements elicitation.pdf:pdf},
issn = {21565570},
journal = {International Journal of Advanced Computer Science and Applications},
keywords = {Continuous requirements elicitation,Crowd data mining,NLP,RE,Requirements engineering,Twitter,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {9},
pages = {45--50},
title = {{Crowd-generated data mining for continuous requirements elicitation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072985597{\&}partnerID=40{\&}md5=33ed74fa572838f04f78fe915b85cb10},
volume = {10},
year = {2019}
}
@conference{Krishnan2010815,
abstract = {Requirement analysis is the preliminary step in software development process. The requirements stated by the clients are analyzed and an abstraction of it is created which is termed as requirements model. Unified Modeling Language (UML) models are helpful for understanding the problems, communicating with application experts and preparing documentation. The static design view of the system can be modeled using a UML class diagram. System requirements stated by the user are usually in natural language form despite a wide variety of formal languages and UML. This is an imprecise and inconsistent form which is difficult to be used by the developer for design. We present a new methodology for generating UML class diagrams or models from natural language problem statement or requirement specification. We have named our methodology as Relative Extraction Methodology which uses an intermediate graphical representation called dependency graph. This serves as a skeleton for the generation of UML class models. The natural language processing is done for the construction of dependency graph which is finally mapped to a class diagram. {\textcopyright}2010 IEEE.},
annote = {cited By 4},
author = {Krishnan, Hema and Samuel, Philip},
booktitle = {2010 IEEE International Conference on Communication Control and Computing Technologies, ICCCCT 2010},
doi = {10.1109/ICCCCT.2010.5670730},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/krishnan Relative extraction methodology for class diagram generation using dependency graph.pdf:pdf},
isbn = {9781424477692},
keywords = {Class,Concept,Dependency graph,Relative extraction methodology,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {815--820},
title = {{Relative extraction methodology for class diagram generation using dependency graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751489684{\&}doi=10.1109{\%}2FICCCCT.2010.5670730{\&}partnerID=40{\&}md5=882bd7db64c6aae9f3981786a01ac9ad},
year = {2010}
}
@inproceedings{5628829,
abstract = {This paper reports on work that is investigating the application of ontology engineering and natural language processing to software engineering. Our focus is the transition from requirements to design which remains one of the main challenges in software engineering. A key reason for why this is so challenging is that the vast majority of requirements documents are informal, written in natural language, whereas the final goal (code) is formal. System models, as an intermediate step between the requirements and code, help understand requirements. Even a seemingly precise requirements document typically contains a lot of inconsistencies and omissions, which become visible when we model the system. Our hypothesis is that these inconsistencies become apparent when we compare the project-specific model with a generic model of the application domain. To test our hypothesis, we need to transform natural language representations of requirements information into a form that facilitates comparison with a domain model. Naturally, we also need a domain model against which to compare and this presupposes a means to construct such models. In the paper, we extract a conceptual model (an ontology) and a behavioural model from different sources. An ontology is generated from a generic domain description, and a project-specific model is generated from requirements documents. For ontology generation, natural language processing techniques are used to aid the construction. By comparing the resulting models, we validate both of them. When inconsistencies are found, we generate feedback for the analyst. The generated feedback was validated on a case study and has proven useful to improve both requirements documents and models. {\textcopyright} 2010 IEEE.},
author = {Kof, Leonid and Gacitua, Ricardo and Rouncefield, Mark and Sawyer, Pete},
booktitle = {Proceedings - 2010 IEEE 4th International Conference on Semantic Computing, ICSC 2010},
doi = {10.1109/ICSC.2010.95},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kof, Leonid and Gacitua, Ricardo and Rouncefield, Mark and Sawyer, Pete{\_} {\_}{\_}Ontology and model alignment as a means for requirements validation{\_}{\_} (2010).pdf:pdf},
isbn = {9780769541549},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,ontologies (artificial},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {46--51},
title = {{Ontology and model alignment as a means for requirements validation}},
year = {2010}
}
@inproceedings{8048885,
abstract = {Twitter enables large populations of end-users of software to publicly share their experiences and concerns about software systems in the form of micro-blogs. Such data can be collected and classified to help software developers infer users' needs, detect bugs in their code, and plan for future releases of their systems. However, automatically capturing, classifying, and presenting useful tweets is not a trivial task. Challenges stem from the scale of the data available, its unique format, diverse nature, and high percentage of irrelevant information and spam. Motivated by these challenges, this paper reports on a three-fold study that is aimed at leveraging Twitter as a main source of software user requirements. The main objective is to enable a responsive, interactive, and adaptive data-driven requirements engineering process. Our analysis is conducted using 4,000 tweets collected from the Twitter feeds of 10 software systems sampled from a broad range of application domains. The results reveal that around 50{\%} of collected tweets contain useful technical information. The results also show that text classifiers such as Support Vector Machines and Naive Bayes can be very effective in capturing and categorizing technically informative tweets. Additionally, the paper describes and evaluates multiple summarization strategies for generating meaningful summaries of informative software-relevant tweets.},
author = {Williams, Grant and Mahmoud, Anas},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.14},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Williams, Grant and Mahmoud, Anas{\_} {\_}{\_}Mining Twitter Feeds for Software User Requirements{\_}{\_} (2017).pdf:pdf},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {Twitter,ieee{\_}inc{\_}nlp{\_}x{\_}re,summarization,text classification,user requirements},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--10},
title = {{Mining Twitter Feeds for Software User Requirements}},
year = {2017}
}
@article{Bakar20161297,
abstract = {Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
annote = {cited By 19},
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.},
doi = {10.1016/j.asoc.2016.07.048},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah and Jalab, Hamid A.{\_} {\_}{\_}Extracting features from online software reviews to aid requirements reuse{\_}{\_} (2016).pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Latent semantic analysis,Natural language processing,Requirements reuse,Software engineering,Unsupervised learning,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1297--1315},
title = {{Extracting features from online software reviews to aid requirements reuse}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124422{\&}doi=10.1016{\%}2Fj.asoc.2016.07.048{\&}partnerID=40{\&}md5=e7c61585c2eae8eaf3f9c8cca858e5a2},
volume = {49},
year = {2016}
}
@inproceedings{10.1145/2372251.2372299,
abstract = {Opportunities for global software development are limited in those countries with a lack of English-speaking professionals. Machine translation technology is today available in the form of cross-language web services and can be embedded into multiuser and multilingual chats without disrupting the conversation flow. However, we still lack a thorough understanding of how real-time machine translation may affect communication in global software teams. In this paper, we present the replication of a controlled experiment that assesses the effect of real-time machine translation on multilingual teams while engaged in distributed requirements meetings. In particular, in this replication we specifically evaluate whether non-English speaking groups benefit from communicating in their own native languages when their English is not fluid enough for a fast-paced conversation. Copyright 2012 ACM.},
address = {New York, NY, USA},
author = {Calefato, Fabio and Lanubile, Filippo and Conte, Tayana and Prikladnicki, Rafael},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1145/2372251.2372299},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/calefato Assessing the impact of real-time machine translation on requirements meetings A replicated experiment.pdf:pdf},
isbn = {9781450310567},
issn = {19493770},
keywords = {Controlled experiment,Global software engineering,Machine translation,Requirements meetings,acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {251--260},
publisher = {Association for Computing Machinery},
series = {ESEM '12},
title = {{Assessing the impact of real-time machine translation on requirements meetings: A replicated experiment}},
url = {https://doi.org/10.1145/2372251.2372299},
year = {2012}
}
@inproceedings{8049156,
abstract = {The area of Traffic Management (TM) is characterized by uncertainty, complexity, and imprecision. The complexity of software systems in the TM domain which contributes to a more challenging Requirements Engineering (RE) job mainly stems from the diversity of stakeholders and complexity of requirements elicitation in this domain. This work brings an interactive solution for exploring functional and non-functional requirements of software-reliant systems in the area of traffic management. We prototyped the RETTA tool which leverages the wisdom of the crowd and combines it with machine learning approaches such as Natural Language Processing and Na{\"{i}}ve Bayes to help with the requirements elicitation and classification task in the TM domain. This bridges the gap among stakeholders from both areas of software development and transportation engineering. The RETTA prototype is mainly designed for requirements engineers and software developers in the area of TM and can be used on Android-based devices.},
archivePrefix = {arXiv},
arxivId = {1707.01927},
author = {Noaeen, Mohammad and Abad, Zahra Shakeri Hossein and Far, Behrouz Homayoun},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.78},
eprint = {1707.01927},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Noaeen, Mohammad and Abad, Zahra Shakeri Hossein and Far, Behrouz Homayoun{\_} {\_}{\_}Let's Hear it from RETTA{\_} A Requirements Elicitation Tool for TrAffic Management Systems{\_}{\_} (2017).pdf:pdf},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {Requirements Engineering,Tool Development,Traffic Signal Timing,Transportation Management,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {450--451},
title = {{Let's Hear it from RETTA: A Requirements Elicitation Tool for TrAffic Management Systems}},
year = {2017}
}
@inproceedings{7320415,
abstract = {User stories are a widely used notation for formulating requirements in agile development projects. Despite their popularity in industry, little to no academic work is available on assessing their quality. The few existing approaches are too generic or employ highly qualitative metrics. We propose the Quality User Story Framework, consisting of 14 quality criteria that user story writers should strive to conform to. Additionally, we introduce the conceptual model of a user story, which we rely on to design the AQUSA software tool. AQUSA aids requirements engineers in turning raw user stories into higher-quality ones by exposing defects and deviations from good practice in user stories. We evaluate our work by applying the framework and a prototype implementation to three user story sets from industry.},
author = {Lucassen, Garm and Dalpiaz, Fabiano and {Van Der Werf}, Jan Martijn E.M. and Brinkkemper, Sjaak},
booktitle = {2015 IEEE 23rd International Requirements Engineering Conference, RE 2015 - Proceedings},
doi = {10.1109/RE.2015.7320415},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/LUCASS{\~{}}1.PDF:PDF},
isbn = {9781467369053},
issn = {2332-6441},
keywords = {AQUSA,NLP,QUS Framework,User stories,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirements engineering,requirements quality,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {126--135},
title = {{Forging high-quality User Stories: Towards a discipline for Agile Requirements}},
year = {2015}
}
@inproceedings{10.1145/2590748.2590766,
abstract = {Textual use cases are commonly used to represent software requirements at initial stages. However in most of the cases, these documents are unstructured. In this paper, we present a linguistic engine for processing textual use cases and extract a structured model in terms of an annotation model out of these use cases. An annotation model of a use case can further be used to generate various UML requirements models, Business Process Models and ontology. The implementation details of Natural Language Processing (NLP) technique employed by us for the linguistic engine is described in this paper in detail. Also, we consider a corpus containing 123 use cases from real-life industrial projects within our company, and translate them into annotation models using our NLP technique. For evaluating the performance of conversion we use a few metrics and report some promising results for our linguistic engine. Copyright 2014 ACM.},
address = {New York, NY, USA},
author = {Sawant, Kiran Prakash and Roy, Suman and Parachuri, Deepti and Plesse, Fran{\c{c}}ois and Bhattacharya, Pushpak},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2590748.2590766},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sawant, Kiran Prakash and Roy, Suman and Sripathi, Srivibha and Plesse, Fran{\_}{\_}c{\_}c{\_}{\_}ois and Sajeev, A. S.M.{\_} {\_}{\_}Deriving requirements model from textual use cases{\_}{\_} (2014).pdf:pdf},
isbn = {9781450327763},
keywords = {Natural language processing,Requirements engineering,Requirements model,Textual use cases,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ISEC '14},
title = {{Enforcing structure on textual use cases via annotation models}},
url = {https://doi.org/10.1145/2590748.2590766},
year = {2014}
}
@inproceedings{6767215,
abstract = {This paper proposes a new method for analyzing software requirements specified using a natural language. The aim of this study was to transform requirements which are specified using a natural language into a formal model. A grammatical analysis based on Part-of-Speech Tagging technique and english sentence pattern matching were adopted to analyze the software requirements in order to obtain the structure of sentences of requirements specification that will eventually be transformed into formal models. {\textcopyright} 2013 IEEE.},
author = {Fatwanto, Agung},
booktitle = {2nd International Conference on Future Generation Communication Technologies, FGCT 2013},
doi = {10.1109/FGCT.2013.6767215},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Fatwanto, Agung{\_} {\_}{\_}Natural language requirements specification analysis using Part-of-Speech Tagging{\_}{\_} (2013).pdf:pdf},
issn = {2377-2638},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,part-of-speech tagging,pattern matching,requirements analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re,software requirements specification,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {nov},
pages = {98--102},
title = {{Natural language requirements specification analysis using Part-of-Speech Tagging}},
year = {2013}
}
@incollection{halpin_analyzing_2011,
abstract = {In Model Driven Development (MDD), models replace software code as the development artifact. At the same time, requirements represent the information that is elaborated in models. However, despite the tight relationship between models and requirements, only a few MDD approaches provide the necessary methodological guidelines and tool support to explicitly facilitate this relationship. We analyze approaches for integrating requirements with models within MDD and highlight the common characteristics, benefits, and problems. Based on the analysis, we elicit a set of general properties that need to be fulfilled when considering the integration of requirements and models, and we assess the contribution of the considered approaches accordingly. {\textcopyright} 2011 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {Series Title: Lecture Notes in Business Information Processing},
author = {Zikra, Iyad and Stirna, Janis and Zdravkovic, Jelena},
booktitle = {Lecture Notes in Business Information Processing},
doi = {10.1007/978-3-642-21759-3_25},
editor = {Halpin, Terry and Nurcan, Selmin and Krogstie, John and Soffer, Pnina and Proper, Erik and Schmidt, Rainer and Bider, Ilia},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Zikra, Iyad and Stirna, Janis and Zdravkovic, Jelena{\_} {\_}{\_}Analyzing the integration between requirements and models in model driven development{\_}{\_} (2011).pdf:pdf},
isbn = {9783642217586},
issn = {18651348},
keywords = {MDD,MDE,Model Driven Development,Model-Driven Engineering,Requirements,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {342--356},
publisher = {Springer Berlin Heidelberg},
title = {{Analyzing the integration between requirements and models in model driven development}},
url = {http://link.springer.com/10.1007/978-3-642-21759-3{\_}25},
volume = {81 LNBIP},
year = {2011}
}
@inproceedings{8049155,
abstract = {Use case modeling is a popular and widely used specification documentation strategy that facilitates a developer to specify the functional requirements of a software system. There have been many efforts made to document problem specification in the use cases by employing a restricted form of natural language, authoring guidelines and checklist, but no tool support is available to assess and validate their quality. In this paper, we present a tool, UCAnalyzer, to analyze use case textual descriptions. UCAnalyzer has three key modules: (1) a use case textual description editor module, (2) an analysis module to assess the quality of use case textual description, and (3) a module to highlight errors and provide suggestions.},
author = {Tiwari, Saurabh and Laddha, Mayank},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.39},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Tiwari, Saurabh and Laddha, Mayank{\_} {\_}{\_}UCAnalyzer{\_} A Tool to Analyze Use Case Textual Descriptions{\_}{\_} (2017).pdf:pdf},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {Use case,authoring guidelines,checklist,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirement analysis},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {448--449},
title = {{UCAnalyzer: A Tool to Analyze Use Case Textual Descriptions}},
year = {2017}
}
@inproceedings{7077118,
abstract = {Requirements analysis process involves developing abstract models for the envisioned or the proposed software system. These models are used to help refine and enrich the requirements for the system. Unified Modelling Language (UML) has become the standard for modelling software requirements. However, software requirements are captured in the form of Natural Language and, generating UML models from natural language requirements relies heavily on individual expertise. In this paper, we present an approach towards automated generation of behavioural UML models, namely activity diagrams and sequence diagrams. Our approach is based on transforming the requirements statements to intermediary structured representations - frames and then, translate them to the behavioural UML models. We are using Grammatical Knowledge Patterns and lexical and syntactic analysis of requirements statements to populate frames for the corresponding statements. Knowledge stored in frames is then used to automatically generate activity and sequence diagram. We present our approach through the case-studies performed.},
author = {Sharma, Richa and Gulia, Sarita and Biswas, K. K.},
booktitle = {ENASE 2014 - Proceedings of the 9th International Conference on Evaluation of Novel Approaches to Software Engineering},
doi = {10.5220/0004893600690077},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/sharma gulia biswas Automated generation of activity and sequence diagrams from natural language requirements.pdf:pdf},
isbn = {9789897580307},
keywords = {Activity diagram,Frames,Natural language processing,Sequence diagram,UML models,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
pages = {69--77},
title = {{Automated generation of activity and sequence diagrams from natural language requirements}},
year = {2014}
}
@inproceedings{8396645,
abstract = {Requirements engineering is the initial step in the development of software. Eliciting and understanding requirements are crucial in that the quality of the resulting system is highly dependent on a clear understanding of the customers' needs. Customers frequently express requirements in natural language, and software engineers then transform these natural language requirements to a more formalized representation that is useful during the remaining steps of software development. This transformation process has the potential to introduce errors and misunderstandings in the requirements because of ambiguities and incompleteness found in the natural language requirements. A communication gap typically exists between customers and software developers who may not share the same technical expertise. Software developers typically have computer science backgrounds, while the software application areas and thus the expertise of the customers may be from different domains, including such domains as chemistry, aerospace, healthcare, and economics. This communication gap can lead to software failures that can have devastating consequences in many different domains, including the aerospace domain, in terms of time, money and mission-critical efforts. Numerous research efforts are found in the literature that describe how a large textual corpus such as Wikipedia can be useful for helping fill that communication gap between the customer and the developer when natural language requirements are used in the requirements engineering process [1][2][3]. Wikipedia is a large online information repository that has an interface that emulates a paper encyclopedia in its coverage of diverse topics. It provides a large number of real-world concepts organized in hierarchical semantic structures. The quantity of the information and the ability to search the information in Wikipedia make it a valuable resource for research related to requirements and natural language processing [4][5][6]; however, the quantity of information makes searching it time consuming. In this paper, we describe research to determine the most efficient approach to retrieve data from Wikipedia for use in improving the requirements engineering process. We refer to this step of retrieving the information of interest as knowledge acquisition. We present our analysis and compare time efficiency of the use of regular expressions and string search algorithms for decreasing the knowledge acquisition bottleneck caused by the high computational time needed to retrieve these data from Wikipedia.},
author = {Rodriguez, Danissa V. and Carver, Doris L. and Mahmoud, Anas},
booktitle = {IEEE Aerospace Conference Proceedings},
doi = {10.1109/AERO.2018.8396645},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/RODRIG{\~{}}1.PDF:PDF},
isbn = {9781538620144},
issn = {1095323X},
keywords = {encyclopaedias,ieee{\_}inc{\_}nlp{\_}x{\_}re,information retrieval,knowledge acq,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--16},
title = {{An efficient wikipedia-based approach for better understanding of natural language text related to user requirements}},
volume = {2018-March},
year = {2018}
}
@inproceedings{8981830,
abstract = {The world keeps moving, software products too. An application's objectives, structures, requirements, and assumptions that have been elicited and analyzed previously may need to be reassessed and updated. In order to fully understand these requirements evolutions, what changes are necessary, and why those changes are needed, one essential source of requirements is user feedback. However, handling and analyzing so many user feedbacks can be time-consuming. Using natural language processing tools for Bahasa Indonesia and Naive Bayes classifier, this research aims to develop a tool to process natural language and classify user feedbacks. The developed tool is expected to make feedback classification less time-consuming so that developers can project their energy to more productive and creative works. The machine learning models are built using the feedback dataset taken from an up-and-running university e-learning system and show promising results. The highest confusion matrix scores are 92.5{\%} for accuracy, 85.6{\%} precision, 85.1{\%} recall, and lastly, 85.4{\%} for the F-measure score. The resulting web application for feedback management is then evaluated to the users, and even though it still needs to be further polished and improved for real industrial use, it is perceived to be useful and easy to use.},
author = {Ferdino, Ivan and Rusli, Andre},
booktitle = {2019 5th International Conference on New Media Studies (CONMEDIA)},
doi = {10.1109/conmedia46929.2019.8981830},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ferdino, Ivan and Rusli, Andre{\_} {\_}{\_}Using Na{\_}{\_}{\_}{\_}i{\_}{\_}ve Bayes Classifier for Application Feedback Classification and Management in Bahasa Indonesia{\_}{\_} (2020).pdf:pdf},
keywords = {Bayes methods,Internet,computer aided instruction,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {217--222},
title = {{Using Na{\"{i}}ve Bayes Classifier for Application Feedback Classification and Management in Bahasa Indonesia}},
year = {2020}
}
@inproceedings{10.1109/ASE.2013.6693131,
abstract = {Requirements engineers need to be confident that enough requirements analysis has been done before a project can move forward. In the context of KAOS, this information can be derived from the soundness of the refinements: sound refinements indicate that the requirements in the goal-graph are mature enough or good enough for implementation. We can estimate how close we are to 'good enough' requirements using the judgments of experts and other data from the goals. We apply Toulmin's model of argumentation to evaluate how sound refinements are. We then implement the resulting argumentation model using Bayesian Belief Networks and provide a semi-automated way aided by Natural Language Processing techniques to carry out the proposed evaluation. We have performed an initial validation on our work using a small case-study involving an electronic document management system. {\textcopyright} 2013 IEEE.},
author = {Veerappa, Varsha and Harrison, Rachel},
booktitle = {2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
doi = {10.1109/ASE.2013.6693131},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Veerappa, Varsha and Harrison, Rachel{\_} {\_}{\_}Assessing the maturity of requirements through argumentation{\_} A good enough approach{\_}{\_} (2013).pdf:pdf},
isbn = {9781479902156},
keywords = {Bayesian belief networks,Soundness of refinements,acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,maturity of requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {670--675},
publisher = {IEEE Press},
series = {ASE'13},
title = {{Assessing the maturity of requirements through argumentation: A good enough approach}},
url = {https://doi.org/10.1109/ASE.2013.6693131},
year = {2013}
}
@inproceedings{9007263,
abstract = {Security Requirements Engineering is a very important process in the Software Development Life Cycle (SDLC) with Security Engineering being given profound attention in the development of software. It is imperative to build security within a software product. This ensures that software that is deployed is secure and can withstand attack. The research work explores Security Requirements extraction and classification techniques and application of Machine to the process. Techniques such as Na{\"{i}}ve Bayes Classifier, K-NN, Support Vector Machine (SVM), ANN among others have been applied to the various tasks embedded in the process. This research will pave a way to techniques that can aid in the process of Security Requirements extraction and classification.},
author = {Kadebu, Prudence and Thada, Vikas and Chiurunge, Panashe},
booktitle = {Proceedings of the 3rd International Conference on Contemporary Computing and Informatics, IC3I 2018},
doi = {10.1109/IC3I44769.2018.9007263},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/kadebu 2018.pdf:pdf},
isbn = {9781538668948},
keywords = {Machine Learning in Software Security,Natural Language Processing,Security requirements classification,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {129--134},
title = {{Security requirements extraction and classification: A survey}},
year = {2018}
}
@inproceedings{10.1145/2593801.2593802,
abstract = {Automated trace creation techniques are based on a variety of algorithms ranging from basic term matching approaches to more sophisticated expert systems. In this position paper we propose a classification scheme for categorizing the intelligence level of automated traceability techniques. We show that the vast majority of relevant work in the past decade has been focused at the lowest level of the Traceability Intelligence Quotient (tIQ) and posit that achieving high quality automated traceability will require re-focusing research efforts on the development of more intelligent algorithms capable of reasoning about concepts, their relationships and constraints, and the contexts in which they occur.},
address = {New York, NY, USA},
author = {Cleland-Huang, Jane and Guo, Jin},
booktitle = {3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2014 - Proceedings},
doi = {10.1145/2593801.2593802},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Cleland-Huang, Jane and Guo, Jin{\_} {\_}{\_}Towards more intelligent trace retrieval algorithms{\_}{\_} (2014).pdf:pdf},
isbn = {9781450328463},
keywords = {Domain ontology,Expert system,Traceability,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--6},
publisher = {Association for Computing Machinery},
series = {RAISE 2014},
title = {{Towards more intelligent trace retrieval algorithms}},
url = {https://doi.org/10.1145/2593801.2593802},
year = {2014}
}
@article{Ahmed2018168,
abstract = {The major objective of Software Requirements Specification (SRS) is providing sufficient information for software developers to build software product successfully. However, the current features of natural language hinders processing and analysis of requirements due to its ambiguous nature. Over the years, many Natural Language Processing (NLP) approaches were emerged to tackle this problem to detect errors or extract useful information from requirements documents. In this paper, a review of these approaches has been represented to reveal the role of NLP in requirement engineering and depict the current dilemma of SRS processing.},
annote = {cited By 0},
author = {Ahmed, Hussin and Hussain, Azham and Baharom, Fauziah},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ahmed The role of Natural Language Processing in requirement engineering.pdf:pdf},
issn = {2227524X},
journal = {International Journal of Engineering and Technology(UAE)},
keywords = {Natural language processing,Requirements methods,Software requirements specification,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {4.19 Special Issue 19},
pages = {168--171},
title = {{The role of Natural Language Processing in requirement engineering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082344507{\&}partnerID=40{\&}md5=8799ccae15cf02613b05e1aa1a61b921},
volume = {7},
year = {2018}
}
@inproceedings{8054881,
abstract = {Bridging the gap between informal, imprecise, and vague user requirements descriptions and precise formalized specifications is the main task of requirements engineering. Techniques such as interviews or story telling are used when requirements engineers try to identify a user's needs. The requirements specification process is typically done in a dialogue between users, domain experts, and requirements engineers. In our research, we aim at automating the specification of requirements. The idea is to distinguish between untrained users and trained users, and to exploit domain knowledge learned from previous runs of our system. We let untrained users provide unstructured natural language descriptions, while we allow trained users to provide examples of behavioral descriptions. In both cases, our goal is to synthesize formal requirements models similar to statecharts. From requirements specification processes with trained users, behavioral ontologies are learned which are later used to support the requirements specification process for untrained users. Our research method is original in combining natural language processing and search-based techniques for the synthesis of requirements specifications. Our work is embedded in a larger project that aims at automating the whole software development and deployment process in envisioned future software service markets.},
author = {{Van Rooijen}, Lorijn and B{\"{a}}umer, Frederik Simon and Platenius, Marie Christin and Geierhos, Michaela and Hamann, Heiko and Engels, Gregor},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.26},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/van rooijen From user demand to software service Using machine learning to automate the requirements specification process.pdf:pdf},
isbn = {9781538634882},
keywords = {Natural languages,Ontologies,Requirements engineering,Search problems,Software,Unified modeling language,behavioral descriptions,behavioral ontologies,formal requirements models,formal specification,ieee{\_}inc{\_}nlp{\_}x{\_}re,learning (artificial intelligence),natural language processing,ontologies (artificial intelligence),requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,search-based technique,software deployment process automation,software development management,software development process automation,software service markets,systems analysis,trained users,unstructured natural language description,untrained users,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {379--385},
title = {{From user demand to software service: Using machine learning to automate the requirements specification process}},
year = {2017}
}
@conference{Mori2020,
abstract = {This paper highlights the importance of careful selection of appropriate NLP tasks or techniques to derive value from past documents and im- prove the requirement engineering process. As a case study, an experi- ence about introducing NLP techniques to find the lack of requirements by using heterogeneous documents are shown. Using word similarity is one of the ways to determine the relevance between two documents though, the result of proposed scheme in finding meaningfully related pairs of document and further investigation shows that word similarity is not able to solve our problem. In our experimental results, CNN (convolutional neural network) model could estimate the relevance the best compare to other trial models.},
annote = {cited By 0},
author = {Mori, Kenji and Okubo, Naoko and Ueda, Yasushi and Katahira, Masafumi and Amagasa, Toshiyuki},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/mori Toward latent knowledge extraction based on the correlation of heterogeneous text data related to space system development.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Toward latent knowledge extraction based on the correlation of heterogeneous text data related to space system development}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082701235{\&}partnerID=40{\&}md5=ccdd37d83f4e5cb0c93a9619ac0a06bb},
volume = {2584},
year = {2020}
}
@inproceedings{7809611,
abstract = {Context: Expert judgment is the most frequently used method of effort estimation in Agile software development. Unfortunately, Agile teams often underestimate development effort. Therefore, it seems beneficial to support such teams with the information regarding the functional size of requirements they are estimating. Hussain, Kosseim and Ormandjieva (HKO) proposed a method that can be used to automatically classify textual requirements with respect to their COSMIC functional size. Unfortunately, the method has not been sufficiently validated to confirm its usefulness. Objective: To provide external validation of the HKO method and investigate if it can be applied to classify scenario-based requirements (in the form of use cases) with respect to their COSMIC size. Method: Similarily to the original study, we used a set of natural language processing tools to extract syntactic linguistic features and the C4.5 decision tree-based classifiers to classify requirements. We validated the performance of the classifiers using the 10-fold cross-validation procedure on a dataset containing 93 use cases. We compared the performance of the HKO method with the performance of the classifiers trained using a single prediction feature - the number of steps in a use case. Results: Depending on the considered number of size classes and the algorithm used to compute boundaries of the classes, the accuracy of the HKO method ranged between.387 and.785 while the Cohen's kappa index was between.194 and.577. The accuracy of the use-case-steps-based classifiers performed slightly worse. Their accuracy ranged between.015 and.769 while Cohen's kappa was between.067 and.423. We observed that the performance of both types of classifiers dropped visibly when applied to four or more size classes. Conclusion: The classification performance of the HKO method was moderate. However, it was still better than the classification based on the number of steps. Unfortunately, we also observed that the accuracy of the HKO method is sensitive to the language used in descriptions of requirements.},
author = {Ochodek, Miroslaw},
booktitle = {Proceedings - 26th International Workshop on Software Measurement, IWSM 2016 and the 11th International Conference on Software Process and Product Measurement, Mensura 2016},
doi = {10.1109/IWSM-Mensura.2016.039},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ochodek Approximation of COSMIC functional size of scenario-based requirements in agile based on syntactic linguistic features - A replication study.pdf:pdf},
isbn = {9781509041473},
keywords = {Agile,COSMIC,Functional size measurement,Replication study,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {201--211},
title = {{Approximation of COSMIC functional size of scenario-based requirements in agile based on syntactic linguistic features - A replication study}},
year = {2017}
}
@inproceedings{8920404,
abstract = {An important task in requirements engineering is to identify and determine how to verify a requirement (e.g., by manual review, testing, or simulation; also called potential verification method). This information is required to effectively create test cases and verification plans for requirements. [Objective] In this paper, we propose an automatic approach to classify natural language requirements with respect to their potential verification methods (PVM). [Method] Our approach uses a convolutional neural network architecture to implement a multiclass and multilabel classifier that assigns probabilities to a predefined set of six possible verification methods, which we derived from an industrial guideline. Additionally, we implemented a backtracing approach to analyze and visualize the reasons for the network's decisions. [Results] In a 10-fold cross validation on a set of about 27,000 industrial requirements, our approach achieved a macro averaged F1 score of 0.79 across all labels. For the classification into test or non-test, the approach achieves an even higher F1 score of 0.94. [Conclusions] The results show that our approach might help to increase the quality of requirements specifications with respect to the PVM attribute and guide engineers in effectively deriving test cases and verification plans.},
author = {Winkler, Jonas Paul and Gr{\"{o}}nberg, Jannis and Vogelsang, Andreas},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00023},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Winkler, Jonas Paul and Gr{\_}{\_}{\_}{\_}o{\_}{\_}nberg, Jannis and Vogelsang, Andreas{\_} {\_}{\_}Predicting how to test requirements{\_} An automated approach{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Machine Learning,Natural Language Processing,Neural Networks,Requirements Engineering,Requirements Validation,Test Engineering,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {120--130},
title = {{Predicting how to test requirements: An automated approach}},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{7972273,
abstract = {This paper describes a tool evidence be used to formalize the software requirements. This formalization is based on Z-language. The tool converts the informal to formal Z-language. For this the informal requirements are written in well-defined templates. The tool focuses the requirements and converts it to Z-specification automatically. Test cases are generated to express the correct and completeness of the requirements using the FASTEST tool for the derived z specifications.},
author = {Madhan, V. and Kalaiselvi, V. K.G. and Donald, J. P.},
booktitle = {Proceedings of the 2017 2nd International Conference on Computing and Communications Technologies, ICCCT 2017},
doi = {10.1109/ICCCT2.2017.7972273},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Madhan, V. and Kalaiselvi, V. K.G. and Donald, J. P.{\_} {\_}{\_}Tool development for formalizing the requirement for the safety critical software engineering process{\_}{\_} (2017).pdf:pdf},
isbn = {9781509062201},
keywords = {Formal Methods,Natural language,SRS,Software specifications,Z-Schema,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {feb},
pages = {161--164},
title = {{Tool development for formalizing the requirement for the safety critical software engineering process}},
year = {2017}
}
@inproceedings{10.1145/2591062.2591193,
abstract = {In this paper, we present an approach to derive structured requirements models from textual use case requirements in the form of process diagrams and ontology using methods based on computational linguistics. These proposed requirements models are capable of modelling both structural and behavioural entities present in a use case. We consider a corpus containing 123 actual requirements use cases created by Infosys Ltd. and translated them to process diagrams and ontology. For evaluating the performance of conversion we propose a few metrics and show that on average our linguistic engine miss-identified 2{\%} of actions and missed out only 3{\%} of the actions described in the input text. Copyright {\textcopyright} 2014 ACM.},
address = {New York, NY, USA},
author = {Sawant, Kiran Prakash and Roy, Suman and Sripathi, Srivibha and Plesse, Fran{\c{c}}ois and Sajeev, A. S.M.},
booktitle = {36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings},
doi = {10.1145/2591062.2591193},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sawant, Kiran Prakash and Roy, Suman and Sripathi, Srivibha and Plesse, Fran{\_}{\_}c{\_}c{\_}{\_}ois and Sajeev, A. S.M.{\_} {\_}{\_}Deriving requirements model from textual use cases{\_}{\_} (2014).pdf:pdf},
isbn = {9781450327688},
keywords = {Business process diagrams,Ontology,Requirements engineering,Requirements model,Textual use cases,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {235--244},
publisher = {Association for Computing Machinery},
series = {ICSE Companion 2014},
title = {{Deriving requirements model from textual use cases}},
url = {https://doi.org/10.1145/2591062.2591193},
year = {2014}
}
@conference{Jain2020,
abstract = {The requirements engineering process is a crucial stage of the software development life cycle. It involves various stakeholders from different professional backgrounds, particularly in the requirements elicitation phase. Each stakeholder carries distinct domain knowledge, causing them to differently interpret certain words, leading to cross-domain ambiguity. This can result in misunderstanding amongst them and jeopardize the entire project. This paper proposes a natural language processing approach to find potentially ambiguous words for a given set of domains. The idea is to apply linear transformations on word embedding models trained on different domain corpora, to bring them into a unified embedding space. The approach then finds words with divergent embeddings as they signify a variation in the meaning across the domains. It can help a requirements analyst in preventing misunderstandings during elicitation interviews and meetings by defining a set of potentially ambiguous terms in advance. The paper also discusses certain problems with the existing approaches and discusses how the proposed approach resolves them.},
annote = {cited By 0},
archivePrefix = {arXiv},
arxivId = {1910.12956},
author = {Jain, Vaibhav and Malhotra, Ruchika and Jain, Sanskar and Tanwar, Nishant},
booktitle = {CEUR Workshop Proceedings},
eprint = {1910.12956},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Jain, Vaibhav and Malhotra, Ruchika and Jain, Sanskar and Tanwar, Nishant{\_} {\_}{\_}Cross-domain ambiguity detection using linear transformation of word embedding spaces{\_}{\_} (2020).pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Cross-domain ambiguity detection using linear transformation of word embedding spaces}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082653472{\&}partnerID=40{\&}md5=d81d9ab5ee8295ba5ce682d8c55e1cb7},
volume = {2584},
year = {2020}
}
@inproceedings{8864199,
abstract = {To specify good requirements, system analysts need to understand the domain knowledge of the system. There are several techniques in requirements elicitation to improve domain knowledge understanding, such as user interviews, questioners, document analysis, and brainstorming. Most of these techniques require profound stakeholder involvement. However, not all software projects can do this task due to limited time or availability of stakeholders. In agile software development, the user story is the de facto standard used for capturing and writing functional requirements. The user story is an appropriate format and easy to understand for writing the results of requirements elicitation. This study purposes a conceptual model to extract user story from online news for improving domain knowledge understanding. The information in the online news contained lesson learned related to certain events. This information may improve the functionality of the software products. The user story consists of three aspects, namely: who, what, and why. Aspect of who represents the role or user, aspect of what shows the purpose or feature, while the aspect of why explains the reason. This format can summarize the lessons learned in the news. Our experimental results indicate that this conceptual model can extract user story from online news. The model manages to extract 105 user stories from 92 aspects of what/why candidate and 109 aspects of who candidate.},
author = {Raharjana, Indra Kharisma and Siahaan, Daniel and Fatichah, Chastine},
booktitle = {JCSSE 2019 - 16th International Joint Conference on Computer Science and Software Engineering: Knowledge Evolution Towards Singularity of Man-Machine Intelligence},
doi = {10.1109/JCSSE.2019.8864199},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Raharjana, Indra Kharisma and Siahaan, Daniel and Fatichah, Chastine{\_} {\_}{\_}User Story Extraction from Online News for Software Requirements Elicitation{\_} A Conceptual Model{\_}{\_} (2019).pdf:pdf},
isbn = {9781728107196},
issn = {2642-6579},
keywords = {agile software development,ieee{\_}inc{\_}nlp{\_}x{\_}re,information retrieval,natural language processing,requirements elicitation,scopus{\_}inc{\_}nlp{\_}x{\_}re,software requirements,user stories},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {342--347},
title = {{User Story Extraction from Online News for Software Requirements Elicitation: A Conceptual Model}},
year = {2019}
}
@inproceedings{7337623,
abstract = {Nominalizations in natural language requirements specifications can lead to imprecision. For example, in the phrase "transportation of pallets" it is unclear who transports the pallets from where to where and how. Guidelines for requirements specifications therefore recommend avoiding nominalizations. However, not all nominalizations are problematic. We present an industrial-strength text analysis tool called DeNom, which detects problematic nominalizations and reports them to the user for reformulation. DeNom uses Stanford's parser and the Cyc ontology. It classifies nominalizations as problematic or acceptable by first detecting all nominalizations in the specification and then subtracting those which are sufficiently specified within the sentence through word references, attributes, nominal phrase constructions, etc. All remaining nominalizations are incompletely specified, and are therefore prone to conceal complex processes. These nominalizations are deemed problematic. A thorough evaluation used 10 real-world requirements specifications from Daimler AG consisting of 60,000 words. DeNom identified over 1,100 nominalizations and classified 129 of them as problematic. Only 45 of which were false positives, resulting in a precision of 66{\%}. Recall was 88{\%}. In contrast, a naive nominalization detector would overload the user with 1,100 warnings, a thousand of which would be false positives.},
author = {L{\"{a}}ndh{\"{a}}u{\ss}er, Mathias and K{\"{o}}rner, Sven J. and Tichy, Walter F. and Keim, Jan and Krisch, Jennifer},
booktitle = {2nd International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2015 - Proceedings},
doi = {10.1109/AIRE.2015.7337623},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/L{\_}{\_}{\_}{\_}A{\~{}}1.PDF:PDF},
isbn = {9781509001255},
keywords = {Context,Distortion,Headphones,Inspection,Natural languages,Pragmatics,Writing,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {9--16},
title = {{DeNom: A tool to find problematic nominalizations using NLP}},
year = {2015}
}
@conference{Ibrahim2010200,
abstract = {The automation of class generation from natural language requirements is highly challenging. This paper proposes a method and a tool to facilitate requirements analysis process and class diagram extraction from textual requirements supporting natural language processing NLP and Domain Ontology techniques. Requirements engineers analyze requirements manually to come out with analysis artifacts such as class diagram. The time spent on the analysis and the low quality of human nalysis proved the need of automated support. A "Requirements Analysis and Class Diagram Extraction (RACE)" is a desktop instrument to assist requirements analysts and SE students in analyzing textual requirements, finding core concepts and its relationships, and step by step extraction of the class diagram. The evaluation of RACE system is in the process and will be conducted using two forms of evaluation, experimental and expert evaluation. {\textcopyright} 2010 IEEE.},
annote = {cited By 23},
author = {Ibrahim, Mohd and Ahmad, Rodina},
booktitle = {2nd International Conference on Computer Research and Development, ICCRD 2010},
doi = {10.1109/ICCRD.2010.71},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ibrahim Class diagram extraction from textual requirements using natural language processing (NLP) techniques.pdf:pdf},
isbn = {9780769540436},
keywords = {Domain Ontology,Natural language processing (NLP),UML Class Diagram,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {200--204},
title = {{Class diagram extraction from textual requirements using natural language processing (NLP) techniques}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955200725{\&}doi=10.1109{\%}2FICCRD.2010.71{\&}partnerID=40{\&}md5=e052f93a3055902ae74a9ae29f60c09e},
year = {2010}
}
@incollection{cordeiro_specqua_2015,
abstract = {Requirements specifications describe multiple technical concerns of a system and are used throughout the project life-cycle to help sharing a system's common understanding among multiple stakeholders. The interest to support the definition and the management of system requirements specifications (SRSs) is evident by the diversity of many generic and RE-specific tools. However, little work has been done in what concerns the quality of SRSs. Indeed, most recommended practices are mainly focused on human-intensive tasks, mainly dependent on domain experts, and so, these practices tend to be time-consuming, error-prone and unproductive. This paper proposes and discusses an innovative approach to mitigate this status, and defends that with proper tool support – such as the SpecQua framework discussed in the paper –, we can increase the overall quality of SRSs as well as we can increase the productivity associated to traditional tasks of RE such as documentation and validation.},
address = {Cham},
annote = {Series Title: Lecture Notes in Business Information Processing},
author = {{Da Silva}, Alberto Rodrigues},
booktitle = {Lecture Notes in Business Information Processing},
doi = {10.1007/978-3-319-22348-3_15},
editor = {Cordeiro, Jos{\'{e}} and Hammoudi, Slimane and Maciaszek, Leszek and Camp, Olivier and Filipe, Joaquim},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/da-silva- SpecQua Towards a framework for requirements specifications with increased quality.pdf:pdf},
isbn = {9783319223476},
issn = {18651348},
keywords = {Quality of requirements specification,Requirements specification,Requirements validation,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {265--281},
publisher = {Springer International Publishing},
shorttitle = {SpecQua},
title = {{SpecQua: Towards a framework for requirements specifications with increased quality}},
url = {http://link.springer.com/10.1007/978-3-319-22348-3{\_}15},
volume = {227},
year = {2015}
}
@inproceedings{7815601,
abstract = {Requirements analysts can model regulated data practices to identify and reason about risks of non-compliance. If terminology is inconsistent or ambiguous, however, these models and their conclusions will be unreliable. To study this problem, we investigated an approach to automatically construct an information type ontology by identifying information type hyponymy in privacy policies using Tregex patterns. Tregex is a utility to match regular expressions against constituency parse trees, which are hierarchical expressions of natural language clauses, including noun and verb phrases. We discovered the Tregex patterns by applying content analysis to 15 privacy policies from three domains (shopping, telecommunication and social networks) to identify all instances of information type hyponymy. From this dataset, three semantic and four syntactic categories of hyponymy emerged based on category completeness and word-order. Among these, we identified and empirically evaluated 26 Tregex patterns to automate the extraction of hyponyms from privacy policies. The patterns identify information type hypernym-hyponym pairs with an average precision of 0.83 and recall of 0.52 across our dataset of 15 policies.},
author = {Bhatia, Jaspreet and Evans, Morgan C. and Wadkar, Sudarshan and Breaux, Travis D.},
booktitle = {2016 IEEE 24th International Requirements Engineering Conference Workshops (REW)},
doi = {10.1109/rew.2016.018},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bhatia, Jaspreet and Evans, Morgan C. and Wadkar, Sudarshan and Breaux, Travis D.{\_} {\_}{\_}Automated Extraction of Regulated Information Types Using Hyponymy Relations{\_}{\_} (2017).pdf:pdf},
keywords = {authoring systems,ieee{\_}inc{\_}nlp{\_}x{\_}re,information analysis,law adminis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {19--25},
title = {{Automated Extraction of Regulated Information Types Using Hyponymy Relations}},
year = {2017}
}
@inproceedings{10.1145/3220228.3220247,
abstract = {Use case is a model delivered by requirements engineering phase, which is considered as an input to the forthcoming design phase and test phase. A use case model is a simplest representation of an actor's interactions with the system in which the user is involved. The development of a use case model requires the finding out the use case itself and the actor that uses this use case to interact with the system. These two tasks are achieved manually via analyst's experience, who starts with different sources of data to develop use case model. User requirements document is a common source of data that may be started with to develop use case model. The extracting of actors and their actions (use cases) is subjected to the linguistic properties of each on. The aim of this paper is to define a new algorithmic approach for extracting actors and their use cases by using thematic role technique. This algorithmic approach had been manually tested using known examples, and shown its validity. The success of this technique will lead to develop an Intelligent Computer Aided Software Engineering (I-CASE) tool that automatically extracts actions and actors of use case model from functional requirements by using Semantic Role Labelling (SRL) of Natural Language Processing (NLP) approach.},
address = {New York, NY, USA},
author = {Jebril, Eyad M. and Imam, Ayad Tareq and Al-Fayuomi, Mohammad},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3220228.3220247},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Jebril, Eyad M. and Imam, Ayad Tareq and Al-Fayuomi, Mohammad{\_} {\_}{\_}An Algorithmic Approach to Extract Actions and Actors (AAEAA){\_}{\_} (2018).pdf:pdf},
isbn = {9781450364454},
keywords = {I-CASE,NLP,Semantic role labeling,Sequence diagram,Software engineering,Software requirements,Thematic role,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {13--17},
publisher = {Association for Computing Machinery},
series = {ICGDA '18},
title = {{An Algorithmic Approach to Extract Actions and Actors (AAEAA)}},
url = {https://doi.org/10.1145/3220228.3220247},
year = {2018}
}
@article{Ali2019,
abstract = {Social network services allow a large population of end-users of software products to publicly share their concerns and experiences about software systems. From a software engineering perspective, such data can be collected and analyzed to help software development organizations to infer users' emerging demands, receive their feedback, and plan the rapid evolution of software product lines. For the evolution of software product lines, organizations supplement emerging requirements in their products to meet user's needs and also to retain their dominance in the market. Therefore, social network services, being a communication channel, have supported a number of software development activities such as requirements engineering. It has supported software development organizations to cope with numerous limitations of the traditional requirements engineering approaches by eliciting, prioritizing, and negotiating user requirements. However, these approaches do not consider eliciting requirements in terms of variability and commonality while identifying requirements. To address this issue, we have proposed a social network service-based requirement engineering process. It considers the attributes of users' opinions to determine variability and commonality. In order to justify our proposed approach, a controlled experiment was conducted on a sample set of end-users on Facebook and Twitter. The experimental results show that the team using the proposed approach performed better in terms of efficiency and effectiveness than the team that used a traditional requirements engineering approach.},
annote = {cited By 0},
author = {Ali, Nazakat and Hong, Jang Eui},
doi = {10.3390/app9193944},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ali, Nazakat and Hong, Jang Eui{\_} {\_}{\_}Value-oriented requirements{\_} Eliciting domain requirements from social network services to evolve software product lines{\_}{\_} (2019).pdf:pdf},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Requirement engineering,Requirements elicitation,Social networks,Software product lines,Text analytics,Variability and commonality,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {19},
title = {{Value-oriented requirements: Eliciting domain requirements from social network services to evolve software product lines}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073285093{\&}doi=10.3390{\%}2Fapp9193944{\&}partnerID=40{\&}md5=54b2bd7ec0b5e814395343b47d55f3d6},
volume = {9},
year = {2019}
}
@inproceedings{6681336,
abstract = {Context, Boilerplates have long been used in Requirements Engineering (RE) to increase the precision of natural language requirements and to avoid ambiguity problems caused by unrestricted natural language. When boilerplates are used, an important quality assurance task is to verify that the requirements indeed conform to the boilerplates. Objective. If done manually, checking conformance to boilerplates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to requirements changes. Our objective is to provide automation for checking conformance to boilerplates using a Natural Language Processing (NLP) technique, called Text Chunking, and to empirically validate the effectiveness of the automation. Method. We use an exploratory case study, conducted in an industrial setting, as the basis for our empirical investigation. Results. We present a generalizable and tool-supported approach for boilerplate conformance checking. We report on the application of our approach to the requirements document for a major software component in the satellite domain. We compare alternative text chunking solutions and argue about their effectiveness for boilerplate conformance checking. Conclusion. Our results indicate that: (1) text chunking provides a robust and accurate basis for checking conformance to boilerplates, and (2) the effectiveness of boilerplate conformance checking based on text chunking is not compromised even when the requirements glossary terms are unknown. This makes our work particularly relevant to practice, as many industrial requirements documents have incomplete glossaries. {\textcopyright} 2013 IEEE.},
author = {Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank and Gnaga, Raul},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2013.13},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/ARORA{\_}{\~{}}1.PDF:PDF},
issn = {19493770},
keywords = {Case Study Research,Natural Language Processing (NLP),Requirement Boilerplates,Text Chunking,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {35--44},
title = {{Automatic checking of conformance to requirement boilerplates via text chunking: An industrial case study}},
year = {2013}
}
@article{Lucassen2017339,
abstract = {Extracting conceptual models from natural language requirements can help identify dependencies, redundancies, and conflicts between requirements via a holistic and easy-to-understand view that is generated from lengthy textual specifications. Unfortunately, existing approaches never gained traction in practice, because they either require substantial human involvement or they deliver too low accuracy. In this paper, we propose an automated approach called Visual Narrator based on natural language processing that extracts conceptual models from user story requirements. We choose this notation because of its popularity among (agile) practitioners and its focus on the essential components of a requirement: Who? What? Why? Coupled with a careful selection and tuning of heuristics, we show how Visual Narrator enables generating conceptual models from user stories with high accuracy. Visual Narrator is part of the holistic Grimm method for user story collaboration that ranges from elicitation to the interactive visualization and analysis of requirements.},
annote = {cited By 18},
author = {Lucassen, Garm and Robeer, Marcel and Dalpiaz, Fabiano and van der Werf, Jan Martijn E.M. and Brinkkemper, Sjaak},
doi = {10.1007/s00766-017-0270-1},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/LUCASS{\~{}}3.PDF:PDF},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Case study,Conceptual model visualization,Conceptual modeling,NLP,Requirements engineering,User stories,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {339--358},
title = {{Extracting conceptual models from user stories with Visual Narrator}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019539096{\&}doi=10.1007{\%}2Fs00766-017-0270-1{\&}partnerID=40{\&}md5=13f828392b416c4c8a6fa448617a85c8},
volume = {22},
year = {2017}
}
@inproceedings{10.1145/1858996.1859046,
abstract = {Automated trace retrieval methods can significantly reduce the cost and effort needed to create and maintain requirements traces. However, the set of generated traces is generally quite imprecise and must be manually evaluated by analysts. In applied settings when the retrieval algorithm is unable to find the relevant links for a given query, a human user can improve the trace results by manually adding additional search terms and filtering out unhelpful ones. However, the effectiveness of this approach is largely dependent upon the knowledge of the user. In this paper we present an automated technique for replacing the original query with a new set of query terms. These query terms are learned through seeding a web-based search with the original query and then processing the results to identify a set of domain-specific terms. The query-mining algorithm was evaluated and fine-tuned using security regulations from the USA government's Health Insurance Privacy and Portability Act (HIPAA) traced against ten healthcare related requirements specifications. {\textcopyright} 2010 ACM.},
address = {New York, NY, USA},
author = {Gibiec, Marek and Czauderna, Adam and Cleland-Huang, Jane},
booktitle = {ASE'10 - Proceedings of the IEEE/ACM International Conference on Automated Software Engineering},
doi = {10.1145/1858996.1859046},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Gibiec, Marek and Czauderna, Adam and Cleland-Huang, Jane{\_} {\_}{\_}Towards mining replacement queries for hard-to-retrieve traces{\_}{\_} (2010).pdf:pdf},
isbn = {9781450301169},
keywords = {Requirements traceability,Trace retrieval,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {245--254},
publisher = {Association for Computing Machinery},
series = {ASE '10},
title = {{Towards mining replacement queries for hard-to-retrieve traces}},
url = {https://doi.org/10.1145/1858996.1859046},
year = {2010}
}
@inproceedings{10.1145/1852786.1852810,
abstract = {Though very important in software engineering, linking artifacts of the same type (clone detection) or of different types (traceability recovery) is extremely tedious, error-prone and requires significant effort. Past research focused on supporting analysts with mechanisms based on Natural Language Processing (NLP) to identify candidate links. Because a plethora of NLP techniques exists, and their performances vary among contexts, it is important to characterize them according to the provided level of support. The aim of this paper is to characterize a comprehensive set of NLP techniques according to the provided level of support to human analysts in detecting equivalent requirements. The characterization consists on a case study, featuring real requirements, in the context of an Italian company in the defense and aerospace domain. The major result from the case study is that simple NLP are more precise than complex ones. {\textcopyright} 2010 ACM.},
address = {New York, NY, USA},
author = {Falessi, Davide and Cantone, Giovanni and Canfora, Gerardo},
booktitle = {ESEM 2010 - Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1145/1852786.1852810},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Falessi, Davide and Cantone, Giovanni and Canfora, Gerardo{\_} {\_}{\_}A comprehensive characterization of NLP techniques for identifying equivalent requirements{\_}{\_} (2010).pdf:pdf},
isbn = {9781450300391},
keywords = {acm{\_}e2{\_}gdpr{\_}x{\_}nlp,acm{\_}inc{\_}nlp{\_}x{\_}re,case study,natural language processing,requirements},
mendeley-tags = {acm{\_}e2{\_}gdpr{\_}x{\_}nlp,acm{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ESEM '10},
title = {{A comprehensive characterization of NLP techniques for identifying equivalent requirements}},
url = {https://doi.org/10.1145/1852786.1852810},
year = {2010}
}
@article{Rosadini2017344,
abstract = {Context and motivation: In the railway safety-critical domain requirements documents have to abide to strict quality criteria. Rule-based natural language processing (NLP) techniques have been developed to automatically identify quality defects in natural language requirements. However, the literature is lacking empirical studies on the application of these techniques in industrial settings. Question/problem: Our goal is to investigate to which extent NLP can be practically applied to detect defects in the requirements documents of a railway signalling manufacturer. Principal idea/results: To address this goal, we first identified a set of typical defects classes, and, for each class, an engineer of the company implemented a set of defect-detection patterns by means of the GATE tool for text processing. After a preliminary analysis, we applied the patterns to a large set of 1866 requirements previously annotated for defects. The output of the patterns was further inspected by two domain experts to check the false positive cases. Contribution: This is one of the first works in which defect detection NLP techniques are applied on a very large set of industrial requirements annotated by domain experts. We contribute with a comparison between traditional manual techniques used in industry for requirements analysis, and analysis performed with NLP. Our experience tells that several discrepancies can be observed between the two approaches. The analysis of the discrepancies offers hints to improve the capabilities of NLP techniques with company specific solutions, and suggests that also company practices need to be modified to effectively exploit NLP tools.},
annote = {cited By 27},
author = {Rosadini, Benedetta and Ferrari, Alessio and Gori, Gloria and Fantechi, Alessandro and Gnesi, Stefania and Trotta, Iacopo and Bacherini, Stefano},
doi = {10.1007/978-3-319-54045-0_24},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/ROSADI{\~{}}1.PDF:PDF},
isbn = {9783319540443},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Ambiguity,Defect detection,NLP,Quality,Requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {344--360},
title = {{Using NLP to detect requirements defects: An industrial experience in the railway domain}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013922541{\&}doi=10.1007{\%}2F978-3-319-54045-0{\_}24{\&}partnerID=40{\&}md5=2d83f83b7081602869ca796e96477bf0},
volume = {10153 LNCS},
year = {2017}
}
@inproceedings{8491169,
abstract = {Software development is fault-prone especially during the fuzzy phases (requirements and design). Software inspections are commonly used in industry to detect and fix problems in requirements and design artifacts thereby mitigating the fault propagation to later phases where same faults are harder to find and fix. The output of an inspection process is natural language (NL) reviews that report the location and description of faults in software requirements specification document (SRS). The artifact author must manually read through the reviews and differentiate between true-faults and false-positives before fixing the faults. The time spent in making effective post-inspection decisions (number of true faults and deciding whether to re-inspect) could be spent in doing actual development work. The goal of this research is to automate the validation of inspection reviews, finding common patterns that describe high-quality requirements, identify fault prone requirements pre-inspection, and interrelated requirements to assist fixation of reported faults post-inspection. To accomplish these goals, this research employs various classification approaches, NL processing with semantic analysis and mining solutions from graph theory to requirement reviews and NL requirements. Initial results w.r.t. validation of inspection reviews have shown that our proposed approaches were able to successfully categorize useful and non-useful reviews.},
author = {Singh, Maninder},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00062},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/singh maninder Automated validation of requirement reviews A machine learning approach.pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {Classification,High quality requirements,Interrelated requirements,Part of speech tags,Requirement inspections,Semantic analysis,Topic modeling,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {460--465},
title = {{Automated validation of requirement reviews: A machine learning approach}},
year = {2018}
}
@conference{DiThommazo201426,
abstract = {One of the most commonly used ways to represent requirements traceability is the requirements traceability matrix (RTM). The difficulty of manually creating it motivates investigation into alternatives to generate it automatically. This article presents two approaches to automatically creating the RTM using artificial intelligence techniques: RTM-Fuzzy, based on fuzzy logic and RTM-N, based on neural networks. They combine two other approaches, one based on functional requirements entry data (RTM-E) and the other based on natural language processing (RTM-NLP). The RTMs were evaluated through an experimental study and the approaches were improved using a genetic algorithm and a decision tree. On average, the approaches that used fuzzy logic and neural networks to combine RTM-E and RTM-NLP had better results compared with RTM-E and RTM-NLP singly. The results show that artificial intelligence techniques can enhance effectiveness for determining the requirement's traceability links. Copyright {\textcopyright} 2014 SCITEPRESS - Science and Technology Publications.},
annote = {cited By 1},
author = {{Di Thommazo}, Andr{\'{e}} and Rovina, Rafael and Ribeiro, Thiago and Olivatto, Guilherme and Hernandes, Elis and Wernek, Vera and Fabbri, Sandra},
booktitle = {ICEIS 2014 - Proceedings of the 16th International Conference on Enterprise Information Systems},
doi = {10.5220/0004879600260038},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/di thommazo Using artificial intelligence techniques to enhance traceability links.pdf:pdf},
isbn = {9789897580284},
keywords = {Fuzzy Logic,Requirements Engineering,Requirements Management Techniques,Software Engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {26--38},
title = {{Using artificial intelligence techniques to enhance traceability links}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902314755{\&}partnerID=40{\&}md5=72b83bf592bcea43c89db1442ee3f5ca},
volume = {2},
year = {2014}
}
@article{Dalpiaz2018119,
abstract = {[Context and motivation] Identifying requirements defects such as ambiguity and incompleteness is an important and challenging task in requirements engineering (RE). [Question/Problem] We investigate whether combining humans' cognitive and analytical capabilities with automated reasoning is a viable method to support the identification of requirements quality defects. [Principal ideas/results] We propose a tool-supported approach for pinpointing terminological ambiguities between viewpoints as well as missing requirements. To do so, we blend natural language processing (conceptual model extraction and semantic similarity) with information visualization techniques that help interpret the type of defect. [Contribution] Our approach is a step forward toward the identification of ambiguity and incompleteness in a set of requirements, still an open issue in RE. A quasi-experiment with students, aimed to assess whether our tool delivers higher accuracy than manual inspection, suggests a significantly higher recall but does not reveal significant differences in precision.},
annote = {cited By 9},
author = {Dalpiaz, Fabiano and van der Schalk, Ivor and Lucassen, Garm},
doi = {10.1007/978-3-319-77243-1_8},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Dalpiaz, Fabiano and van der Schalk, Ivor and Lucassen, Garm{\_} {\_}{\_}Pinpointing ambiguity and incompleteness in requirements engineering via information visualization and NLP{\_}{\_} (2018).pdf:pdf},
isbn = {9783319772424},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Ambiguity,Information visualization,Natural language processing,Requirements engineering,User stories,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {119--135},
title = {{Pinpointing ambiguity and incompleteness in requirements engineering via information visualization and NLP}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043375447{\&}doi=10.1007{\%}2F978-3-319-77243-1{\_}8{\&}partnerID=40{\&}md5=26bc6432d6acef64a78637cc781f86bd},
volume = {10753 LNCS},
year = {2018}
}
@conference{Iftikhar2017706,
abstract = {This paper presents an approach to automate the procedure of textual entailment recognition from business rules. Business rules are most important part of software requirements specifications, as little mistake in this phase results in absurd software design. Business rules are used in software industry. When we automatic translate these business rules we find entailment issue because business rule is not an independent sentence they have many module related to each other. so when we translate these business text we have found many issues such as discourse, semantic and negation problem. The evaluation method for business rules texts is to test against a list of sentences, each of which is paired with yes or no. For this case I have study business text and their problems in my MS thesis. What business rules are and what issues are found when we automatic translate these business rules. We used Stanford dependency parser for text translation.},
annote = {cited By 0},
author = {Iftikhar, Erum and Iftikhar, Anum and Mehmood, Muhammad Khalid},
booktitle = {2016 6th International Conference on Innovative Computing Technology, INTECH 2016},
doi = {10.1109/INTECH.2016.7845104},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Iftikhar, Erum and Iftikhar, Anum and Mehmood, Muhammad Khalid{\_} {\_}{\_}Identification of textual entailments in business rules{\_}{\_} (2017).pdf:pdf},
isbn = {9781509020003},
keywords = {Textual entailment,business rules,natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {706--711},
title = {{Identification of textual entailments in business rules}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015276745{\&}doi=10.1109{\%}2FINTECH.2016.7845104{\&}partnerID=40{\&}md5=b023850b4dd07ea64e7692a1cbd8d8fb},
year = {2017}
}
@inproceedings{8049151,
abstract = {As different types of user feedback are becoming available, from a variety of sources and in large amount, several analysis techniques have been developed with the purpose of extracting information that can be useful for requirements engineering purposes. For instance, automated extraction and prioritization of feature requests have been recently investigated for the specific case of app development, where the key prioritization criterion is value for the user. For other types of software applications and services, software evolution relies on multi-criteria requirements prioritization, which may take into account different stakeholders' perspectives, thus leading to a complex decision-making problem. Different automated reasoning techniques have been proposed to support multi-criteria requirements prioritization, aimed at reducing human effort and improving the quality of the resulting ranking of the candidate requirements.The goal of our research is to understand how we can exploit user feedback in tool-supported multi-criteria requirements prioritization processes. Towards this objective, we discuss the properties of user feedback which are relevant for requirements prioritization, formulate a multi-criteria requirements prioritization problem, and outline a possible solution that integrates state of the art automated reasoning techniques which we extend to cope with information derived from user feedback.},
author = {Morales-Ramirez, Itzel and Munante, Denisse and Kifetew, Fitsum and Perini, Anna and Susi, Angelo and Siena, Alberto},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.41},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/MORALE{\~{}}2.PDF:PDF},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,multi-criteria automated requirements prioritizati,requirements prioritization,user feedback},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {424--429},
title = {{Exploiting User Feedback in Tool-Supported Multi-criteria Requirements Prioritization}},
year = {2017}
}
@conference{Furnari2018,
abstract = {[Context {\&} motivation] Requirements Engineering (RE) is considered as one of the most critical phases in software development. Inside RE, interdependency detection and requirements reuse are areas that could be improved and that have been of interest for the research community. [Problem] Similarity detection is an activity that emerges in the context of natural language requirements. This activity can be used for interdependency detection and requirements reuse. Although there exist several software components to detect similar texts in English, creating the setup to test them is time-consuming and difficult. [Principal ideas/results] In this paper, we present ORSIM (OpenReq-Similarity), a tool which integrates different existing similarity detection components in the same platform. These components are: Cortical, Gensim, ParallelDots, and Semilar. [Contribution] ORSIM enables requirements engineers to concentrate on evaluating and choosing the similarity detection component that best suits their user's data rather than worrying about the technical setup of these components.},
annote = {cited By 1},
author = {Furnari, Carlos Adri{\'{a}}n and Palomares, Cristina and Franch, Xavier},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/furnari ORSIM Integrating existing software components to detect similar natural language requirements.pdf:pdf},
issn = {16130073},
keywords = {Natural language processing,Paraphrasing detection,Requirements engineering,Similarity detection,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{ORSIM: Integrating existing software components to detect similar natural language requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045460332{\&}partnerID=40{\&}md5=324a30a4551f387935556d9643b43f5b},
volume = {2075},
year = {2018}
}
@conference{Tsarfaty20141296,
abstract = {We present a model for the automatic semantic analysis of requirements elicitation documents. Our target semantic representation employs live sequence charts, a multi-modal visual language for scenariobased programming, which can be directly translated into executable code. The architecture we propose integrates sentencelevel and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context. We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static (entities, properties) and dynamic (behavioral scenarios) requirements in the document.},
annote = {cited By 1},
author = {Tsarfaty, Reut and Pogrebezky, Ilia and Weiss, Guy and Natan, Yaarit and Szekely, Smadar and Harel, David},
booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/d14-1136},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/TSARFA{\~{}}1.PDF:PDF},
isbn = {9781937284961},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1296--1307},
title = {{Semantic parsing using content and context: A case study from requirements elicitation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961368443{\&}partnerID=40{\&}md5=c99793f74125aac73203a6a5b6a1c5fd},
year = {2014}
}
@inproceedings{ISI:000380489700072,
abstract = {This paper presents a solution to a requirements reuse problem that utilises natural language processing and information retrieval technique. We proposed a semi-Automated approach to extract the software features from online software review to assist the process to reuse natural language requirements. We have conducted an experiment to compare the manual feature extraction versus the semi-Automated feature extraction. We used compilations of software review from the Internet as a source of this extraction process. The extracted software features are compared against the features obtained manually by human and the evaluation results obtained in terms of time, precision, recall, and F-Measure indicate a promising result.},
address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
annote = {2nd International Conference on Information Science and Security (ICISS
2015), Seoul, SOUTH KOREA, DEC 14-16, 2015},
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
booktitle = {2015 IEEE 2nd International Conference on InformationScience and Security, ICISS 2015},
doi = {10.1109/ICISSEC.2015.7371034},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah{\_} {\_}{\_}Terms extractions{\_} An approach for requirements reuse{\_}{\_} (2016).pdf:pdf},
isbn = {9781467386111},
keywords = {Features extraction,Requirements engineering,Software reuse,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {wos{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
organization = {IEEE; IEEE Comp Soc; Korea Convergence Security Soc; Korea Ind Security Forum; Seoul Metropolitan Govt; Korean Federat Sci Technol Soc; CMSS},
pages = {188--191},
publisher = {IEEE},
title = {{Terms extractions: An approach for requirements reuse}},
type = {Proceedings Paper},
year = {2016}
}
@article{Dalpiaz201955,
abstract = {[Context {\&} Motivation] App store reviews are a rich source for analysts to elicit requirements from user feedback, for they describe bugs to be fixed, requested features, and possible improvements. Product development teams need new techniques that help them make real-time decisions based on user feedback. [Question/Problem] Researchers have proposed natural language processing (NLP) techniques for extracting and organizing requirements-relevant knowledge from the reviews for one specific app. However, no attention has been paid to studying whether and how requirements can be identified from competing products. [Principal ideas/results] We propose RE-SWOT, a tool-supported method for eliciting requirements from app store reviews through competitor analysis. RE-SWOT combines NLP algorithms with information visualization techniques. We evaluate the usefulness of RE-SWOT with expert product managers from three mobile app companies. [Contribution] Our preliminary results show that competitor analysis is a promising path for research that has direct impact on the requirements engineering practice in modern app development companies.},
annote = {cited By 1},
author = {Dalpiaz, Fabiano and Parente, Micaela},
doi = {10.1007/978-3-030-15538-4_4},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Dalpiaz, Fabiano and Parente, Micaela{\_} {\_}{\_}RE-SWOT{\_} From User Feedback to Requirements via Competitor Analysis{\_}{\_} (2019).pdf:pdf},
isbn = {9783030155377},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {CrowdRE,Natural language processing,Requirements analytics,Requirements engineering,SWOT analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {55--70},
title = {{RE-SWOT: From User Feedback to Requirements via Competitor Analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064037076{\&}doi=10.1007{\%}2F978-3-030-15538-4{\_}4{\&}partnerID=40{\&}md5=446975a362b9a191ca3d7312a882401f},
volume = {11412 LNCS},
year = {2019}
}
@article{Gleich2010218,
abstract = {[Context and motivation] Natural language is the main representation means of industrial requirements documents, which implies that requirements documents are inherently ambiguous. There exist guidelines for ambiguity detection, such as the Ambiguity Handbook [1]. In order to detect ambiguities according to the existing guidelines, it is necessary to train analysts. [Question/problem] Although ambiguity detection guidelines were extensively discussed in literature, ambiguity detection has not been automated yet. Automation of ambiguity detection is one of the goals of the presented paper. More precisely, the approach and tool presented in this paper have three goals: (1) to automate ambiguity detection, (2) to make plausible for the analyst that ambiguities detected by the tool represent genuine problems of the analyzed document, and (3) to educate the analyst by explaining the sources of the detected ambiguities. [Principal ideas/results] The presented tool provides reliable ambiguity detection, in the sense that it detects four times as many genuine ambiguities as than an average human analyst. Furthermore, the tool offers high precision ambiguity detection and does not present too many false positives to the human analyst. [Contribution] The presented tool is able both to detect the ambiguities and to explain ambiguity sources. Thus, besides pure ambiguity detection, it can be used to educate analysts, too. Furthermore, it provides a significant potential for considerable time and cost savings and at the same time quality improvements in the industrial requirements engineering. {\textcopyright} 2010 Springer-Verlag.},
annote = {cited By 66},
author = {Gleich, Benedikt and Creighton, Oliver and Kof, Leonid},
doi = {10.1007/978-3-642-14192-8_20},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Gleich, Benedikt and Creighton, Oliver and Kof, Leonid{\_} {\_}{\_}Ambiguity detection{\_} Towards a tool explaining ambiguity sources{\_}{\_} (2010).pdf:pdf},
isbn = {3642141919},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {ambiguity detection,natural language processing,requirements analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {218--232},
title = {{Ambiguity detection: Towards a tool explaining ambiguity sources}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955464250{\&}doi=10.1007{\%}2F978-3-642-14192-8{\_}20{\&}partnerID=40{\&}md5=b8aab8508fd6f145efe978454c7d7c44},
volume = {6182 LNCS},
year = {2010}
}
@article{Steinberger2016243,
abstract = {Analyzing differences among software artifacts is beneficial in a variety of scenarios, such as feasibility study, configuration management, and software product line engineering. Currently variability analysis is mainly done based on artifacts developed in a certain development phase (most notably, requirements engineering). We will demonstrate a tool that utilizes both functional requirements and test cases in order to analyze variability more comprehensively. The tool implements the ideas of SOVA R-TC method.},
annote = {cited By 1},
author = {Steinberger, Michal and Reinhartz-Berger, Iris and Tomer, Amir},
doi = {10.1007/978-3-319-47717-6_21},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Steinberger, Michal and Reinhartz-Berger, Iris and Tomer, Amir{\_} {\_}{\_}A tool for analyzing variability based on functional requirements and testing artifacts{\_}{\_} (2016).pdf:pdf},
isbn = {9783319477169},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Feature diagrams,Natural language processing,Ontology,Software product line engineering,Variability analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {243--250},
title = {{A tool for analyzing variability based on functional requirements and testing artifacts}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995921720{\&}doi=10.1007{\%}2F978-3-319-47717-6{\_}21{\&}partnerID=40{\&}md5=c6be5a722c446deb18acf82cec349743},
volume = {9975 LNCS},
year = {2016}
}
@inproceedings{10.1145/3239235.3267441,
abstract = {Identifying relationships between requirements described in natural language (NL) is a difcult task in requirements engineering (RE). This paper presents a novel approach that uses Semantic Frames in FrameNet to fnd the relationships between requirements. Our initial validation shows that the approach is promising, with an FScore of 83{\%}. Our next step is to use the approach to identify implicit requirements relationships and fnding requirements traceability links.},
address = {New York, NY, USA},
author = {Alhoshan, Waad and Zhao, Liping and Batista-Navarro, Riza},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1145/3239235.3267441},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/alhosan2018.pdf:pdf},
isbn = {9781450358231},
issn = {19493789},
keywords = {Frame Embeddings,FrameNet,Requirement Engineering,Semantic Frame,Semantic Relatedness,Word Embeddings,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ESEM '18},
title = {{Using semantic frames to identify related textual requirements: An initial validation}},
url = {https://doi.org/10.1145/3239235.3267441},
year = {2018}
}
@inproceedings{7916978,
abstract = {Automated or semi-automated analysis of requirements specification documents, expressed in Natural Language (NL), has always been desirable. An important precursor to this goal is the identification and correction of potentially ambiguous requirements statements. Pronominal Anaphora ambiguity is one such type of pragmatic or referential ambiguity in NL requirements, which needs attention. However, identification of such ambiguous requirements statements is a challenging task since the count of such statements is relatively lower. We present a solution to this challenge by considering the task as that of a classification problem to classify ambiguous requirements statements having pronominal anaphora ambiguity from a corpus of potentially ambiguous requirements statements with pronominal anaphora ambiguity. We show how a classifier can be trained in semi-supervised manner to detect such instances of pronominal anaphoric ambiguous requirements statements. Our study indicates a recall of 95{\%} with Bayesian network classification algorithm.},
author = {Sharma, Richa and Sharma, Nidhi and Biswas, K. K.},
booktitle = {Proceedings - 4th International Conference on Applied Computing and Information Technology, 3rd International Conference on Computational Science/Intelligence and Applied Informatics, 1st International Conference on Big Data, Cloud Computing, Data Science},
doi = {10.1109/ACIT-CSII-BCD.2016.043},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sharma, Richa and Sharma, Nidhi and Biswas, K. K.{\_} {\_}{\_}Machine Learning for Detecting Pronominal Anaphora Ambiguity in NL Requirements{\_}{\_} (2017).pdf:pdf},
isbn = {9781509048717},
keywords = {Ambiguity,Anaphora ambiguity,Machine learning,Requirements analysis,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {177--182},
title = {{Machine Learning for Detecting Pronominal Anaphora Ambiguity in NL Requirements}},
year = {2017}
}
@inproceedings{10.1109/ASE.2013.6693146,
abstract = {Tool support for automatically constructing analysis models from the natural language specification of requirements (NLR) is critical to model driven development (MDD), as it can bring forward the use of precise formal languages from the coding to the specification phase in the MDD lifecycle. TRAM provides such a support through a novel approach. By using a set of conceptual patterns to facilitate the transformation of an NLR to its target software model, TRAM has shown its potential as an automated tool to support the earliest phase of MDD. This paper describes TRAM and evaluates the tool against three benchmark approaches. {\textcopyright} 2013 IEEE.},
author = {Letsholo, Keletso J. and Zhao, Liping and Chioasca, Erol Valeriu},
booktitle = {2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
doi = {10.1109/ASE.2013.6693146},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Letsholo, Keletso J. and Zhao, Liping and Chioasca, Erol Valeriu{\_} {\_}{\_}TRAM{\_} A tool for transforming textual requirements into analysis models{\_}{\_} (2013).pdf:pdf},
isbn = {9781479902156},
keywords = {Model transformation,acm{\_}inc{\_}nlp{\_}x{\_}re,analysis models,conceptual patterns,natural language processing,semantic object models},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {738--741},
publisher = {IEEE Press},
series = {ASE'13},
title = {{TRAM: A tool for transforming textual requirements into analysis models}},
url = {https://doi.org/10.1109/ASE.2013.6693146},
year = {2013}
}
@inproceedings{6611715,
abstract = {While all systems have non-functional requirements (NFRs), they may not be explicitly stated in a formal requirements specification. Furthermore, NFRs may also be externally imposed via government regulations or industry standards. As some NFRs represent emergent system proprieties, those NFRs require appropriate analysis and design efforts to ensure they are met. When the specified NFRs are not met, projects incur costly re-work to correct the issues. The goal of our research is to aid analysts in more effectively extracting relevant non-functional requirements in available unconstrained natural language documents through automated natural language processing. Specifically, we examine which document types (data use agreements, install manuals, regulations, request for proposals, requirements specifications, and user manuals) contain NFRs categorized to 14 NFR categories (e.g. capacity, reliability, and security). We measure how effectively we can identify and classify NFR statements within these documents. In each of the documents evaluated, we found NFRs present. Using a word vector representation of the NFRs, a support vector machine algorithm performed twice as effectively compared to the same input to a multinomial na{\"{i}}ve Bayes classifier. Our k-nearest neighbor classifier with a unique distance metric had an F1 measure of 0.54, outperforming in our experiments the optimal na{\"{i}}ve Bayes classifier which had a F1 measure of 0.32. We also found that stop word lists beyond common determiners had no minimal performance effect. {\textcopyright} 2013 IEEE.},
author = {Slankas, John and Williams, Laurie},
booktitle = {2013 1st International Workshop on Natural Language Analysis in Software Engineering, NaturaLiSE 2013 - Proceedings},
doi = {10.1109/NAturaLiSE.2013.6611715},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Slankas, John and Williams, Laurie{\_} {\_}{\_}Automated extraction of non-functional requirements in available documentation{\_}{\_} (2013).pdf:pdf},
isbn = {9781467362719},
keywords = {classification,documentation,ieee{\_}inc{\_}nlp{\_}x{\_}re,machine learning,natural language processing,non-functional requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {9--16},
title = {{Automated extraction of non-functional requirements in available documentation}},
year = {2013}
}
@inproceedings{7320434,
abstract = {Data-driven Natural Language Processing (NLP) methods have noticeably advanced in the past few years. These advances can be tied to the drastic growth of the quality of collaborative knowledge bases (KB) available on the World Wide Web. Such KBs contain vast amounts of up-to-date structured human knowledge and common sense data that can be exploited by NLP methods to discover otherwise-unseen semantic dimensions in text, aiding in tasks related to natural language understanding, classification, and retrieval. Motivated by these observations, we describe our research agenda for exploiting online human knowledge in Requirements Engineering (RE). The underlying assumption is that requirements are a product of the human domain knowledge that is expressed mainly in natural language. In particular, our research is focused on methods that exploit the online encyclopedia Wikipedia as a textual corpus. Wikipedia provides access to a massive number of real-world concepts organized in hierarchical semantic structures. Such knowledge can be analyzed to provide automated support for several exhaustive RE activities including requirements elicitation, understanding, modeling, traceability, and reuse, across multiple application domains. This paper describes our preliminary findings in this domain, current state of research, and prospects of our future work.},
author = {Mahmoud, Anas and Carver, Doris},
booktitle = {2015 IEEE 23rd International Requirements Engineering Conference, RE 2015 - Proceedings},
doi = {10.1109/RE.2015.7320434},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Mahmoud, Anas and Carver, Doris{\_} {\_}{\_}Exploiting online human knowledge in Requirements Engineering{\_}{\_} (2015).pdf:pdf},
isbn = {9781467369053},
issn = {2332-6441},
keywords = {Electronic publishing,Encyclopedias,Internet,NLP,Ontologies,Semantics,Software,Web sites,Wikipedia,World Wide Web,collaborative knowledge bases,data-driven natural language processing,groupware,ieee{\_}inc{\_}nlp{\_}x{\_}re,knowledge based systems,natural language processing,online encyclopedia,online human knowledge,requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,semantic dimensions,systems analysis,textual corpus,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {262--267},
title = {{Exploiting online human knowledge in Requirements Engineering}},
year = {2015}
}
@incollection{hutchison_automated_2010,
abstract = {Use cases are commonly used to structure and document requirements while UML activity diagrams are often used to visualize and formalize use cases, for example to support automated test case generation. Therefore the automated support for the transition from use cases to activity diagrams would provide significant, practical help. Additionally, traceability could be established through automated transformation, which could then be used for instance to relate requirements to design decisions and test cases. In this paper, we propose an approach to automatically generate activity diagrams from use cases while establishing traceability links. Data flow information can also be generated and added to these activity diagrams. Our approach is implemented in a tool, which we used to perform five case studies. The results show that high quality activity diagrams can be generated. Our analysis also shows that our approach outperforms existing academic approaches and commercial tools. {\textcopyright} 2010 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Yue, Tao and Briand, Lionel C. and Labiche, Yvan},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-13595-8_26},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard and K{\"{u}}hne, Thomas and Selic, Bran and Gervais, Marie-Pierre and Terrier, Fran{\c{c}}ois},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Yue, Tao and Briand, Lionel C. and Labiche, Yvan{\_} {\_}{\_}An automated approach to transform use cases into activity diagrams{\_}{\_} (2010).pdf:pdf},
isbn = {3642135943},
issn = {03029743},
keywords = {Activity Diagram,Automation,Natural Language Processing,Traceability,Transformation,UML,Use Case,Use Case Modeling,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {337--353},
publisher = {Springer Berlin Heidelberg},
title = {{An automated approach to transform use cases into activity diagrams}},
url = {http://link.springer.com/10.1007/978-3-642-13595-8{\_}26},
volume = {6138 LNCS},
year = {2010}
}
@inproceedings{5540935,
abstract = {In a distributed environment, non-technical stakeholders are required to write down requirement statements by themselves. Nature language is the first choice for them. In order to alleviate the burden of reading free-text requirement documents by requirements engineers, we extract goals and relevant stakeholders from requirement statements automatically by a computer-assisted way. In this paper, requirements are divided into system level requirements and instance level requirements. Methods are proposed to solve two types of requirements by analyzing the characteristics of requirement expressions, and combining techniques of nature language processing with semantic web. Semantic-enhanced segment and domain sentence pattern are two novel techniques utilized in our methods. Our approach accelerates goal extraction from text-based requirements and alleviates the burden of requirements engineers significantly. {\textcopyright} 2010 IEEE.},
author = {Chen, Huafeng and He, Keqing and Liang, Peng and Li, Rong},
booktitle = {2010 International Conference on Computer Design and Applications, ICCDA 2010},
doi = {10.1109/ICCDA.2010.5540935},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Chen, Huafeng and He, Keqing and Liang, Peng and Li, Rong{\_} {\_}{\_}Text-based requirements preprocessing using nature language processing techniques{\_}{\_} (2010).pdf:pdf},
isbn = {9781424471638},
keywords = {Domain ontology,Nature language processing,Non-technical stakeholder,Requirements preprocessing,Wiki,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {V1--14--V1--18},
title = {{Text-based requirements preprocessing using nature language processing techniques}},
volume = {1},
year = {2010}
}
@conference{Böschen2016,
abstract = {In this paper, we discuss the problem of transforming a natural language requirements specification into a formal specification. We present several methods to support the process and implemented them in a commercial tool, the Requirements Quality Suite. We achieve this by enriching the requirement text with additional structure (using a knowledge base) and asking the requirement engineer to formulate the requirements in Boilerplates. The additional structure is used to analyze the requirements automatically or semi-automatically leading finally to a formal specification. The formal specification then enables verification activities, such as testing or formal analysis. We discuss our methods by examples from an industrial case study and report on our experiences.},
annote = {cited By 1},
author = {B{\"{o}}schen, Martin and Bogusch, Ralf and Fraga, Anabel and Rudat, Christian},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/boscher Bridging the gap between natural language requirements and formal specifications.pdf:pdf},
issn = {16130073},
keywords = {Boilerplates,Formalization,Requirement patterns,Requirements,Testing,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Bridging the gap between natural language requirements and formal specifications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964692769{\&}partnerID=40{\&}md5=7a068ece59f871118b511dad36c2c2a3},
volume = {1564},
year = {2016}
}
@conference{Rossanez2016123,
abstract = {Problems with the specification of software requirements documents are a common cause of software defects. In the space applications domain, such defects are very costly, especially when detected after deployment in the field. It is imperative to ensure that software requirements are well written to avoid the introduction of these defects. The quality of software requirements is frequently assessed via checklists, based on standards for space application software, and on problems found in previous projects. Given the importance of quality assessment, and the fact that it is manually performed by domain experts, we propose to develop a semi-automatic, natural language processing tool, to diminish the reviewer's effort in the assessment, and to reduce errors in this process.},
annote = {cited By 1},
author = {Rossanez, Anderson and Carvalho, Ariadne M.B.R.},
booktitle = {Proceedings - 7th Latin-American Symposium on Dependable Computing, LADC 2016},
doi = {10.1109/LADC.2016.26},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rossanez, Anderson and Carvalho, Ariadne M.B.R.{\_} {\_}{\_}Semi-automatic checklist quality assessment of natural language requirements for space applications{\_}{\_} (2016).pdf:pdf},
isbn = {9781509051205},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {123--126},
title = {{Semi-automatic checklist quality assessment of natural language requirements for space applications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013218951{\&}doi=10.1109{\%}2FLADC.2016.26{\&}partnerID=40{\&}md5=3a36da26c3fc5ce4a90981fc7e184712},
year = {2016}
}
@conference{Santos2019,
abstract = {Online user feedback about software products is a promising source of user requirements. To allow scaling analyses to large amounts of user feedback, research on Crowd-based Requirements Engineering (CrowdRE) seeks to tailor natural language processing (NLP) techniques to Requirements Engineering (RE). Various frameworks have been proposed, but it remains largely unclear why particular NLP techniques are better suited for CrowdRE than others, which makes it hard to make a well-founded choice for a technique. We found that CrowdRE research most often uses machine learning (ML) and has so far applied twelve clusters of ML algorithms and seven clusters of ML features. The prevalent algorithm–feature pair is Na{\"{i}}ve Bayes with Bag of Words – Term Frequency (BOW-TF), followed by Support Vector Machines (SVM) with BOW-TF. An initial comparison of the reported precision and recall suggests that classifications in RE need further improvement. Our research presents a preliminary overview of the current landscape of automated classification techniques for RE whose results may inspire researchers to apply new strategies to advance research in this field, or to include ML models they had not considered previously in their benchmarks.},
annote = {cited By 0},
author = {Santos, Rubens and Groen, Eduard C. and Villela, Karina},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/santos An overview of user feedback classification approaches.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{An overview of user feedback classification approaches}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068033480{\&}partnerID=40{\&}md5=13d93d66ef8490dd3b8752aeda8a6fa5},
volume = {2376},
year = {2019}
}
@inproceedings{6345795,
abstract = {Stakeholders frequently use speculative language when they need to convey their requirements with some degree of uncertainty. Due to the intrinsic vagueness of speculative language, speculative requirements risk being misunderstood, and related uncertainty overlooked, and may benefit from careful treatment in the requirements engineering process. In this paper, we present a linguistically-oriented approach to automatic detection of uncertainty in natural language (NL) requirements. Our approach comprises two stages. First we identify speculative sentences by applying a machine learning algorithm called Conditional Random Fields (CRFs) to identify uncertainty cues. The algorithm exploits a rich set of lexical and syntactic features extracted from requirements sentences. Second, we try to determine the scope of uncertainty. We use a rule-based approach that draws on a set of hand-crafted linguistic heuristics to determine the uncertainty scope with the help of dependency structures present in the sentence parse tree. We report on a series of experiments we conducted to evaluate the performance and usefulness of our system. {\textcopyright} 2012 IEEE.},
author = {Yang, Hui and {De Roeck}, Anne and Gervasi, Vincenzo and Willis, Alistair and Nuseibeh, Bashar},
booktitle = {2012 20th IEEE International Requirements Engineering Conference, RE 2012 - Proceedings},
doi = {10.1109/RE.2012.6345795},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/YANG{\_}H{\~{}}1.PDF:PDF},
isbn = {9781467327855},
issn = {2332-6441},
keywords = {Uncertainty,ieee{\_}inc{\_}nlp{\_}x{\_}re,machine learning,natural language requirements,rule-based approach,scopus{\_}inc{\_}nlp{\_}x{\_}re,speculative requirements,uncertainty cues,uncertainty scopes},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {11--20},
title = {{Speculative requirements: Automatic detection of uncertainty in natural language requirements}},
year = {2012}
}
@inproceedings{6912279,
abstract = {Requirements are a part of every project life cycle; everything going forward in a project depends on them. The VARED tool chain aims to provide an integrated environment to analyze and verify the requirements and early design of a system. Natural language requirements are processed automatically into formal specifications using a state model of the system under design and its environment. The specifications are formally checked and then are used to verify the controller model meets the requirements.},
author = {Badger, Julia and Throop, David and Claunch, Charles},
booktitle = {2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings},
doi = {10.1109/RE.2014.6912279},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/badger2014.pdf:pdf},
isbn = {9781479930333},
issn = {2332-6441},
keywords = {formal specification,formal verification,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural l},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {325--326},
title = {{VARED: Verification and analysis of requirements and early designs}},
year = {2014}
}
@inproceedings{8920644,
abstract = {Defects in requirements specifications can have severe consequences during the software development lifecycle. Some of them result in overall project failure due to incorrect or missing quality characteristics such as security. There are several concerns that make security difficult to deal with; for instance, (1) when stakeholders discuss general requirements in meetings, they are often unaware that they should also discuss security-related topics, and (2) they typically do not have enough expertise in security. This often leads to unspecified or ill-defined security-related aspects. These concerns become even more challenging in agile contexts, where lightweight documentation is typically involved. The goal of this paper is to design and evaluate an approach for reviewing security-related aspects in agile requirements specifications of web applications. The approach considers user stories and security specifications as input and relates those user stories to security properties via Natural Language Processing. Based on the related security properties, our approach then identifies high-level security requirements from the Open Web Application Security Project to be verified and generates a reading technique to support reviewers in detecting defects. We evaluate our approach via two controlled experiment trials. We compare the effectiveness and efficiency of novice inspectors verifying security aspects in agile requirements using our approach against using the complete list of high-level security requirements. The (statistically significant) results indicate that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency.},
author = {Villamizar, Hugo and {Anderlin Neto}, Amadeu and Kalinowski, Marcos and Garcia, Alessandro and M{\'{e}}ndez, Daniel},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00020},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/VILLAM{\~{}}1.PDF:PDF},
isbn = {9781728139128},
issn = {23326441},
keywords = {Agile requirements,Requirements verification,Software inspection,Software security,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {86--97},
title = {{An approach for reviewing security-related aspects in agile requirements specifications of web applications}},
volume = {2019-Septe},
year = {2019}
}
@conference{Hassan2017459,
abstract = {In the recent past, domain specific solutions for detailed semantic analysis have got acceptable by natural language processing community and use of applications involving natural language based user interface. Different approaches that has been previously used is focusing on quality of text and improving the text contents by adding semantic information with text then the existing approaches used for semantic analysis can provide better results. In this, an approach was presented to address the problem of non-availability of semantic information required for better semantic analysis. This problem is solved by using semantic technology to annotate text of software requirements expressed in a natural language with their domain specific semantics and investigate the effect of semantic analysis with attached semantics. The presented approach uses a semantic framework specifically designed for interpretation and detailed semantic analysis of natural language software requirement specifications. The used framework is based on semantic technology involves knowledge extracted from existing software requirement documents and knowledge extracted from existing applications. The presented approach shows that by adapting and combing existing ontologies to support knowledge management, developing system and performing experiments on requirement of real world software systems. In this approach start with software requirement specification, after this clean the irrelevant requirements, convert the cleaned requirements into graph that represents inter related different elements. Represent the requirement graph into sparse matrix, after these all steps; we generate ontology with the help of OntoGen tool.},
annote = {cited By 2},
author = {Hassan, Taimoor and Hassan, Shoaib and Yar, Muhammad Asfand and Younas, Waleed},
booktitle = {2016 6th International Conference on Innovative Computing Technology, INTECH 2016},
doi = {10.1109/INTECH.2016.7845013},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Hassan, Taimoor and Hassan, Shoaib and Yar, Muhammad Asfand and Younas, Waleed{\_} {\_}{\_}Semantic analysis of natural language software requirement{\_}{\_} (2017).pdf:pdf},
isbn = {9781509020003},
keywords = {Natural Language Processing,Ontology,Semantic Technology,Software Requirement Specification,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {459--463},
title = {{Semantic analysis of natural language software requirement}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015300821{\&}doi=10.1109{\%}2FINTECH.2016.7845013{\&}partnerID=40{\&}md5=2c6d1a24ff6950a82b20f9113c25e9e7},
year = {2017}
}
@article{Mokammel2018555,
abstract = {The quality of requirements is fundamental in engineering projects. Requirements are usually expressed partly or totally in a natural language (NL) format and come from different documents. Their qualities are difficult to analyze manually, especially when hundreds of thousands of them have to be considered. The assistance of software tools is becoming a necessity. In this article, the goal was to develop a set of metrics supported by NL processing (NLP) methods supporting different types of analysis of requirements and especially the dependencies between requirements. An NLP approach is used to extract requirements from text; to analyze their quality, links, similarities, and contradictions; and to cluster them automatically. The analysis framework includes different combinations of methods such as cosine similarity, singular value decomposition, and K-means clustering. One objective is to assess the possible combinations and their impacts on detections to establish optimal metrics. Three case studies exemplify and support the validation of the work. Graphs are used to represent the automatically clustered requirements, as well as similarities and contradictions. A new contradiction analysis process that includes a rules-based approach is proposed. Finally, the combined results are presented as graphs, which unveil the semantic relationships between requirements. Subsection 4.8 compares the results provided by the tool and the results obtained from experts. The proposed methodology and network presentation not only support the understanding of the semantics of the requirements but also help requirements engineers to review the interconnections and consistency of requirements systems and manage traceability. The approach is valuable during the early phases of projects when requirements are evolving dynamically and rapidly.},
annote = {cited By 2},
author = {Mokammel, Faisal and Coatan{\'{e}}a, Eric and Coatan{\'{e}}a, Joonas and Nenchev, Vladislav and Blanco, Eric and Pietola, Matti},
doi = {10.1002/sys.21461},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/mokammel Automatic requirements extraction, analysis, and graph representation using an approach derived from computational linguistics.pdf:pdf},
issn = {15206858},
journal = {Systems Engineering},
keywords = {contradictions analysis,network representation,requirements management,scopus{\_}inc{\_}nlp{\_}x{\_}re,similarity},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {6},
pages = {555--575},
title = {{Automatic requirements extraction, analysis, and graph representation using an approach derived from computational linguistics}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051070662{\&}doi=10.1002{\%}2Fsys.21461{\&}partnerID=40{\&}md5=3eda877d27e29b86325972739debf3d8},
volume = {21},
year = {2018}
}
@inproceedings{10.5555/2691365.2691483,
abstract = {This tutorial paper summarizes selective research results from the field of automated requirement engineering. Automatization is achieved by employing natural language processing techniques. We show algorithms that work directly on the natural language text and algorithms that translate natural language text to formal models. To ensure quality, we further illustrate verification algorithms that can proof correctness of the extracted formal models.},
author = {Drechsler, Rolf and Soeken, Mathias and Wille, Robert},
booktitle = {IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
doi = {10.1109/ICCAD.2014.7001410},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Drechsler, Rolf and Soeken, Mathias and Wille, Robert{\_} {\_}{\_}Automated and quality-driven requirements engineering{\_}{\_} (2015).pdf:pdf},
isbn = {9781479962785},
issn = {10923152},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {January},
pages = {586--590},
publisher = {IEEE Press},
series = {ICCAD '14},
title = {{Automated and quality-driven requirements engineering}},
volume = {2015-Janua},
year = {2015}
}
@inproceedings{7107202,
abstract = {Requirements analysis is the most important phase of the software life cycle process. Some studies have shown the most faults of software are from the requirements phase. Therefore, the quality of Software Requirements Specification has become the key to project success, which correctness, consistency, no ambiguity of software requirements specification is more important. This paper presents a classification method based on natural language processing techniques and grey similar correlation. The first step of this method is that keywords refined from various functional requirements through segmentation of natural language processing, thus made up of heavy weight vector based on the weight of functional requirements, such a functional requirement corresponds to a weight vector. The second step is that the related technology of grey system is used to compute grey correlation coefficient between two weight vectors, in order to construct a correlation matrix. Finally, the appropriate statistical tools are used to classify functional requirements statements. The clustering results based on this method can provide work guidance for requirement analysts, software developers, software testers, software maintenance.},
author = {Hu, Wensheng and Wu, Shiyi and Zhao, Ming and Yang, Jianfeng},
booktitle = {ICRMS 2014 - Proceedings of 2014 10th International Conference on Reliability, Maintainability and Safety: More Reliable Products, More Secure Life},
doi = {10.1109/ICRMS.2014.7107202},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/hu Requires analysis based on software maintainability.pdf:pdf},
isbn = {9781479919925},
keywords = {Classify,Grey clustering,Requries,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {354--357},
title = {{Requires analysis based on software maintainability}},
year = {2014}
}
@inproceedings{8049149,
abstract = {Security threats should be identified in the early phases of a project so that design solutions can be explored and mitigating requirements specified. In this paper, we present a crowd-sourcing approach for creating Personae non Gratae (PnGs), which model attack goals and techniques of unwanted, potentially malicious users. We present a proof of concept study that takes a diverse collection of potentially redundant PnGs and merges them into a single set. Our approach combines machine learning techniques and visualization. It is illustrated and evaluated using a collection of PnGs collected from undergraduate students for a drone-based rescue scenario. Lessons learned from the proof of concept study are discussed and lay the foundations for future work.},
author = {Mead, Nancy and Shull, Forrest and Spears, Janine and Heibl, Stefan and Weber, Sam and Cleland-Huang, Jane},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.63},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/MEAD{\_}N{\~{}}1.PDF:PDF},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {Personae non gratae,Security Requirements,Threat Modeling,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {412--417},
title = {{Crowd Sourcing the Creation of Personae Non Gratae for Requirements-Phase Threat Modeling}},
year = {2017}
}
@article{Li2015181,
abstract = {[Context and motivation] Validating natural language requirements is an important but difficult task. Although there are techniques available for validating formalized requirements, the gap between natural language requirements and formalism is huge. [Question/problem] As part of a larger piece of work on temporal requirements consistency checking, we developed a front end to semi-automatically translate natural language requirements into an formal language called Temporal Action Language or TeAL. This work is based on an underlying assumption that human analysts can assist us in filling in the missing pieces as we translate natural language temporal requirements to TeAL.[Principal ideas/results] We performed a study to validate this assumption. We found that using the statements generated by our frontend tool appears to be more effective and efficient than a manual process. [Contribution] We present the design of our front-end and a study that measures the performance of human analysts in formalizing requirements with the help of an automated tool.},
annote = {cited By 2},
author = {Li, Wenbin and Hayes, Jane Huffman and Truszczy{\'{n}}ski, Miros{\l}aw},
doi = {10.1007/978-3-319-16101-3_12},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Li, Wenbin and Hayes, Jane Huffman and Truszczy{\_}{\_}'{\_}n{\_}{\_}ski, Miros{\_}{\_}l{\_}aw{\_} {\_}{\_}Towards more efficient requirements formalization{\_} A study{\_}{\_} (2015).pdf:pdf},
isbn = {9783319161006},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Formal specification,Requirement comprehension,Temporal requirements,Translation,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {181--197},
title = {{Towards more efficient requirements formalization: A study}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930459809{\&}doi=10.1007{\%}2F978-3-319-16101-3{\_}12{\&}partnerID=40{\&}md5=403d2bfa7e7fde9a3f9ab1ae29f4e303},
volume = {9013},
year = {2015}
}
@conference{Jyothilakshmi2012380,
abstract = {Domain ontology formally represents knowledge as a set of concepts within a domain, and the relationships among those concepts. This paper proposes a method to generate class diagram from functional requirement specification which is written in natural language by using Domain ontology and Natural language processing techniques. Major steps in this method are identification of nouns and verbs from requirement specification statements and linking them with the ontology. From the ontology we get information about core concepts and relationships among those concepts. Thus domain ontology helps in the identification of classes, attributes and relationships for the particular domain for which the system is to be developed. {\textcopyright} 2012 IEEE.},
annote = {cited By 0},
author = {Jyothilakshmi, M. S. and Samuel, Philip},
booktitle = {International Conference on Intelligent Systems Design and Applications, ISDA},
doi = {10.1109/ISDA.2012.6416568},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Jyothilakshmi, M. S. and Samuel, Philip{\_} {\_}{\_}Domain ontology based class diagram generation from functional requirements{\_}{\_} (2012).pdf:pdf},
isbn = {9781467351188},
issn = {21647143},
keywords = {Domain Ontology,Natural language processing UML Class Diagram,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {380--385},
title = {{Domain ontology based class diagram generation from functional requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874358230{\&}doi=10.1109{\%}2FISDA.2012.6416568{\&}partnerID=40{\&}md5=e5bfe4df1ff74430f48f17644e6b9055},
year = {2012}
}
@inproceedings{9038987,
abstract = {Describing requirements and building entity-relation diagrams are processes that cannot be avoided when creating an information system. With the advent of modern technologies, such as Natural Language Processing and Automatic Speech Recognition, opportunities for automation of the mentioned activities have appeared.The paper describes the process of creating the web-based application that allows user to generate formalized requirements for the system and its entity-relation diagram based on the informal audio description of the system. The paper describes possibilities and limitations of the offered approach. As the main result, the paper discusses architecture of the solution and possibilities of using freeware components and modules. Existing approaches and software to solve discussed tasks also described.},
author = {Timofeev, Andrey A. and Belyaev, Sergey A.},
booktitle = {Proceedings of the 2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, EIConRus 2020},
doi = {10.1109/EIConRus49466.2020.9038987},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Timofeev, Andrey A. and Belyaev, Sergey A.{\_} {\_}{\_}Architecture of Software for Automated Generation of Technical Specifications{\_}{\_} (2020).pdf:pdf},
isbn = {9781728157610},
issn = {2376-6565},
keywords = {ER-diagram,ieee{\_}inc{\_}nlp{\_}x{\_}re,machine learning,natural language processing,requirements classification},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {jan},
pages = {533--537},
title = {{Architecture of Software for Automated Generation of Technical Specifications}},
year = {2020}
}
@inproceedings{8981850,
abstract = {User feedback can be used as a tool for application developers to find out and understand users' needs, preferences, and complaints. Developers need to identify problems that arise from the user-given feedbacks, which is very difficult to do, considering the amount of feedback received every day. Reading and classifying every feedback takes a long time, and it is very ineffective. In order to overcome this problem, a sentiment analysis system based on the Multinomial Na{\"{i}}ve Bayes classification algorithm was built to determine whether a user-feedback has a positive or negative sentiment. Na{\"{i}}ve Bayes algorithm is generally used for classification because it is straightforward and effective. Based on previous research, the Multinomial Na{\"{i}}ve Bayes algorithm gives off the best performance compared to other traditional machine learning algorithms. This study aims to implement the Multinomial Na{\"{i}}ve Bayes classification algorithm on a web application and calculate the accuracy of class predictions made by the system. Based on the results of several test, the accuracy of class predictions which were evaluated using confusion matrix, shows that the model with training-testing comparison of 70:30, balanced datasets, and oversampling each dataset by 30{\%} produces the best performance, with 71.6{\%} for accuracy, 76.92{\%} for precision, 61.73{\%} for recall and 68.49{\%} for F1 score.},
author = {Wiratama, Gabriella Putri and Rusli, Andre},
booktitle = {2019 5th International Conference on New Media Studies (CONMEDIA)},
doi = {10.1109/conmedia46929.2019.8981850},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Wiratama, Gabriella Putri and Rusli, Andre{\_} {\_}{\_}Sentiment Analysis of Application User Feedback in Bahasa Indonesia Using Multinomial Naive Bayes{\_}{\_} (2020).pdf:pdf},
keywords = {data mining,ieee{\_}inc{\_}nlp{\_}x{\_}re,learning (artificial intelligence),nat},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {223--227},
title = {{Sentiment Analysis of Application User Feedback in Bahasa Indonesia Using Multinomial Naive Bayes}},
year = {2020}
}
@article{6857327,
abstract = {This paper presents a computational method that employs Natural Language Processing (NLP) and text mining techniques to support requirements engineers in extracting and modeling goals from textual documents. We developed a NLP-based goal elicitation approach within the context of KAOS goal-oriented requirements engineering method. The hierarchical relationships among goals are inferred by automatically building taxonomies from extracted goals. We use smart metering system as a case study to investigate the proposed approach. Smart metering system is an important subsystem of the next generation of power systems (smart grids). Goals are extracted by semantically parsing the grammar of goal-related phrases in abstracts of research publications. The results of this case study show that the developed approach is an effective way to model goals for complex systems, and in particular, for the research-intensive complex systems.},
author = {Casagrande, Erik and Woldeamlak, Selamawit and Woon, Wei Lee and Zeineldin, H. H. and Svetinovic, Davor},
doi = {10.1109/TSE.2014.2339811},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/CASAGR{\~{}}1.PDF:PDF},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {NLP,Requirements engineering,bibliometrics,data mining,goal elicitation,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {10},
pages = {941--956},
title = {{NLP-KAOS for systems goal elicitation: Smart metering system case study}},
volume = {40},
year = {2014}
}
@inproceedings{8933719,
abstract = {With the rise of social media like Twitter and of software distribution platforms like app stores, users got various ways to express their opinion about software products. Popular software vendors get user feedback thousandfold per day. Research has shown that such feedback contains valuable information for software development teams such as problem reports or feature and support inquires. Since the manual analysis of user feedback is cumbersome and hard to manage many researchers and tool vendors suggested to use automated analyses based on traditional supervised machine learning approaches. In this work, we compare the results of traditional machine learning and deep learning in classifying user feedback in English and Italian into problem reports, inquiries, and irrelevant. Our results show that using traditional machine learning, we can still achieve comparable results to deep learning, although we collected thousands of labels.},
archivePrefix = {arXiv},
arxivId = {1909.05504},
author = {Stanik, Christoph and Haering, Marlo and Maalej, Walid},
booktitle = {Proceedings - 2019 IEEE 27th International Requirements Engineering Conference Workshops, REW 2019},
doi = {10.1109/REW.2019.00046},
eprint = {1909.05504},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Stanik, Christoph and Haering, Marlo and Maalej, Walid{\_} {\_}{\_}Classifying multilingual user feedback using traditional machine learning and deep learning{\_}{\_} (2019).pdf:pdf},
isbn = {9781728151656},
keywords = {Data Mining,Data-Driven Requirements,Deep Learning,Machine Learning,Social Media Analytics,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {220--226},
title = {{Classifying multilingual user feedback using traditional machine learning and deep learning}},
year = {2019}
}
@article{Morales-Ramirez201994,
abstract = {Online discussions about software applications and services that take place on web-based communication platforms represent an invaluable knowledge source for diverse software engineering tasks, including requirements elicitation. The amount of research work on developing effective tool-supported analysis methods is rapidly increasing, as part of the so called software analytics. Textual messages in App store reviews, tweets, online discussions taking place in mailing lists and user forums, are processed by combining natural language techniques to filter out irrelevant data; text mining and machine learning algorithms to classify messages into different categories, such as bug report and feature request. Our research objective is to exploit a linguistic technique based on speech-acts for the analysis of online discussions with the ultimate goal of discovering requirements-relevant information. In this paper, we present a revised and extended version of the speech-acts based analysis technique, which we previously presented at CAiSE 2017, together with a detailed experimental characterisation of its properties. Datasets used in the experimental evaluation are taken from a widely used open source software project (161120 textual comments), as well as from an industrial project in the home energy management domain. We make them available for experiment replication purposes. On these datasets, our approach is able to successfully classify messages into Feature/Enhancement and Other, with F-measure of 0.81 and 0.84 respectively. We also found evidence that there is an association between types of speech-acts and categories of issues, and that there is correlation between some of the speech-acts and issue priority, thus motivating further research on the exploitation of our speech-acts based analysis technique in semi-automated multi-criteria requirements prioritisation.},
annote = {cited By 4},
author = {Morales-Ramirez, I. and Kifetew, Fitsum Meshesha and Perini, Anna},
doi = {10.1016/j.is.2018.08.003},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Morales-Ramirez, Itzel and Papadimitriou, Dimitra and Perini, Anna{\_} {\_}{\_}Crowd Intent{\_} Annotation of Intentions Hidden in Online Discussions{\_}{\_} (2015).pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {Classification techniques,Online discussions,Requirements engineering,Sentiment analysis,Speech-acts analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {94--112},
title = {{Speech-acts based analysis for requirements discovery from online discussions}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054101779{\&}doi=10.1016{\%}2Fj.is.2018.08.003{\&}partnerID=40{\&}md5=06273abeaa8215761eac646836f247e2},
volume = {86},
year = {2019}
}
@inproceedings{8874888,
abstract = {Requirements engineering is the most important stage in software engineering, one of which is to carry out specifications on requirements. Errors that occur at this stage will have a very bad impact on the next stages. A mistake that often occurs is a misunderstanding between stakeholders regarding the document specifications, and this is due to different backgrounds or fields of science. In addition, errors can also occur when making specification documents, for example, there are still non-atomic requirements in the document. Non-atomic requirements are a statement of requirements in which there is not only one element/function of the system. This research was conducted to develop a model that can detect non-atomic requirements in the software specification requirements written in natural languages. The initial stage of this research was to make a list of expert annotations (corpus) containing statements of atomic and non-atomic requirements. This Corpus later used as training data and test data in this study. Based on the corpus created, feature extraction and keyword generation carried out. The best model built in this research was produced by the classification method that used the Bayes Net algorithm. The result of the classification model was evaluated against human annotator using Cohen Kappa. The reliability of the model is considered fair for non-balance data in detecting non-atomic requirements in the software requirements specification. The reliability of the model is considered moderate for balance data in detecting non-atomic requirements.},
author = {Halim, Fahrizal and Siahaan, Daniel},
booktitle = {2019 1st International Conference on Cybernetics and Intelligent System, ICORIS 2019},
doi = {10.1109/ICORIS.2019.8874888},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Halim, Fahrizal and Siahaan, Daniel{\_} {\_}{\_}Detecting Non-Atomic Requirements in Software Requirements Specifications Using Classification Methods{\_}{\_} (2019).pdf:pdf},
isbn = {9781728114729},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language,non-atomic requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re,software requirements specifications,text classification},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {269--273},
title = {{Detecting Non-Atomic Requirements in Software Requirements Specifications Using Classification Methods}},
volume = {1},
year = {2019}
}
@conference{Tamai2018241,
abstract = {The importance of software quality requirements (QR) is being widely recognized, which motivates studies that investigate software requirements specifications (SRS) in practice and collect data on how much QR are written vs. functional requirements (FR) and what kind of QR are specified. It is useful to develop a tool that automates the process of filtering out QR statements from an SRS and classifying them into the quality characteristic attributes such as defined in the ISO/IEC 25000 quality model. We propose an approach that uses a machine learning technique to mechanize the process. With this mechanism, we can identify how each QR characteristic scatters over the document, i.e. how much in volume and in what way. A tool QRMiner is developed to support the process and case studies were conducted, taking thirteen SRS documents that were written for real use. We report our findings from these cases.},
annote = {cited By 5},
author = {Tamai, Tetsuo and Anzai, Taichi},
booktitle = {ENASE 2018 - Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering},
doi = {10.5220/0006694502410248},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/tamai Quality requirements analysis with machine learning.pdf:pdf},
isbn = {9789897583001},
keywords = {Machine learning,Natural language processing,Quality requirements,Requirements classification,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {241--248},
title = {{Quality requirements analysis with machine learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062802050{\&}partnerID=40{\&}md5=c1bce25f2b1ce98f3375a5854272df3a},
volume = {2018-March},
year = {2018}
}
@inproceedings{5719011,
abstract = {Open source projects do have requirements; they are, however, mostly informal, text descriptions found in requests, forums, and other correspondence. Understanding of such requirements can provide insight into the nature of open source projects. Unfortunately, manual analysis of natural language (NL) requirements is time-consuming, and for large projects, error-prone. Automated analysis of NL requirements, even partial, will be of great benefit. Towards that end, we describe the design and validation of an automated NL requirements classifier for open source projects. Initial results suggest that it can reduce the effort required to analyze requirements of open source projects. {\textcopyright} 2011 IEEE.},
author = {Vlas, Radu and Robinson, William N.},
booktitle = {Proceedings of the Annual Hawaii International Conference on System Sciences},
doi = {10.1109/HICSS.2011.28},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/radu A rule-based natural language technique for requirements discovery and classification in open-source software development projects.pdf:pdf},
isbn = {9780769542829},
issn = {15301605},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,knowledge based systems,natural languages,public d},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {jan},
pages = {1--10},
title = {{A rule-based natural language technique for requirements discovery and classification in open-source software development projects}},
year = {2011}
}
@inproceedings{10.1145/3121264.3121269,
abstract = {Users prefer natural language software requirements because of their usability and accessibility. Many approaches exist to elaborate these requirements and to support the users during the elicitation process. But there is a lack of adequate resources, which are needed to train and evaluate approaches for requirement refinement. We are trying to close this gap by using online available software descriptions from SourceForge and app stores. Thus, we present two real-life requirements collections based on online-available software descriptions. Our goal is to show the domain-specific characteristics of content words describing functional requirements. On the one hand, we created a semantic role-labeled requirements set, which we use for requirements classification. On the other hand, we enriched software descriptions with linguistic features and dependencies to provide evidence for the context-awareness of software functionalities.},
address = {New York, NY, USA},
author = {B{\"{a}}umer, Frederik Simon and Dollmann, Markus and Geierhos, Michaela},
booktitle = {WAMA 2017 - Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics, Co-located with FSE 2017},
doi = {10.1145/3121264.3121269},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/B{\_}{\_}{\_}{\_}A{\~{}}3.PDF:PDF},
isbn = {9781450351584},
keywords = {App Store data,Real-life requirements gathering,Semantic role labeling,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {19--25},
publisher = {Association for Computing Machinery},
series = {WAMA 2017},
title = {{Studying software descriptions in sourceforge and app stores for a better understanding of real-life requirements}},
url = {https://doi.org/10.1145/3121264.3121269},
year = {2017}
}
@incollection{zdravkovic_interactive_2015,
abstract = {Traceability links between requirements and source code can assist in software maintenance tasks. There are some automatic traceability recovery methods. Most of them are similarity-based methods recovering links by comparing representation similarity between requirements and code. They cannot work well if there are some links independent of the representation similarity. Herein to cover weakness of them and improve the accuracy of recovery, we propose a method that extends the similarity-based method using two techniques: a log-based traceability recovery method using the configuration management log and a link recommendation from user feedback. These techniques are independent of the representation similarity between requirements and code. As a result of applying our method to a large enterprise system, we successfully improved both recall and precision by more than a 20 percent point in comparison with singly applying the similarity-based method (recall: 60.2{\%} to 80.4{\%}, precision: 41.1{\%} to 64.8{\%}).},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Tsuchiya, Ryosuke and Washizaki, Hironori and Fukazawa, Yoshiaki and Oshima, Keishi and Mibe, Ryota},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-19069-3_16},
editor = {Zdravkovic, Jelena and Kirikova, Marite and Johannesson, Paul},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/TSUCHI{\~{}}2.PDF:PDF},
isbn = {9783319190686},
issn = {16113349},
keywords = {Configuration management log,Interactive method,Traceability,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {247--262},
publisher = {Springer International Publishing},
title = {{Interactive recovery of requirements traceability links using user feedback and configuration management logs}},
url = {http://link.springer.com/10.1007/978-3-319-19069-3{\_}16},
volume = {9097},
year = {2015}
}
@inproceedings{10.1145/3172871.3172879,
abstract = {Software requirements, for complex projects, often contain specifications of non-functional attributes (e.g., security-related features). The process of analyzing such requirements for compliance is laborious and error prone. Due to the inherent free-flowing nature of software requirements, it is appealing to apply Natural Language Processing (NLP) and Machine Learning (ML)-based techniques for analyzing these documents. In this paper, we propose a semi-automatic methodology that assesses the security requirements of software systems with respect to completeness and ambiguity, creating a bridge between the requirements documents and being in compliance with standards. Security standards, such as ISO and OWASP, are compared against software project documents for textual entailment relationships. These entailment results along with the document annotations are used to train a Neural Network model to predict whether a given statement in the document is found within the security standard or not. Hence, this approach aims to identify the appropriate structures that underlie software requirements documents. Once such structures are formalized and empirically validated, they will provide guidelines to software organizations for generating comprehensive and unambiguous requirements specification documents as related to security-oriented features.},
address = {New York, NY, USA},
author = {Hayrapetian, Allenoush and Raje, Rajeev},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3172871.3172879},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Hayrapetian, Allenoush and Raje, Rajeev{\_} {\_}{\_}Empirically analyzing and evaluating security features in software requirements{\_}{\_} (2018).pdf:pdf},
isbn = {9781450363983},
keywords = {Machine learning,Neural networks,Security,Software requirements,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ISEC '18},
title = {{Empirically analyzing and evaluating security features in software requirements}},
url = {https://doi.org/10.1145/3172871.3172879},
year = {2018}
}
@article{moreno_application_2020,
abstract = {It is already common to compute quantitative metrics of requirements to assess their quality. However, the risk is to build assessment methods and tools that are both arbitrary and rigid in the parameterization and combination of metrics. Specifically, we show that a linear combination of metrics is insufficient to adequately compute a global measure of quality. In this work, we propose to develop a flexible method to assess and improve the quality of requirements that can be adapted to different contexts, projects, organizations, and quality standards, with a high degree of automation. The domain experts contribute with an initial set of requirements that they have classified according to their quality, and we extract their quality metrics. We then use machine learning techniques to emulate the implicit expert's quality function. We provide also a procedure to suggest improvements in bad requirements. We compare the obtained rule-based classifiers with different machine learning algorithms, obtaining measurements of effectiveness around 85{\%}. We show as well the appearance of the generated rules and how to interpret them. The method is tailorable to different contexts, different styles to write requirements, and different demands in quality. The whole process of inferring and applying the quality rules adapted to each organization is highly automated.},
author = {Moreno, Valent{\'{i}}n and G{\'{e}}nova, Gonzalo and Parra, Eugenio and Fraga, Anabel},
doi = {10.1007/s11219-020-09511-4},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/MORENO{\~{}}1.PDF:PDF},
isbn = {0000000302},
issn = {0963-9314, 1573-1367},
journal = {Software Quality Journal},
keywords = {automatic classification,automatic improvement,experts,flexible assessment,judgment,machine learning,requirements quality,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
pages = {1--31},
title = {{Application of machine learning techniques to the flexible assessment and improvement of requirements quality}},
url = {http://link.springer.com/10.1007/s11219-020-09511-4},
year = {2020}
}
@article{Meth2015799,
abstract = {The success of information systems (IS) development strongly depends on the accuracy of the requirements gathered from users and other stakeholders. When developing a new IS, about 80 percent of these requirements are recorded in informal requirements documents (e.g., interview transcripts or discussion forums) using natural language. However, processing the resultant natural language requirements resources is inherently complex and often error prone due to ambiguity, inconsistency, and incompleteness. Thus, even highly qualified requirements engineers often struggle to process large amounts of natural language requirements resources efficiently and effectively. In this paper, we propose a design theory for requirement mining systems (RMSs) based on two design principles: (1) semi-automatic requirement mining and (2) usage of imported and retrieved knowledge. As part of an extensive design project, which led to these principles, we also implemented a prototype based on this design theory (REMINER). It supports requirements engineers in identifying and classifying requirements documented in natural language and allows us to evaluate the artifact's viability and the conceptual soundness of our design. The results of our evaluation suggest that an RMS based on our proposed design principles can significantly improve recall while maintaining precision levels.},
annote = {cited By 26},
author = {Meth, Hendrik and Mueller, Benjamin and Maedche, Alexander},
doi = {10.17705/1jais.00408},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/MethMuellerMaedche-2015-DesigningaRequirementMiningSystem.pdf:pdf},
issn = {15583457},
journal = {Journal of the Association for Information Systems},
keywords = {Advice taking,Design science research,Design theory,Requirements engineering,Requirements mining productivity,Requirements mining systems,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {9},
pages = {799--837},
title = {{Designing a requirement mining system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942851466{\&}partnerID=40{\&}md5=165c08d3e490ad33c07e823dd9a64f55},
volume = {16},
year = {2015}
}
@incollection{king_requirements_2011,
abstract = {Requirements managers aim at keeping the set of requirements consistent and up to date throughout the project by conducting the following tasks: requirements categorization, requirements conflict analysis, and requirements tracing. However, the manual conduct of these tasks takes significant effort and is error-prone. In this paper we propose to use semantic technology as foundation for automating the requirements management tasks and introduce the ontology-based reporting approach OntRep. We evaluate the effectiveness and effort the OntRep approach based on a real-world industrial empirical study with professional Austrian IT project managers. Major results were that OntRep provides reasonable capabilities for the automated categorization of requirements, was when compared to a manual approach considerably more effective to identify conflicts, and produced less false positives with similar effort. {\textcopyright} 2011 Springer-Verlag.},
address = {Cham},
annote = {Series Title: Notes on Numerical Fluid Mechanics and Multidisciplinary Design},
author = {Moser, Thomas and Winkler, Dietmar and Heindl, Matthias and Biffl, Stefan},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-21640-4_3},
editor = {King, Rudibert},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/HDASCWI4/Moser e.a. - 2011 - Requirements Management with Semantic Technology .pdf:pdf},
isbn = {9783642216398},
issn = {03029743},
keywords = {Requirements categorization,case study,consistency checking,empirical evaluation,requirements conflict analysis,requirements tracing,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {3--17},
publisher = {Springer International Publishing},
shorttitle = {Requirements {\{}Management{\}} with {\{}Semantic{\}} {\{}Technol}},
title = {{Requirements management with semantic technology: An empirical study on automated requirements categorization and conflict analysis}},
url = {http://link.springer.com/10.1007/978-3-642-21640-4{\_}3},
volume = {6741 LNCS},
year = {2011}
}
@article{BenAbdessalemKaraa20161443,
abstract = {Software development life cycle is a structured process, including the definition of user requirements specification, the system design, and programming. The design task comprises the transfer of natural language specifications into models. The class diagram of Unified Modeling Language has been considered as one of the most useful diagrams. It is a formal description of user's requirements and serves as inputs to the developers. The automated extraction of UML class diagram from natural language requirements is a highly challenging task. This paper explains our vision of an automated tool for class diagram generation from user requirements expressed in natural language. Our new approach amalgamates the statistical and pattern recognition properties of natural language processing techniques. More than 1000 patterns are defined for the extraction of the class diagram concepts. Once these concepts are captured, an XML Metadata Interchange file is generated and imported with a Computer-Aided Software Engineering tool to build the corresponding UML class diagram. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
annote = {cited By 14},
author = {{Ben Abdessalem Karaa}, Wahiba and {Ben Azzouz}, Zeineb and Singh, Aarti and Dey, Nilanjan and {S. Ashour}, Amira and {Ben Ghazala}, Henda},
doi = {10.1002/spe.2384},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/{\_}BENAB{\~{}}2.PDF:PDF},
issn = {1097024X},
journal = {Software - Practice and Experience},
keywords = {Unified Modeling Language,XML Metadata Interchange,class diagram,model driven architecture,natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,user's requirements},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {11},
pages = {1443--1458},
title = {{Automatic builder of class diagram (ABCD): an application of UML generation from functional requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952690542{\&}doi=10.1002{\%}2Fspe.2384{\&}partnerID=40{\&}md5=fca531a2b59afe00feaf46927b61856f},
volume = {46},
year = {2016}
}
@inproceedings{10.1145/3239235.3267428,
abstract = {Background Recent research on mining app reviews for software evolution indicated that the elicitation and analysis of user requirements can benefit from supplementing user reviews by data from other sources. However, only a few studies reported results of leveraging app changelogs together with app reviews. [Aims] Motivated by those findings, this exploratory experimental study looks into the role of app changelogs in the classification of requirements derived from app reviews. We aim at understanding if the use of app changelogs can lead to more accurate identification and classification of functional and non-functional requirements from app reviews. We also want to know which classification technique works better in this context. [Method] We did a case study on the effect of app changelogs on automatic classification of app reviews. Specifically, manual labeling, text preprocessing, and four supervised machine learning algorithms were applied to a series of experiments, varying in the number of app changelogs in the experimental data. [Results] We compared the accuracy of requirements classification from app reviews, by training the four classifiers with varying combinations of app reviews and changelogs. Among the four algorithms, Na?ve Bayes was found to be more accurate for categorizing app reviews. [Conclusions] The results show that official app changelogs did not contribute to more accurate identification and classification of requirements from app reviews. In addition, Na?ve Bayes seems to be more suitable for our further research on this topic.},
address = {New York, NY, USA},
author = {Wang, Chong and Zhang, Fan and Liang, Peng and Daneva, Maya and {Van Sinderen}, Marten},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1145/3239235.3267428},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/WANG{\_}C{\~{}}1.PDF:PDF},
isbn = {9781450358231},
issn = {19493789},
keywords = {App changelogs,App reviews,Data-driven requirements engineering,Machine learning,Requirements analysis,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ESEM '18},
title = {{Can app changelogs improve requirements classification from app reviews?: An exploratory study}},
url = {https://doi.org/10.1145/3239235.3267428},
year = {2018}
}
@incollection{jarzabek_nuts_2020,
abstract = {Natural language (NL) requirements documents are often ambiguous, and this is considered as a source of problems in the later interpretation of requirements. Ambiguity detection tools have been developed with the objective of improving the quality of requirement documents. However, defects as vagueness, optionality, weakness and multiplicity at requirements level can in some cases give an indication of possible variability, either in design and in implementation choices or configurability decisions. Variability information is actually the seed of the software engineering development practice aiming at building families of related systems, known as software product lines. Building on the results of previous analyses conducted on large and real word requirement documents, with QuARS NL analysis tool, we provide here a classification of the forms of ambiguity that indicate variation points, and we illustrate the practical aspects of the approach by means of a simple running example. To provide a more complete description of a line of software products, it is necessary to extrapolate, in addition to variability, also the common elements. To this end we propose here to take advantage of the capabilities of the REGICE tool to extract and cluster the glossary terms from the requirement documents. In summary, we introduce the combined application of two different NL processing tools to extract features and variability and use them to model a software product line.},
address = {Cham},
annote = {Series Title: Studies in Computational Intelligence},
author = {Arganese, Eleonora and Fantechi, Alessandro and Gnesi, Stefania and Semini, Laura},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-030-26574-8_10},
editor = {Jarzabek, Stan and Poniszewska-Mara{\'{n}}da, Aneta and Madeyski, Lech},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Arganese2020{\_}Chapter{\_}NutsAndBoltsOfExtractingVariab.pdf:pdf},
isbn = {978-3-030-26573-1 978-3-030-26574-8},
issn = {18609503},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {125--143},
publisher = {Springer International Publishing},
title = {{Nuts and Bolts of Extracting Variability Models from Natural Language Requirements Documents}},
url = {http://link.springer.com/10.1007/978-3-030-26574-8{\_}10},
volume = {851},
year = {2020}
}
@inproceedings{6632546,
abstract = {Understanding software requirements has been widely acknowledged as a crucial task in software development projects. This problem emerges since software requirements, which are originally specified using a particular natural language, are often ambiguous and incomplete. The condition will be completely different if the requirements are specified using formal language in which ambiguity and incompleteness could be obviously found and thus quickly anticipated. Transforming 'natural' software requirements into a more formal specification may therefore reduce their ambiguity and incompleteness. This paper hence proposes a method to transform software requirements specified in a natural language to formal specification (in this context is object-oriented specification). The proposed method uses natural language processing technique. {\textcopyright} 2013 IEEE.},
author = {Fatwanto, Agung},
booktitle = {2013 International Conference on Quality in Research, QiR 2013 - In Conjunction with ICCS 2013: The 2nd International Conference on Civic Space},
doi = {10.1109/QiR.2013.6632546},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Fatwanto, Agung{\_} {\_}{\_}Software requirements specification analysis using natural language processing technique{\_}{\_} (2013).pdf:pdf},
isbn = {9781467357852},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,object-oriented,scopus{\_}inc{\_}nlp{\_}x{\_}re,software requirements specification},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {105--110},
title = {{Software requirements specification analysis using natural language processing technique}},
year = {2013}
}
@conference{Liu2014785,
abstract = {Use cases, as the primary techniques in the user requirement analysis, have been widely adopted in the requirement engineering practice. As developed early, use cases also serve as the basis for function requirement development, system design and testing. Errors in the use cases could potentially lead to problems in the system design or implementation. It is thus highly desirable to detect errors in use cases. Automatically analyzing use case documents is challenging primarily because they are written in natural languages. In this work, we aim to achieve automatic defect detection in use case documents by leveraging on advanced parsing techniques. In our approach, we first parse the use case document using dependency parsing techniques. The parsing results of each use case are further processed to form an activity diagram. Lastly, we perform defect detection on the activity diagrams. To evaluate our approach, we have conducted experiments on 200+ real-world as well as academic use cases. The results show the effectiveness of our method.},
annote = {cited By 10},
author = {Liu, Shuang and Sun, Jun and Liu, Yang and Zhang, Yue and Wadhwa, Bimlesh and Dong, Jin Song and Wang, Xinyu},
booktitle = {ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
doi = {10.1145/2642937.2642969},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Liu, Shuang and Sun, Jun and Liu, Yang and Zhang, Yue and Wadhwa, Bimlesh and Dong, Jin Song and Wang, Xinyu{\_} {\_}{\_}Automatic early defects detection in use case documents{\_}{\_} (2014).pdf:pdf},
isbn = {9781450330138},
keywords = {Natural language processing,Use cases,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {785--790},
title = {{Automatic early defects detection in use case documents}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908613033{\&}doi=10.1145{\%}2F2642937.2642969{\&}partnerID=40{\&}md5=449ad10a70376fcb19aaa5174a236cf6},
year = {2014}
}
@conference{Liepin2019,
abstract = {The latest developments in natural language processing and machine learning have created new opportunities in legal text analysis. In particular, we look at the texts of online privacy policies after the implementation of the European General Data Protection Regulation (GDPR). We analyse 32 privacy policies to design a methodology for automated detection and assessment of compliance of these documents. Preliminary results confirm the pressing issues with current privacy policies and the beneficial use of this approach in empowering consumers in making more informed decisions. However, we also encountered several serious issues in the process. This paper introduces the challenges through concrete examples of context dependence, omission of information, and multilingualism.},
annote = {cited By 0},
author = {Liepin, Rūta and Contissa, Giuseppe and Drazewski, Kasper and Lagioia, Francesca and Lippi, Marco and Micklitz, Hans Wolfgang and Pa{\l}ka, Przemys{\l}aw and Sartor, Giovanni and Torroni, Paolo},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/liepina GDPR privacy policies in CLAUDETTE Challenges of omission, context and multilingualism.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}gdpr{\_}x{\_}nlp},
mendeley-tags = {scopus{\_}inc{\_}gdpr{\_}x{\_}nlp},
title = {{GDPR privacy policies in CLAUDETTE: Challenges of omission, context and multilingualism}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067852141{\&}partnerID=40{\&}md5=560aea75e98cbd2153101bbdfcb11bac},
volume = {2385},
year = {2019}
}
@conference{Malhotra201626,
abstract = {Software requirements, for complex projects, often contain specifications of non-functional attributes (e.g., security-related features). The process of analyzing such requirements is laborious and error prone. Due to the inherent free-flowing nature of software requirements, it is tempting to apply Natural Language Processing (NLP) based Machine Learning (ML) techniques for analyzing these documents from the point of view of comprehensiveness and consistency. In this paper, we propose novel semi-Automatic methodology that can assess the security requirements of the software system from the perspective of completeness, contradiction, and inconsistency. Security standards introduced by the ISO are used to construct a model for classifying security-based requirements using NLP-based ML techniques. Hence, this approach aims to identify the appropriate structures that underlie software requirement documents. Once such structures are formalized and empirically validated, they will provide guidelines to software organizations for generating comprehensive and unambiguous requirement specification documents as related to security-oriented features. The proposed solution will assist organizations during the early phases of developing secure software and reduce overall development effort and costs.},
annote = {cited By 3},
author = {Malhotra, Ruchika and Chug, Anuradha and Hayrapetian, Allenoush and Raje, Rajeev},
booktitle = {2016 1st International Conference on Innovation and Challenges in Cyber Security, ICICCS 2016},
doi = {10.1109/ICICCS.2016.7542334},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Malhotra, Ruchika and Chug, Anuradha and Hayrapetian, Allenoush and Raje, Rajeev{\_} {\_}{\_}Analyzing and evaluating security features in software requirements{\_}{\_} (2016).pdf:pdf},
isbn = {9781509020843},
keywords = {Concept Graphs,Machine Learning,Natural Language Processing,Quality of Service,Security,Software Requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {26--30},
title = {{Analyzing and evaluating security features in software requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985996112{\&}doi=10.1109{\%}2FICICCS.2016.7542334{\&}partnerID=40{\&}md5=d6f098cd8a0c9edc57edbabc9bd9ef18},
year = {2016}
}
@conference{Dollmann20161807,
abstract = {Users prefer natural language software requirements because of their usability and accessibility. When they describe their wishes for software development, they often provide off-topic information. We therefore present REaCT1, an automated approach for identifying and semantically annotating the on-topic parts of requirement descriptions. It is designed to support requirement engineers in the elicitation process on detecting and analyzing requirements in user-generated content. Since no lexical resources with domain-specific information about requirements are available, we created a corpus of requirements written in controlled language by instructed users and uncontrolled language by uninstructed users. We annotated these requirements regarding predicate-argument structures, conditions, priorities, motivations and semantic roles and used this information to train classifiers for information extraction purposes. REaCT achieves an accuracy of 92{\%} for the on- and off-topic classification task and an F1measure of 72{\%} for the semantic annotation.},
annote = {cited By 8},
author = {Dollmann, Markus and Geierhos, Michaela},
booktitle = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/dollman On eliciting requirements from end-users in the ICT4D domain.pdf:pdf},
isbn = {9781945626258},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1807--1816},
title = {{On- And off-topic classification and semantic annotation of user-generated software requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034637610{\&}partnerID=40{\&}md5=da67a58cdc8806a81ff8d36c025ad136},
year = {2016}
}
@article{Nguyen2016117,
abstract = {There is a pressing need in the modern business environment for business-supporting software products to address countless consumers' desires, where customer orientation is a key success factor. Consumer preference is thus an essential input for the requirements elicitation process of public-facing enterprise systems. Previous studies in this area have proposed a process to capture and translate consumer preferences into system-related goals using the Consumer Preference Meta-Model (CPMM) used to integrate consumer values from the marketing domain into objectives of information systems. However, there exists a knowledge gap between how this process can be automated at a large scale, when massive data sources, such as social media data, are used as inputs for the process. To address this problem, a case in which social media data related to four major US airlines is collected from Twitter, is analyzed by a set of text mining techniques and hosted in a consumer preference model, and is further translated to goal models in the ADOxx modelling platform. The analysis of experimental results revealed that the collection, recognition, model creation, and mapping of consumer preferences can be fully or partly automated. The result of this study is a semi-automated method for capturing and filtering consumer preferences as goals for system development, a method which significantly increases the efficiency of large-scale consumer data processing.},
annote = {cited By 1},
author = {Nguyen, Vu and Svee, Eric Oluf and Zdravkovic, Jelena},
doi = {10.1007/978-3-319-48393-1_9},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Nguyen, Vu and Svee, Eric Oluf and Zdravkovic, Jelena{\_} {\_}{\_}A semi-automated method for capturing consumer preferences for system requirements{\_}{\_} (2016).pdf:pdf},
isbn = {9783319483924},
issn = {18651348},
journal = {Lecture Notes in Business Information Processing},
keywords = {Consumer preferences,Occurrence analysis,Requirements engineering,Sentiment analysis,Social media,Term frequency analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {117--132},
title = {{A semi-automated method for capturing consumer preferences for system requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994882501{\&}doi=10.1007{\%}2F978-3-319-48393-1{\_}9{\&}partnerID=40{\&}md5=9afee786cc71397254ed08a4ee39a127},
volume = {267},
year = {2016}
}
@inproceedings{8590177,
abstract = {Non-functional requirements describe important constraints upon the software development and should therefore be considered and specified as early as possible during the system analysis. Effective elicitation of requirements is arguably among the most important of the resulting recommended RE practices. Recent research has shown that artificial intelligence techniques such as Machine Learning and Text Mining perform the automatic extraction and classification of quality attributes from text documents with relevant results. This paper aims to define a systematic process of dataset generation through NFR Framework catalogues improving the NFR's classification process using Machine Learning techniques. A well-known dataset (Promise) was used to evaluate the precision of our approach reaching interesting results. Regarding to security and performance we obtained a precision and recall ranging between {\~{}}85{\%} and {\~{}}98{\%}. And we achievement a F1 above {\~{}}79{\%} when classified the security, performance and usability together.},
author = {Marinho, Matheus and Arruda, Danilo and Wanderley, Fernando and Lins, Anthony},
booktitle = {Proceedings - 2018 International Conference on the Quality of Information and Communications Technology, QUATIC 2018},
doi = {10.1109/QUATIC.2018.00024},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/MARINH{\~{}}1.PDF:PDF},
isbn = {9781538658413},
keywords = {Artificial inteligence,Machine learning,NFR framework,Non-fucntional requirements,SIG,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {110--118},
title = {{A systematic approach of dataset definition for a supervised machine learning using NFR framework}},
year = {2018}
}
@inproceedings{7330171,
abstract = {Requirements elicitation is the activity of identifying facts that compose the system requirements. One of the steps of this activity is the identification of information sources, which is a time-consuming task. Text documents are typically an important and abundant information source. However, their analysis to gather useful information is also time consuming and hard to automate. Because of its characteristics, the identification of information sources and analysis of text documents are critical in time-constrained projects, which are typically addressed through agile approaches. This paper presents a strategy for time-constrained elicitation, which is based on mining GitHub content. The strategy aims the identification of information sources (similar projects) and the automatic analysis of textual documents (projects content) through text mining techniques. Furthermore, it maintains the traceability between the data mined and its sources, boosting the reuse of existing information. A tool is being created to support the strategy.},
author = {Portugal, Roxana Lisette Quintanilla and {Do Prado Leite}, Julio Cesar Sampaio and Almentero, Eduardo},
booktitle = {1st International Workshop on Just-in-Time Requirements Engineering, JIT RE 2015 - Proceedings},
doi = {10.1109/JITRE.2015.7330171},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Portugal, Roxana Lisette Quintanilla and {\_}Do Prado Leite{\_}, Julio Cesar Sampaio and Almentero, Eduardo{\_} {\_}{\_}Time-constrained requirements elicitation{\_} Reusing GitHub content{\_}{\_} (2015).pdf:pdf},
isbn = {9781509001194},
keywords = {Requirements elicitation,ieee{\_}inc{\_}nlp{\_}x{\_}re,just-in-time requirements,reuse of information,scopus{\_}inc{\_}nlp{\_}x{\_}re,text-mining,time-constrained requirements},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {5--8},
title = {{Time-constrained requirements elicitation: Reusing GitHub content}},
year = {2015}
}
@article{Knauss20114,
abstract = {Context {\&} motivation: More and more software projects today are security-related in one way or the other. Many environments are initially not considered security-related and no security experts are assigned. Requirements engineers often fail to recognise indicators for security problems. Question/problem: Ignoring security issues early in a project is a major source of recurring security problems in practice. Identifying security-relevant requirements is labour-intensive and error-prone. Security may be neglected in order to finish on time and in budget. Principal ideas/results: I , we address this problem by presenting a tool-supported method that provides assistance for requirements engineering, with an emphasis on security requirements. We investigate whether security-relevant requirements can be automatically identified using a Bayesian classifier. Our results indicate that this is feasible, in particular if the classifier is trained with domain specific data and documents from previous projects. Contribution: We show how the ability to identify security-relevant requirements can be integrated in a workflow of requirements analysis and reuse of experience. In practice, this can increase security awareness within the software development process. We discuss limitations and potential of this approach. {\textcopyright} 2011 Springer-Verlag Berlin Heidelberg.},
annote = {cited By 10},
author = {Knauss, Eric and Houmb, Siv and Schneider, Kurt and Islam, Shareeful and J{\"{u}}rjens, Jan},
doi = {10.1007/978-3-642-19858-8_2},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/CFTGLTGK/Knauss e.a. - 2011 - Supporting Requirements Engineers in Recognising S.pdf:pdf},
isbn = {9783642198571},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {empirical study,natural language processing,requirements analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re,secure software engineering,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {4--18},
title = {{Supporting requirements engineers in recognising security issues}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953098501{\&}doi=10.1007{\%}2F978-3-642-19858-8{\_}2{\&}partnerID=40{\&}md5=11dba3d7c12c0baf1147cdc9777df943},
volume = {6606 LNCS},
year = {2011}
}
@inproceedings{8933736,
abstract = {Software requirements are usually written in common natural language. An important quality criterion for each documented requirement is unambiguity. This simply means that all readers of the requirement must arrive at the same understanding of the requirement. Due to differences in the domain expertise of requirements engineer and other stakeholders of the project, it is possible that requirements contain several words that allow alternative interpretations. Our objective is to identify and detect domain specific ambiguous words in natural language text. This paper applies an NLP technique based on word embeddings to detect such ambiguous words. More specifically, we measure the ambiguity potential of most frequently used computer science (CS) words when they are used in other application areas or subdomains of engineering, e.g., aerospace, civil, petroleum, biomedical and environmental etc. Our extensive and detailed experiments with several different subdomains show that word embedding based techniques are very effective in identifying domain specific ambiguities. Our findings also demonstrate that this technique can be applied to documents of varying sizes. Finally, we provide pointers for future research.},
author = {Mishra, Siba and Sharma, Arpit},
booktitle = {Proceedings - 2019 IEEE 27th International Requirements Engineering Conference Workshops, REW 2019},
doi = {10.1109/REW.2019.00048},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Mishra, Siba and Sharma, Arpit{\_} {\_}{\_}On the use of word embeddings for identifying domain specific ambiguities in requirements{\_}{\_} (2019).pdf:pdf},
isbn = {9781728151656},
keywords = {Ambiguity,Domain,Natural language,Quality,Requirements,Word embedding,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {234--240},
title = {{On the use of word embeddings for identifying domain specific ambiguities in requirements}},
year = {2019}
}
@inproceedings{5636629,
abstract = {The identification of abstractions, i.e. terms that have a particular significance in a given domain, and such that they can indirectly characterize the most salient features of the document in which they appear, has often been recognized as a useful tool in the analysis of domain descriptions and requirements documents in software development. In this paper we propose a new technique for the identification of single- and multi-word abstractions named Relevancedriven abstraction identification (RAI) and a corresponding tool implementation, present an experiment comparing the effectiveness of our technique with human judgement and with a different technique proposed in the literature, and discuss a number of ways in which the abstractions so identified can be used to good profit in requirements engineering. {\textcopyright} 2010 IEEE.},
author = {Gacitua, Ricardo and Sawyer, Peter and Gervasi, Vincenzo},
booktitle = {Proceedings of the 2010 18th IEEE International Requirements Engineering Conference, RE2010},
doi = {10.1109/RE.2010.12},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Gacitua, Ricardo and Sawyer, Peter and Gervasi, Vincenzo{\_} {\_}{\_}On the effectiveness of abstraction identification in requirements engineering{\_}{\_} (2010).pdf:pdf},
isbn = {9780769541624},
issn = {2332-6441},
keywords = {Abstractions,Evaluation of tool,Natural language text,Requirement elicitation,Tool use method,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {5--14},
title = {{On the effectiveness of abstraction identification in requirements engineering}},
year = {2010}
}
@inproceedings{8054882,
abstract = {Most system requirements are currently written in common, i.e., unstructured, natural language, which existing requirements analysis tools are poorly equipped to handle. Extracting mentions of model elements from common natural language requirements is a first step toward the automation of model-driven requirements analysis. We propose an approach in which we identify mentions of elements of a component state transition (CST) model in natural language requirements by creating classifiers using a recurrent neural network with long short-term memory. To evaluate our approach, we performed a study on a pacemaker system requirements document, and the results show promising directions for future research.},
author = {Madala, Kaushik and Gaither, Danielle and Nielsen, Rodney and Do, Hyunsook},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.73},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Madala, Kaushik and Gaither, Danielle and Nielsen, Rodney and Do, Hyunsook{\_} {\_}{\_}Automated identification of component state transition model elements from requirements{\_}{\_} (2017).pdf:pdf},
isbn = {9781538634882},
keywords = {Component state transition diagrams,Information extraction,Recurrent neural network,Word embeddings,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {386--392},
title = {{Automated identification of component state transition model elements from requirements}},
year = {2017}
}
@inproceedings{6597262,
abstract = {Despite being the most suitable language to communicate requirements, the intrinsic ambiguity of natural language often undermines requirements quality criteria, specially clearness and consistency. Several proposals have been made to increase the rigor of requirements representations through conceptual models, which encompass different perspectives to completely describe the system. However, this multi-representation strategy warrants significant human effort to produce and reuse such models, as well as to enforce their consistency. This paper presents RSL-IL, a Requirements Specification Language that tackles the requirements formalization problem by providing a minimal set of constructs. To cope with the most typical Requirements Engineering concerns, RSL-IL constructs are internally organized into viewpoints. Since these constructs are tightly integrated, RSL-IL enables the representation of requirements in a way that makes them formal enough for being tractable by a computer. Given that RSL-IL provides a stable intermediate representation that can improve the quality and enables requirements reuse, it can be regarded as a requirements interlingua. Also, RSL-IL can be used as a source language within the context of model-to-model transformations to produce specific conceptual models. To illustrate how RSL-IL can be applied in a real project, this paper provides a running example based on a case study. {\textcopyright} 2013 IEEE.},
author = {{De Almeida Ferreira}, David and {Da Silva}, Alberto Rodrigues},
booktitle = {2013 3rd International Workshop on Model-Driven Requirements Engineering, MoDRE 2013 - Proceedings},
doi = {10.1109/MoDRE.2013.6597262},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/{\_}De Almeida Ferreira{\_}, David and {\_}Da Silva{\_}, Alberto Rodrigues{\_} {\_}{\_}RSL-IL{\_} An interlingua for formally documenting requirements{\_}{\_} (2013).pdf:pdf},
isbn = {9781479909469},
keywords = {Information Extraction,Requirements Modeling,Requirements Specification Language,Requirements Transformations,Requirements Verification,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {40--49},
title = {{RSL-IL: An interlingua for formally documenting requirements}},
year = {2013}
}
@inproceedings{10.1145/3011141.3011183,
abstract = {In the era of globalization, it is strongly recommended for a company when developing a new project to identify companies (at local and international levels) that have already developed similar projects to reuse their experience and reproduce their findings. It should be noticed that the reuse is an eternal grail to increase projects productivity. It may concern all aspects of the life cycle of project development that ranges from user requirements to codes. In this study, we focus on user requirements. In such a context, each company (site) has already expressed its own requirements using different modeling formalisms, conceptualizations and vocabularies and scheduling of tasks. This situation makes the reuse and exploitation of the requirements harder, since the heterogeneity is everywhere. To reduce this heterogeneity, the most important studies assume the existence of pivot models and shared ontologies that allow reducing heterogeneities. Unfortunately, these hypotheses are out of phase with the reality, in which the sites are autonomous and usually multilingual. In this paper, we propose to relax these hypotheses. Firstly, we propose a framework for requirements integration using an ontology-based matching system that identifies and unifies different heterogeneous items. Secondly, a reasoning mechanism is proposed to deduce relationships between integrated requirements. Finally, our proposed framework is evaluated w.r.t its feasibility, results quality and effiectiveness. A tool, called MURGROOM implementing the framework is also proposed.},
address = {New York, NY, USA},
author = {Djilani, Zouhir and Khiat, Abderrahmane and Khouri, Selma and Bellatreche, Ladjel},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3011141.3011183},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Djilani, Zouhir and Khiat, Abderrahmane and Khouri, Selma and Bellatreche, Ladjel{\_} {\_}{\_}MURGROOM{\_} Multi-site requirement reuse through GRaph and ontology matching{\_}{\_} (2016).pdf:pdf},
isbn = {9781450348072},
keywords = {Integration,Matching System,Ontology,Requirements Engineering,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {160--169},
publisher = {Association for Computing Machinery},
series = {iiWAS '16},
title = {{MURGROOM: Multi-site requirement reuse through GRaph and ontology matching}},
url = {https://doi.org/10.1145/3011141.3011183},
year = {2016}
}
@article{Rago201367,
abstract = {Quality-attribute requirements describe constraints on the development and behavior of a software system, and their satisfaction is key for the success of a software project. Detecting and analyzing quality attributes in early development stages provides insights for system design, reduces risks, and ultimately improves the developers' understanding of the system. A common problem, however, is that quality-attribute information tends to be understated in requirements specifications and scattered across several documents. Thus, making the quality attributes first-class citizens becomes usually a time-consuming task for analysts. Recent developments have made it possible to mine concerns semi-automatically from textual documents. Leveraging on these ideas, we present a semi-automated approach to identify latent quality attributes that works in two stages. First, a mining tool extracts early aspects from use cases, and then these aspects are processed to derive candidate quality attributes. This derivation is based on an ontology of quality-attribute scenarios. We have built a prototype tool called QAMiner to implement our approach. The evaluation of this tool in two case studies from the literature has shown interesting results. As main contribution, we argue that our approach can help analysts to skim requirements documents and quickly produce a list of potential quality attributes for the system. {\textcopyright} 2011 Springer-Verlag London Limited.},
annote = {cited By 17},
author = {Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andr{\'{e}}s},
doi = {10.1007/s00766-011-0142-z},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andr{\_}{\_}'{\_}e{\_}{\_}s{\_} {\_}{\_}Uncovering quality-attribute concerns in use case specifications via early aspect mining{\_}{\_} (2013).pdf:pdf},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Early aspect,Quality attribute,Text mining,Tool support,Use case specification,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {1},
pages = {67--84},
title = {{Uncovering quality-attribute concerns in use case specifications via early aspect mining}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874022798{\&}doi=10.1007{\%}2Fs00766-011-0142-z{\&}partnerID=40{\&}md5=22cab7730e42ed76a4b2b97dae234854},
volume = {18},
year = {2013}
}
@inproceedings{10.1145/3383219.3383288,
abstract = {Early characterization of security requirements supports system designers to integrate security aspects into early architectural design. However, distinguishing security related requirements from other functional and non-functional requirements can be tedious and error prone. To address this issue, machine learning techniques have proven to be successful in the identification of security requirements. In this paper, we have conducted an empirical study to evaluate the performance of 22 supervised machine learning classification algorithms and two deep learning approaches, in classifying security requirements, using the publicly availble SecReq dataset. More specifically, we focused on the robustness of these techniques with respect to the overhead of the pre-processing step. Results show that Long short-term memory (LSTM) network achieved the best accuracy (84{\%}) among non-supervised algorithms, while Boosted Ensemble achieved the highest accuracy (80{\%}), among supervised algorithms.},
address = {New York, NY, USA},
author = {Kobilica, Armin and Ayub, Mohammed and Hassine, Jameleddine},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
doi = {10.1145/3383219.3383288},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/kobilica Automated Identification of Security Requirements A Machine Learning Approach.pdf:pdf},
isbn = {9781450377317},
keywords = {Fast Pre-processing Techniques,Machine Learning,Security Requirements,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {475--480},
publisher = {Association for Computing Machinery},
series = {EASE '20},
title = {{Automated Identification of Security Requirements: A Machine Learning Approach}},
url = {https://doi.org/10.1145/3383219.3383288},
year = {2020}
}
@conference{Jindal20162027,
abstract = {Requirement engineers are not able to elicit and analyze the security requirements clearly, that are essential for the development of secure and reliable software. Proper identification of security requirements present in the Software Requirement Specification (SRS) document has been a problem being faced by the developers. As a result, they are not able to deliver the software free from threats and vulnerabilities. Thus, in this paper, we intend to mine the descriptions of security requirements present in the SRS document and thereafter develop the classification models. The security-based descriptions are analyzed using text mining techniques and are then classified into four types of security requirements viz. authentication-authorization, access control, cryptography-encryption and data integrity using J48 decision tree method. Corresponding to each type of security requirement, a prediction model has been developed. The effectiveness of the prediction models is evaluated against requirement specifications collected from 15 projects which have been developed by MS students at DePaul University. The result analysis indicated that all the four models have performed very well in predicting their respective type of security requirements.},
annote = {cited By 14},
author = {Jindal, Rajni and Malhotra, Ruchika and Jain, Abha},
booktitle = {2016 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016},
doi = {10.1109/ICACCI.2016.7732349},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Jindal, Rajni and Malhotra, Ruchika and Jain, Abha{\_} {\_}{\_}Automated classification of security requirements{\_}{\_} (2016).pdf:pdf},
isbn = {9781509020287},
keywords = {Machine learning,Non-functional requirements,Receiver Operating Characteristics,Requirement elicitation,Requirement engineering,Security requirements,Text mining,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {2027--2033},
title = {{Automated classification of security requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007400793{\&}doi=10.1109{\%}2FICACCI.2016.7732349{\&}partnerID=40{\&}md5=8d3c3ef8a9259f52267b7d3637f9879f},
year = {2016}
}
@article{Ellis-Braithwaite2017167,
abstract = {Stakeholder requirements (also known as user requirements) are defined at an early stage of a software project to describe the problem(s) to be solved. At a later stage, abstract solutions to those problems are prescribed in system requirements. The quality of these requirements has long been linked to the quality of the software system and its development or procurement process. However, little is known about the quality defect of redundancy between these two sets of requirements. Previous literature is anecdotal rather than exploratory, and so this paper empirically investigates its occurrence and consequences with a case study from a UK defense contractor. We report on a survey of sixteen consultants to understand their perception of the problem, and on an analysis of real-world software requirements documents using natural language processing techniques. We found that three quarters of the consultants had seen repetition in at least half of their projects. Additionally, we found that on average, a third of the requirement pairs' (comprised of a system and its related stakeholder requirement) description fields were repeated such that one requirement in the pair added only trivial information. That is, solutions were described twice while their respective problems were not described, which ultimately lead to suboptimal decisions later in the development process, as well as reduced motivation to read the requirements set. Furthermore, the requirement fields considered to be secondary to the primary “description” field, such as the “rationale” or “fit criterion” fields, had considerably more repetition within UR–SysR pairs. Finally, given that the UR–SysR repetition phenomena received most of its discussion in the literature over a decade ago, it is interesting that the survey participants did not consider its occurrence to have declined since then. We provide recommendations on preventing the defect, and describe the freely available tool developed to automatically detect its occurrence and alleviate its consequences.},
annote = {cited By 5},
author = {Ellis-Braithwaite, Richard and Lock, Russell and Dawson, Ray and King, Tim},
doi = {10.1007/s00766-015-0239-x},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/S768UU5L/Ellis-Braithwaite e.a. - 2017 - Repetition between stakeholder (user) and system r.pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Duplicate detection,Redundancy,Stakeholder requirements,System requirements,User requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {2},
pages = {167--190},
title = {{Repetition between stakeholder (user) and system requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944628882{\&}doi=10.1007{\%}2Fs00766-015-0239-x{\&}partnerID=40{\&}md5=e5ca9f28e2726ec7ac3b7707267747bf},
volume = {22},
year = {2017}
}
@inproceedings{8054848,
abstract = {The ever increasing accessibility of the web for the crowd offered by various electronic devices such as smartphones has facilitated the communication of the needs, ideas, and wishes of millions of stakeholders. To cater for the scale of this input and reduce the overhead of manual elicitation methods, data mining and text mining techniques have been utilised to automatically capture and categorise this stream of feedback, which is also used, amongst other things, by stakeholders to communicate their requirements to software developers. Such techniques, however, fall short of identifying some of the peculiarities and idiosyncrasies of the natural language that people use colloquially. This paper proposes CRAFT, a technique that utilises the power of the crowd to support richer, more powerful text mining by enabling the crowd to categorise and annotate feedback through a context menu. This, in turn, helps requirements engineers to better identify user requirements within such feedback. This paper presents the theoretical foundations as well as the initial evaluation of this crowd-based feedback annotation technique for requirements identification.},
author = {Hosseini, Mahmood and Groen, Eduard C. and Shahri, Alimohammad and Ali, Raian},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.27},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Hosseini, Mahmood and Groen, Eduard C. and Shahri, Alimohammad and Ali, Raian{\_} {\_}{\_}CRAFT{\_} A crowd-annotated feedback technique{\_}{\_} (2017).pdf:pdf},
isbn = {9781538634882},
keywords = {Crowdsourced text mining,Crowdsourcing,Feedback categorisation,Requirements elicitation,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {170--175},
title = {{CRAFT: A crowd-annotated feedback technique}},
year = {2017}
}
@inproceedings{8920512,
abstract = {System specifications are generally organized according to several documents hierarchies levels linked in order to represent the traceability information. Requirements engineering experts verify manually the links between each specification which allows to generate a traceability matrix. The purpose of this paper is to automatize the generation of the traceability matrix since it is a time consuming and costly task. We propose an artificial intelligence based approach to deal with this problem through a clustering approach. This latter is an unsupervised algorithm that doesn't need any prior knowledge on the language neither the domain of the specifications. Our approach generates duplicates and clusters containing linked requirements. We experiment our approach in an aeronautic domain and a space domain. We obtain better results for high level specifications especially with a pre-processing.},
author = {Mezghani, Manel and Kang, Juyeon and Kang, Eun Bee and S{\`{e}}des, Florence},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00035},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Mezghani, Manel and Kang, Juyeon and Kang, Eun Bee and S{\_}{\_}`{\_}e{\_}{\_}des, Florence{\_} {\_}{\_}Clustering for traceability managing in system specifications{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Clustering,Documents hierarchies,Requirements engineering,System specifications documents,Traceability,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {257--264},
title = {{Clustering for traceability managing in system specifications}},
volume = {2019-Septe},
year = {2019}
}
@article{Ferrari20183684,
abstract = {In the railway safety-critical domain requirements documents have to abide to strict quality criteria. Rule-based natural language processing (NLP) techniques have been developed to automatically identify quality defects in natural language requirements. However, the literature is lacking empirical studies on the application of these techniques in industrial settings. Our goal is to investigate to which extent NLP can be practically applied to detect defects in the requirements documents of a railway signalling manufacturer. To address this goal, we first identified a set of typical defects classes, and, for each class, an engineer of the company implemented a set of defect-detection patterns by means of the GATE tool for text processing. After a preliminary analysis, we applied the patterns to a large set of 1866 requirements previously annotated for defects. The output of the patterns was further inspected by two domain experts to check the false positive cases. Additional discard-patterns were defined to automatically remove these cases. Finally, SREE, a tool that searches for typically ambiguous terms, was applied to the requirements. The experiments show that SREE and our patterns may play complementary roles in the detection of requirements defects. This is one of the first works in which defect detection NLP techniques are applied on a very large set of industrial requirements annotated by domain experts. We contribute with a comparison between traditional manual techniques used in industry for requirements analysis, and analysis performed with NLP. Our experience shows that several discrepancies can be observed between the two approaches. The analysis of the discrepancies offers hints to improve the capabilities of NLP techniques with company specific solutions, and suggests that also company practices need to be modified to effectively exploit NLP tools.},
annote = {cited By 11},
author = {Ferrari, Alessio and Gori, Gloria and Rosadini, Benedetta and Trotta, Iacopo and Bacherini, Stefano and Fantechi, Alessandro and Gnesi, Stefania},
doi = {10.1007/s10664-018-9596-7},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/FE76D7{\~{}}1.PDF:PDF},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Ambiguity,Defect detection,Industrial case study,Natural language processing,Natural language requirements,Precision,Railway,Recall,Requirements analysis,Requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {6},
pages = {3684--3733},
title = {{Detecting requirements defects with NLP patterns: an industrial experience in the railway domain}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042219906{\&}doi=10.1007{\%}2Fs10664-018-9596-7{\&}partnerID=40{\&}md5=4574d1c706f6b010f4db1a154e895177},
volume = {23},
year = {2018}
}
@inproceedings{10.1109/ASE.2013.6693132,
abstract = {Quality of requirements written in natural language has always been a critical concern in software engineering. Poorly written requirements lead to ambiguity and false interpretation in different phases of a software delivery project. Further, incomplete requirements lead to partial implementation of the desired system behavior. In this paper, we present a model for harvesting domain (functional or business) knowledge. Subsequently we present natural language processing and ontology based techniques for leveraging the model to analyze requirements quality and for requirements comprehension. The prototype also provides an advisory to business analysts so that the requirements can be aligned to the expected domain standard. The prototype developed is currently being used in practice, and the initial results are very encouraging. {\textcopyright} 2013 IEEE.},
author = {Annervaz, K. M. and Kaulgud, Vikrant and Sengupta, Shubhashis and Savagaonkar, Milind},
booktitle = {2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
doi = {10.1109/ASE.2013.6693132},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/annervas Natural language requirements quality analysis based on business domain models.pdf:pdf},
isbn = {9781479902156},
keywords = {Business Domain Modeling,Natural Language Processing,Ontology,Requirements Engineering,acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {676--681},
publisher = {IEEE Press},
series = {ASE'13},
title = {{Natural language requirements quality analysis based on business domain models}},
url = {https://doi.org/10.1109/ASE.2013.6693132},
year = {2013}
}
@article{Winkler201857,
abstract = {[Context and motivation] In many companies, textual fragments in specification documents are categorized into requirements and non-requirements. This categorization is important for determining liability, deriving test cases, and many more decisions. In practice, this categorization is usually performed manually, which makes it labor-intensive and error-prone. [Question/problem] We have developed a tool to assist users in this task by providing warnings based on classification using neural networks. However, we currently do not know whether using the tool actually helps increasing the classification quality compared to not using the tool. [Principal idea/results] Therefore, we performed a controlled experiment with two groups of students. One group used the tool for a given task, whereas the other did not. By comparing the performance of both groups, we can assess in which scenarios the application of our tool is beneficial. [Contribution] The results show that the application of an automated classification approach may provide benefits, given that the accuracy is high enough.},
annote = {cited By 4},
author = {Winkler, Jonas Paul and Vogelsang, Andreas},
doi = {10.1007/978-3-319-77243-1_4},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/96FCZIGA/Winkler en Vogelsang - 2018 - Using Tools to Assist Identification of Non-requir.pdf:pdf},
isbn = {9783319772424},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural networks,Machine learning,Natural language processing,Requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {57--71},
title = {{Using tools to assist identification of non-requirements in requirements specifications – A controlled experiment}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043376087{\&}doi=10.1007{\%}2F978-3-319-77243-1{\_}4{\&}partnerID=40{\&}md5=f472d516ae0e70ef4ddb89beeac1100d},
volume = {10753 LNCS},
year = {2018}
}
@inproceedings{10.1145/3278177.3278180,
abstract = {Creativity can often be seen in artistic productions when artists use their imagination to create original and novel work. However, the importance of creativity is being broadly recognized in various areas. Recently, the requirements engineering (RE) research community has been paying more attention to the matter of capturing and generating creative requirements as it plays a pivotal role in a software system sustainability. To further advance the literature, in this paper, we propose a automated system to alleviate creative feature generation by using a Hidden Markov Model with requirements data scraped from Google Play. To evaluate the performance of the system, We experiment our system in two settings, generating features from a specific application domain (i.e., Messaging platform) and from a mixed domains of successful applications including Google Chrome and Dropbox. The results offer encouraging insights on how our system can support capturing creative requirements and aid the development of more advanced software feature generators.},
address = {New York, NY, USA},
author = {Do, Quoc Anh and Bhowmik, Tanmay},
booktitle = {WASPI 2018 - Proceedings of the 1st ACM SIGSOFT International Workshop on Automated Specification Inference, Co-located with FSE 2018},
doi = {10.1145/3278177.3278180},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Do, Quoc Anh and Bhowmik, Tanmay{\_} {\_}{\_}Automated generation of creative software requirements{\_} A data-driven approach{\_}{\_} (2018).pdf:pdf},
isbn = {9781450360579},
keywords = {Hidden markov model,Natural language processing,Probabilistic model,Requirements engineering,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {9--12},
publisher = {Association for Computing Machinery},
series = {WASPI 2018},
title = {{Automated generation of creative software requirements: A data-driven approach}},
url = {https://doi.org/10.1145/3278177.3278180},
year = {2018}
}
@article{Omoronyia2010188,
abstract = {[Context and motivation] In Requirements Management, ontologies are used to reconcile gaps in the knowledge and common understanding among stakeholders during requirement elicitation, and therefore significantly improve the quality of the elicited requirements.[Question/problem] However, a precondition of state-of-the-art ontology approaches for requirements elicitation is an existing domain ontology. While this is not a trivial precondition, there are only a few reports on approaches to systematically and efficiently build domain ontologies, and these approaches are often highly biased towards their intended use. [Principal ideas/results] In this paper, we investigate an approach for building domain ontologies suitable for guiding requirements elicitation. We evaluate the feasibility of the approach based on a real-world industrial use case by analyzing natural language text from technical standards. [Contribution] A major outcome is that the proposed approach can help reduce the effort of building domain ontologies from the scratch. {\textcopyright} 2010 Springer-Verlag.},
annote = {cited By 28},
author = {Omoronyia, Inah and Sindre, Guttorm and St{\aa}lhane, Tor and Biffl, Stefan and Moser, Thomas and Sunindyo, Wikan},
doi = {10.1007/978-3-642-14192-8_18},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/OMORON{\~{}}1.PDF:PDF},
isbn = {3642141919},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Requirements elicitation,domain engineering,domain ontology,natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,semantic analysis},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {188--202},
title = {{A domain ontology building process for guiding requirements elicitation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955443532{\&}doi=10.1007{\%}2F978-3-642-14192-8{\_}18{\&}partnerID=40{\&}md5=d24081505d9b7fe09bb92d274f839388},
volume = {6182 LNCS},
year = {2010}
}
@conference{Fucci2018,
abstract = {The Mobile Applied Software Technology (MAST) group at the University of Hamburg focuses its research on context-aware adaptive systems and the social side of software engineering. In the context of natural language processing for requirements engineering, the group has mostly focused on mining app stores reviews. Currently, the group is involved in the OpenReq project where natural language processing is being used to recommend requirements from diverse sources (e.g., social media, issue trackers), and to improve the structural quality of existing requirements. Copyright c 2018 by the paper's authors.},
annote = {cited By 0},
author = {Fucci, D. and Stanik, C. and Montgomery, L. and Kurtanovi{\'{c}}, Z. and Johann, T. and Maalej, W.},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/fucci Research on NLP for RE at the University of Hamburg A report.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Research on NLP for RE at the University of Hamburg: A report}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045475157{\&}partnerID=40{\&}md5=6fb1859c2c46681da04fd92f4eb70919},
volume = {2376},
year = {2019}
}
@inproceedings{6138247,
abstract = {Word Sense Disambiguation is a crucial problem in documents whose purpose is to serve as specifications for automatic systems. The combination of different techniques of Natural Language Processing can help in this task. In this paper, we show how to detect ambiguous terms in Software Requirements Specifications. And we propose a computer-aided method that signals the reader for possibly ambiguous usage of terms. The method uses compound term measure (C-value), WordNet semantic similarity (WordNet wup-similarity) and a proposed semantic similarity measure between sentences. {\textcopyright} 2011 IEEE.},
author = {Matsuoka, Jin and Lepage, Yves},
booktitle = {NLP-KE 2011 - Proceedings of the 7th International Conference on Natural Language Processing and Knowledge Engineering},
doi = {10.1109/NLPKE.2011.6138247},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/matsuoka Ambiguity spotting using WordnNet semantic similarity in support to recommended practice for software requirements specifications.pdf:pdf},
isbn = {9781612847283},
keywords = {Ambiguity spotting,C-value,Clustering,SRS,Semantic similarity,WordNet,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {nov},
pages = {479--484},
title = {{Ambiguity spotting using WordnNet semantic similarity in support to recommended practice for software requirements specifications}},
year = {2011}
}
@conference{Morales-Ramirez20157,
abstract = {Large, distributed software development projects, like Open Source Software (OSS), adopt different collaborative working tools, including online forums and mailing list discussions that are valuable source of knowledge for requirements engineering tasks in software evolution, such as model revision and evolution. In our research, we aim at providing tool support for retrieving information from these online resources, and for analyzing it. The solution we propose combines natural language processing techniques, machine learning, statistical and search based techniques to address two key problems, namely the so called expert finding problem, and the problem of identifying requests for changing requirements or soliciting new requirements, by exploiting online discussions. In this paper, we describe the solution approach set up so far with the help of an OSS scenario, discuss some preliminary evaluation, and highlight future work.},
annote = {cited By 1},
author = {Morales-Ramirez, Itzel and Vergne, Matthieu and Morandini, Mirko and Perini, Anna and Susi, Angelo},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/morales-ramirez Exploiting online discussions in collaborative distributed requirements engineering.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {7--12},
title = {{Exploiting online discussions in collaborative distributed requirements engineering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943421670{\&}partnerID=40{\&}md5=1c4a0eea691f2d2a525df263cca7e862},
volume = {1402},
year = {2015}
}
@article{assawamekin_ontology-based_2010,
abstract = {Large-scaled software development inevitably involves a group of stakeholders, each of whom may express their requirements differently in their own terminology and representation depending on their perspectives or perceptions of their shared problems. In view of that, the heterogeneity must be well handled and resolved in tracing and managing changes of such requirements. This paper presents our multiperspective requirements traceability (MUPRET) framework which deploys ontology as a knowledge management mechanism to intervene mutual "understanding" without restricting the freedom in expressing requirements differently. Ontology matching is applied as a reasoning mechanism in automatically generating traceability relationships. The relationships are identified by deriving semantic analogy of ontology concepts representing requirements elements. The precision and recall of traceability relationships generated by the framework are verified by comparing with a set of traceability relationships manually identified by users as a proof-of-concept of this framework. {\textcopyright} 2009 Springer-Verlag London Limited.},
author = {Assawamekin, Namfon and Sunetnanta, Thanwadee and Pluempitiwiriyawej, Charnyote},
doi = {10.1007/s10115-009-0259-2},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Assawamekin, Namfon and Sunetnanta, Thanwadee and Pluempitiwiriyawej, Charnyote{\_} {\_}{\_}Ontology-based multiperspective requirements traceability framework{\_}{\_} (2010).pdf:pdf},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Knowledge management,Multiperspective software development,Ontology,Requirements traceability,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
number = {3},
pages = {493--522},
title = {{Ontology-based multiperspective requirements traceability framework}},
url = {http://link.springer.com/10.1007/s10115-009-0259-2},
volume = {25},
year = {2010}
}
@inproceedings{6805412,
abstract = {We consider the problem of terminological ambiguity in requirement specifications arising from term-aliasing, wherein multiple terms may be referring to the same entity in a corpus of natural language text requirements. We consider the case of syntactic as well as semantic aliasing. Syntactic alias identification involves automated generation of patterns for identifying syntactic variances of terms including abbreviations and introduced-aliases. Semantic alias identification includes extracting multi-dimensional features (linguistic, statistical, and locational) from given requirement text to estimate semantic relatedness between terms. Based upon the estimated relatedness and standard language database based refinement, clusters of potential semantic aliases are generated. Results of these analyses with user refinement should lead to generation of entity-term alias glossary and unification of term usage across requirements. We present experimental results assessing the effectiveness of the presented approach using a prototype tool for an automated analysis of term-aliasing in the requirements given as plain English language text.},
author = {Misra, Janardan and Das, Subhabrata},
booktitle = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
doi = {10.1109/APSEC.2013.41},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Misra, Janardan and Das, Subhabrata{\_} {\_}{\_}Entity disambiguation in natural language text requirements{\_}{\_} (2013).pdf:pdf},
isbn = {9781479921430},
issn = {15301362},
keywords = {Alias identification,Entity disambiguation,Latent semantic analysis,Requirements analysis,Terminological inconsistency analysis,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {239--246},
title = {{Entity disambiguation in natural language text requirements}},
volume = {1},
year = {2013}
}
@inproceedings{5719000,
abstract = {Requirements assurance aims to increase confidence in the quality of requirements through independent audit and review. One important and effort intensive activity is assurance of the traceability matrix (TM). In this, determining the correctness and completeness of the many-to-many relationships between functional and non-functional requirements (NFRs) is a particularly tedious and error prone activity for assurance personnel to peform manually. We introduce a practical to use method that applies well-established text-mining and statistical methods to reduce this effort and increase TM assurance. The method is novel in that it utilizes both requirements similarity (likelihood that requirements trace to each other) and dissimilarity (or anti-trace, likelihood that requirements do not trace to each other) to generate investigation sets that significantly reduce the complexity of the traceability assurance task and help personnel focus on likely problem areas. The method automatically adjusts to the quality of the requirements specification and TM. Requirements assurance experiences from the SQA group at NASA's Jet Propulsion Laboratory provide motivation for the need and practicality of the method. Results of using the method are verifiably promising based on an extensive evaluation of the NFR data set from the publicly accessible PROMISE repository. {\textcopyright} 2011 IEEE.},
author = {Port, Dan and Nikora, Allen and Hayes, Jane Huffman and Huang, Li Guo},
booktitle = {Proceedings of the Annual Hawaii International Conference on System Sciences},
doi = {10.1109/HICSS.2011.399},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Port, Dan and Nikora, Allen and Hayes, Jane Huffman and Huang, Li Guo{\_} {\_}{\_}Text mining support for software requirements{\_} Traceability assurance{\_}{\_} (2011).pdf:pdf},
isbn = {9780769542829},
issn = {15301605},
keywords = {data mining,formal specification,formal verificati,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {jan},
pages = {1--11},
title = {{Text mining support for software requirements: Traceability assurance}},
year = {2011}
}
@conference{Kulkarni2012326,
abstract = {Significant time is spent by practitioners to analyze use-cases written in Natural Language (NL). With only prescriptive templates to describe complex scenarios, common errors like misinterpretation and oversight can have costly consequence later during system development. A semi-automatic approach based on NL processing can reduce the time spent on requirement analysis and bootstrap design activity. However, linguistic community has adopted pipeline processing to handle NL ambiguities where several sequential tasks aid in solving a bigger task. Choosing NL processing techniques depends on the domain and task to accomplish. As use-cases are domain specific it is crucial to identify suitable pipelines to process them. This is highlighted in our evaluation of two pipelines consisting of syntactic and semantic techniques on use-cases found in theory and practice. We believe, the promising results has opened up the need for exploring more task specific NLP pipelines and evaluation thereof. {\textcopyright} 2012 IEEE.},
annote = {cited By 2},
author = {Kulkarni, Naveen and Parachuri, Deepti and Dasa, Madhuri and Kumar, Abhishek},
booktitle = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
doi = {10.1109/APSEC.2012.83},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kulkarni, Naveen and Parachuri, Deepti and Dasa, Madhuri and Kumar, Abhishek{\_} {\_}{\_}Automated analysis of textual use-cases{\_} Does NLP components and pipelines matter{\_}{\_}{\_} (2012).pdf:pdf},
isbn = {9780769549224},
issn = {15301362},
keywords = {Natural Language Processing,Requirement Engineering,Use Cases,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {326--329},
title = {{Automated analysis of textual use-cases: Does NLP components and pipelines matter?}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874598904{\&}doi=10.1109{\%}2FAPSEC.2012.83{\&}partnerID=40{\&}md5=ee5b805b987467a013318947a7b7a2cf},
volume = {1},
year = {2012}
}
@article{Terawaki2013383,
abstract = {Quality requirements (QR) are a description which indicates how well the software's behavior is to be executed. It is widely recognized that quality requirements are vital for the success of software systems. Therefore, to define the quality requirements and to check the quality attributes carefully is necessary for bringing good- quality software and ensuring quality of the service. This paper proposes a framework that measures the quality attributes in the requirements document such as SRS. The effectiveness of this framework was briefly described, we discuss approach was to enrich the representative quality corpora. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
annote = {cited By 1},
author = {Terawaki, Yuki},
doi = {10.1007/978-3-642-39209-2-44},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Terawaki2013{\_}Chapter{\_}FrameworkForQuantitativelyEval.pdf:pdf},
isbn = {9783642392085},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Non-Functional Requirements,Quality Requirements,Requirements Engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,text-mining},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {PART 1},
pages = {383--392},
title = {{Framework for quantitatively evaluating the quality requirements of software system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880872467{\&}doi=10.1007{\%}2F978-3-642-39209-2-44{\&}partnerID=40{\&}md5=0c44ec26f7809f51294b8fce54a3a04e},
volume = {8016 LNCS},
year = {2013}
}
@incollection{hutchison_semantic_2012,
abstract = {Software Requirements Specifications (SRS) documents are important artifacts in the software industry. A SRS contains all the requirements specifications for a software system, either as functional requirements (FR) or non-functional requirements (NFR). FRs are the features of the system-to-be, whereas NFRs define its quality attributes. NFRs impact the system as a whole and interact both with each other and with the functional requirements. SRS documents are typically written in informal natural language [1], which impedes their automated analysis. The goal of this work is to support software engineers with semantic analysis methods that can automatically extract and analyze requirements written in natural language texts, in order to (i) make SRS documents machine-processable by transforming them into an ontological representation; (ii) apply quality assurance (QA) methods on the extracted requirements, in order to detect defects, like ambiguities or omissions; and (iii) attempt to build traceability links between NFRs and the FRs impacted by them, in order to aid effort estimation models. {\textcopyright} 2012 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Rashwan, Abderahman},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-30353-1_42},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard and Kosseim, Leila and Inkpen, Diana},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rashwan, Abderahman{\_} {\_}{\_}Semantic analysis of functional and non-functional requirements in software requirements specifications{\_}{\_} (2012).pdf:pdf},
isbn = {9783642303524},
issn = {03029743},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {388--391},
publisher = {Springer Berlin Heidelberg},
title = {{Semantic analysis of functional and non-functional requirements in software requirements specifications}},
url = {http://link.springer.com/10.1007/978-3-642-30353-1{\_}42},
volume = {7310 LNAI},
year = {2012}
}
@inproceedings{6912267,
abstract = {Software requirements specifications often focus on functionality and fail to adequately capture quality concerns such as security, performance, and usability. In many projects, quality-related requirements are either entirely lacking from the specification or intermingled with functional concerns. This makes it difficult for stakeholders to fully understand the quality concerns of the system and to evaluate their scope of impact. In this paper we present a data mining approach for automating the extraction and subsequent modeling of quality concerns from requirements, feature requests, and online forums. We extend our prior work in mining quality concerns from textual documents and apply a sequence of machine learning steps to detect quality-related requirements, generate goal graphs contextualized by project-level information, and ultimately to visualize the results. We illustrate and evaluate our approach against two industrial health-care related systems.},
author = {Rahimi, Mona and Mirakhorli, Mehdi and Cleland-Huang, Jane},
booktitle = {2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings},
doi = {10.1109/RE.2014.6912267},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/rahimi mona Automated extraction and visualization of quality concerns from requirements specifications.pdf:pdf},
isbn = {9781479930333},
issn = {2332-6441},
keywords = {goal Model,ieee{\_}inc{\_}nlp{\_}x{\_}re,quality concerns,requirements,visualization},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {253--262},
title = {{Automated extraction and visualization of quality concerns from requirements specifications}},
year = {2014}
}
@inproceedings{6912266,
abstract = {Requirements engineering (RE), framed as a creative problem solving process, plays a key role in innovating more useful and novel requirements and improving a software system's sustainability. Existing approaches, such as creativity workshops and feature mining from web services, facilitate creativity by exploring a search space of partial and complete possibilities of requirements. To further advance the literature, we support creativity from a combinational perspective, i.e., making unfamiliar connections between familiar possibilities of requirements. In particular, we propose a novel framework that extracts familiar ideas from the requirements and stakeholders' comments using topic modeling and applies part-of-speech tagging to obtain unfamiliar idea combinations. We apply our framework on two large open source software systems and further report a human subject evaluation. The results show that our framework complements existing approaches by generating original and relevant requirements in an automated manner.},
author = {Bhowmik, Tanmay and Niu, Nan and Mahmoud, Anas and Savolainen, Juha},
booktitle = {2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings},
doi = {10.1109/RE.2014.6912266},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bhowmik, Tanmay and Niu, Nan and Mahmoud, Anas and Savolainen, Juha{\_} {\_}{\_}Automated support for combinational creativity in requirements engineering{\_}{\_} (2014).pdf:pdf},
isbn = {9781479930333},
issn = {2332-6441},
keywords = {Requirements engineering,creativity,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirements elicitation,topic modeling},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {243--252},
title = {{Automated support for combinational creativity in requirements engineering}},
year = {2014}
}
@inproceedings{10.1145/3084226.3084246,
abstract = {Requirements engineering is assessed as the most important phase of the software development process. This process is especially challenging for app developers, who tend to gather crowd-based feedback after releasing their apps. This feedback is often voluminous, posing prioritization challenges for developers identifying features to fix or add. While previous work has identified frequently mentioned features, and some effort has been dedicated towards providing various prioritization and classification techniques, these do not quite address the prioritization challenge faced by app developers given voluminous app reviews. In fact, there is also need to assess the scale of app reviews' usefulness. We use content analysis and regression to contribute towards this cause by exploring the usefulness of app reviews, and the attributes that predict which app features to fix, respectively. Our outcomes show that reviews tended to either provide information of little value (i.e., no actionable information) or highlighted problems that may directly affect the functionality of app features. For two different apps, we also observe that features that were mentioned the most (the feature frequency attribute) in lower ranked reviews provided by users had the strongest predictive power for identifying severely broken features (as perceived by a developer). However, the ordering did not match with the frequency with which reports were made by users. There were also variances in the attributes that predict which features to fix, for the reviews of different apps. Review mining and prioritization challenges remain given variances in app reviews' content and structure. These findings also point to the need to redesign app review interfaces to consider how reviews are captured.},
address = {New York, NY, USA},
author = {Licorish, Sherlock A. and Savarimuthu, Bastin Tony Roy and Keertipati, Swetha},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3084226.3084246},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/licorish On Satisfying the Android OS Community User Feedback Still Central to Developers' Portfolios.pdf:pdf},
isbn = {9781450348041},
keywords = {Feature prioritization,Requirements engineering,Text mining,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {108--117},
publisher = {Association for Computing Machinery},
series = {EASE'17},
title = {{Attributes that predict which features to fix: Lessons for app store mining}},
url = {https://doi.org/10.1145/3084226.3084246},
volume = {Part F1286},
year = {2017}
}
@inproceedings{8613726,
abstract = {Ambiguity is a critical problem that rears its ugly head in many disciplines including writing, philosophy, law, and of course software engineering, especially requirements engineering. Requirements are typically expressed in a natural language. However, expressions in natural languages are likely to suffer from ambiguities, where a statement can be reasonably interpreted in more than one way and if we know, even then it is difficult to decide which one is correct. This paper presents an approach for ambiguity detection and resolution in natural language requirements as early as possible using natural language processing and semantic web techniques. Hence, we will be able to identify the ambiguous words and provide them all the possible interpretation and clarifying the meaning of the requirements.},
author = {Zait, Fatima and Zarour, Nacereddine},
booktitle = {5th International Symposium on Innovation in Information and Communication Technology, ISIICT 2018},
doi = {10.1109/ISIICT.2018.8613726},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Zait, Fatima and Zarour, Nacereddine{\_} {\_}{\_}Addressing Lexical and Semantic Ambiguity in Natural Language Requirements{\_}{\_} (2019).pdf:pdf},
isbn = {9781538662243},
keywords = {ambiguity,ieee{\_}inc{\_}nlp{\_}x{\_}re,lexical ambiguity,meaning,natural language,natural language processing,requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,semantic ambiguity,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--7},
title = {{Addressing Lexical and Semantic Ambiguity in Natural Language Requirements}},
year = {2019}
}
@article{10.1145/2699697,
abstract = {The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model comprising class, sequence and activity diagrams from a use case model and to automatically establish traceability links between model elements of the use case model and the generated analysis model. Note that our goal is to save effort through automated support, not to replace human abstraction and decision making. Seven (six) case studies were performed to compare class (sequence) diagrams generated by aToucan to the ones created by experts, Masters students, and trained, fourth-year undergraduate students. Results show that aToucan performs well regarding consistency (e.g., 88{\%} class diagram consistency) and completeness (e.g., 80{\%} class completeness) when comparing generated class diagrams with reference class diagrams created by experts and Masters students. Similarly, sequence diagrams automatically generated by aToucan are highly consistent with the ones devised by experts and are also rather complete, for instance, 91{\%} and 97{\%} message consistency and completeness, respectively. Further, statistical tests show that aToucan significantly outperforms fourth-year engineering students in this respect, thus demonstrating the value of automation. We also conducted two industrial case studies demonstrating the applicability of aToucan in two different industrial domains. Results showed that the vast majority of model elements generated by aToucan are correct and that therefore, in practice, such models would be good initial models to refine and augment so as to converge towards to correct and complete analysis models. A performance analysis shows that the execution time of aToucan (when generating class and sequence diagrams) is dependent on the number of simple sentences contained in the use case model and remains within a range of a few minutes. Five different software system descriptions (18 use cases altogether) were performed to evaluate the generation of activity diagrams. Results show that aToucan can generate 100{\%} complete and correct control flow information of activity diagrams and on average 85{\%} data flAow information completeness. Moreover, we show that aToucan outperforms three commercial tools in terms of activity diagram generation.},
address = {New York, NY, USA},
author = {Yue, Tao and Briand, Lionel C. and Labiche, Yvan},
doi = {10.1145/2699697},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Yue, Tao and Briand, Lionel C. and Labiche, Yvan{\_} {\_}{\_}aToucan{\_} An automated framework to derive UML analysis models from use case models{\_}{\_} (2015).pdf:pdf},
issn = {15577392},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {Activity diagram,Analysis model,Automation,Class diagram,Sequence diagram,Traceability,Transformation,UML,Use case modeling,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
publisher = {Association for Computing Machinery},
title = {{aToucan: An automated framework to derive UML analysis models from use case models}},
url = {https://doi.org/10.1145/2699697},
volume = {24},
year = {2015}
}
@inproceedings{6636736,
abstract = {We present a method for the automatic extraction of glossary terms from unconstrained natural language requirements. The glossary terms are identified in two steps - a) compute units (which are candidates for glossary terms) b) disambiguate between the mutually exclusive units to identify terms. We introduce novel linguistic techniques to identify process nouns, abstract nouns and auxiliary verbs. The identification of units also handles co-ordinating conjunctions and adjectival modifiers. This requires solving co-ordination ambiguity and adjectival modifier ambiguity. The identification of terms among the units adapts an in-document statistical metric. We present an evaluation of our method over a real-life set of software requirements' documents and compare our results with that of a base algorithm. The intricate linguistic classification and the tackling of ambiguity result in superior performance of our approach over the base algorithm. {\textcopyright} 2013 IEEE.},
author = {Dwarakanath, Anurag and Ramnani, Roshni R. and Sengupta, Shubhashis},
booktitle = {2013 21st IEEE International Requirements Engineering Conference, RE 2013 - Proceedings},
doi = {10.1109/RE.2013.6636736},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Dwarakanath, Anurag and Ramnani, Roshni R. and Sengupta, Shubhashis{\_} {\_}{\_}Automatic extraction of glossary terms from natural language requirements{\_}{\_} (2013).pdf:pdf},
isbn = {9781467357654},
issn = {2332-6441},
keywords = {Adjectival Ambiguity,Co-ordination Ambiguity,Glossary Term Extraction,Natural Language Processing,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {314--319},
title = {{Automatic extraction of glossary terms from natural language requirements}},
year = {2013}
}
@conference{Masuda201612,
abstract = {Software testing has been one of the important area for software engineering to contribute high quality software. Decision table testing is a general technique to develop test cases from information about conditions and actions from software requirements. Extracting conditions and actions from requirements is the key for efficient decision table testing. We propose, experiment upon, and evaluate the syntactic rules of conditions and actions for automatic software test cases generation. Our approach uses natural language processing to select sentences from the requirements on the basis of syntactic similarity, and then to determines conditions and actions through dependency and case analysis. Experiments revealed that F-measure reached from 0.70 to 0.77 for different style of descriptions. The results on case studies further demonstrate the effectiveness of our technique.},
annote = {cited By 1},
author = {Masuda, Satoshi and Matsuodani, Tohru and Tsuda, Kazuhiko},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3012258.3012262},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Masuda, Satoshi and Matsuodani, Tohru and Tsuda, Kazuhiko{\_} {\_}{\_}Syntactic rules of extracting test cases from software requirements{\_}{\_} (2016).pdf:pdf},
isbn = {9781450347617},
keywords = {Requirement analysis,Software testing,Syntactic rules,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {12--17},
title = {{Syntactic rules of extracting test cases from software requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014789228{\&}doi=10.1145{\%}2F3012258.3012262{\&}partnerID=40{\&}md5=21a1516f8a2191b477a0a64614d6a10b},
year = {2016}
}
@inproceedings{10.5555/2337223.2337463,
abstract = {Textual specification documents do not represent a suitable starting point for software development. This issue is due to the inherent problems of natural language such as ambiguity, impreciseness and incompleteness. In order to overcome these shortcomings, experts derive analysis models such as requirements models. However, these models are difficult and costly to create manually. Furthermore, the level of abstraction of the models is too low, thus hindering the automated transformation process. We propose a novel approach which uses high abstraction requirements models in the form of Object System Models (OSMs) as targets for the transformation of natural language specifications in conjunction with appropriate text mining and machine learning techniques. OSMs allow the interpretation of the textual specification based on a small set of facts and provide structural and behavioral information. This approach will allow both (1) the enhancement of minimal specifications, and in the case of comprehensive specifications (2) the determination of the most suitable structure of reusable requirements. {\textcopyright} 2012 IEEE.},
author = {Chioaşcǎ, Erol Valeriu},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2012.6227055},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Chioaşcǎ, Erol Valeriu{\_} {\_}{\_}Using machine learning to enhance automated requirements model transformation{\_}{\_} (2012).pdf:pdf},
isbn = {9781467310673},
issn = {02705257},
keywords = {Natural language specification,Object System Models,acm{\_}inc{\_}nlp{\_}x{\_}re,machine learning,text mining},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1487--1490},
publisher = {IEEE Press},
series = {ICSE '12},
title = {{Using machine learning to enhance automated requirements model transformation}},
year = {2012}
}
@incollection{hassanien_forming_2018,
abstract = {Requirements Engineering (RE) is one of the most important phases in the software development process, more than fifty percent of the projects failed due to lack of RE. Therefore, most of the developers in order to achieve high software quality they need to satisfy user's requirement without errors (i.e. specific, clear, precise, {\ldots}etc.). In this regard, this paper presents system requirement formulation from user's stories based on previous similar verified requirements with semantic analysis. After semantic verification, the English written requirements are verified by a Case Based Reasoning Engine to be formulated as a standard requirements form. The generated requirements should support the decisions and resolutions of problems arising from new requirements.},
address = {Cham},
annote = {Series Title: Advances in Intelligent Systems and Computing},
author = {ElKafrawy, Passent M. and Khalaf, Mohamed S.},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-64861-3_71},
editor = {Hassanien, Aboul Ella and Shaalan, Khaled and Gaber, Tarek and Tolba, Mohamed F},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ElKafrawy, Passent M. and Khalaf, Mohamed S.{\_} {\_}{\_}Forming system requirements for software development using semantic technology{\_}{\_} (2018).pdf:pdf},
isbn = {9783319648606},
issn = {21945357},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {755--764},
publisher = {Springer International Publishing},
title = {{Forming system requirements for software development using semantic technology}},
url = {http://link.springer.com/10.1007/978-3-319-64861-3{\_}71},
volume = {639},
year = {2018}
}
@inproceedings{10.5555/2667691.2667698,
abstract = {Requirements acquisition is widely recognized as a hard problem, requiring signicant investments in time and effort. Given the availability of large vol- umes of data and of relatively cheap instrumentation for data acquisition, this paper explores the prospect of data-driven model extraction in the context of i* models. The paper presents techniques for extracting dependencies from message logs, and for extracting task-dependency correlations from process logs.},
address = {AUS},
author = {Ghose, Aditya and Santiputri, Metta and Saraswati, Ayu and Dam, Hoa Khanh},
booktitle = {Conferences in Research and Practice in Information Technology Series},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ghose Data-driven requirements modeling Some initial results with i star.pdf:pdf},
isbn = {9781921770364},
issn = {14451336},
keywords = {Data-driven model extraction,I* model,Requirements acquisition,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {55--64},
publisher = {Australian Computer Society, Inc.},
series = {APCCM '14},
title = {{Data-driven requirements modeling: Some initial results with i*}},
volume = {154},
year = {2014}
}
@conference{Šenkýř2018197,
abstract = {In this paper, we investigate methods of grammatical inspection to identify patterns in textual requirements specification. Unfortunately, a text in natural language includes additionally many inaccuracies caused by ambiguity, inconsistency, and incompleteness. Our contribution is that using our patterns, we are able to extract the information from the text that is necessary to fix some of the problems mentioned above. We present our implemented tool TEMOS that is able to detect some inaccuracies in a text and to generate fragments of the UML class model from textual requirements specification. We use external on-line resources to complete the textual information of requirements.},
annote = {cited By 3},
author = {{\v{S}}enk{\'{y}}ř, David and Kroha, Petr},
booktitle = {ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies},
doi = {10.5220/0006827302310238},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/senkyr.pdf:pdf},
isbn = {9789897583209},
keywords = {Ambiguity,Glossary,Grammatical Inspection,Incompleteness,Inconsistency,Text Mining,Textual Requirements Specifications,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {197--204},
title = {{Patterns in textual requirements specification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071507975{\&}doi=10.5220{\%}2F0006827302310238{\&}partnerID=40{\&}md5=2550d3ed45dfc127baeea3e56603eafa},
year = {2018}
}
@article{Masuda20162210,
abstract = {In the early phases of the system development process, stakeholders exchange ideas and describe requirements in natural language. Requirements described in natural language tend to be vague and include logical inconsistencies, whereas logical consistency is the key to raising the quality and lowering the cost of system development. Hence, it is important to find logical inconsistencies in the whole requirements at this early stage. In verification and validation of the requirements, there are techniques to derive logical formulas from natural language requirements and evaluate their inconsistencies automatically. Users manually chunk the requirements by paragraphs. However, paragraphs do not always represent logical chunks. There can be only one logical chunk over some paragraphs on the other hand some logical chunks in one paragraph. In this paper, we present a practical approach to detecting logical inconsistencies by clustering technique in natural language requirements. Software requirements specifications (SRSs) are the target document type. We use k-means clustering to cluster chunks of requirements and develop semantic role labeling rules to derive "conditions" and "actions" as semantic roles from the requirements by using natural language processing. We also construct an abstraction grammar to transform the conditions and actions into logical formulas. By evaluating the logical formulas with input data patterns, we can find logical inconsistencies. We implemented our approach and conducted experiments on three case studies of requirements written in natural English. The results indicate that our approach can find logical inconsistencies.},
annote = {cited By 3},
author = {Masuda, Satoshi and Matsuodani, Tohru and Tsuda, Kazuhiko},
doi = {10.1587/transinf.2015KBP0005},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Masuda, Satoshi and Matsuodani, Tohru and Tsuda, Kazuhiko{\_} {\_}{\_}Detecting logical inconsistencies by clustering technique in natural language requirements{\_}{\_} (2016).pdf:pdf},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Clustering,Natural language processing,Requirement analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {9},
pages = {2210--2218},
title = {{Detecting logical inconsistencies by clustering technique in natural language requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984863751{\&}doi=10.1587{\%}2Ftransinf.2015KBP0005{\&}partnerID=40{\&}md5=0b5eb6fc9a5f03a8f4277672b9870790},
volume = {E99D},
year = {2016}
}
@inproceedings{8498108,
abstract = {Finding qualities in requirements-related information is important. Quality requirements are central in building reliable software. However, an isolated identification of qualities is not enough; the impact of an individual quality may compete with another quality requirement. This can be perceived in the example: 'the trade-off between usability and security is not completely secure'. This work studies the use of sentiment analysis to help finding relations among qualities. We will focus on usability and will depart from available NFR (Non-Functional Requirements) catalogues for this specific NFR. The catalogues will be the seed for building a corpus, based on queries in GitHub's Issues. We are aiming to find a list of sentiment expressions that will characterize important relations among usability and other qualities, through the analysis of our mining study. We will contextualize our results in connection with recent work on sentiment analysis for RE, focusing on the specific case of NFRs.},
author = {Portugal, Roxana L.Q. and {Do Prado Leite}, Julio Cesar Sampaio},
booktitle = {Proceedings - 2018 1st International Workshop on Affective Computing for Requirements Engineering, AffectRE 2018},
doi = {10.1109/AffectRE.2018.00010},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/portugal2018.pdf:pdf},
isbn = {9781538683613},
keywords = {GitHub issues,Non-functional requirements,Reliable software,Sentiment analysis,Text-mining,Usability,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {20--26},
title = {{Usability related qualities through sentiment analysis}},
year = {2018}
}
@inproceedings{8501305,
abstract = {Context: User feedback on apps is essential for gauging market needs and maintaining a competitive edge in the mobile apps development industry. App Store Reviews have been a primary resource for this feedback, however, recent studies have observed that Twitter is another potentially valuable source for this information. Objective: The objective of this study is to assess user feedback from Twitter in terms of timing as well as content and compare with the App Store reviews. Method: This study employs various text analysis and Natural Language Processing methods such as semantic analysis and Latent Dirichlet Allocation (LDA) to analyze tweets and App Store Reviews. Additionally, supervised learning classifiers are used to classify them as semantically similar tweet and App Store reviews. Results: In spite of a difference in the magnitude between tweets and App Store Review counts, frequency analysis shows that bug report and feature request are discussed mostly on Twitter first as the number of Tweets during the reporting time reached the peak a few days earlier. Likewise, timing analysis on a set of 426 tweets and 2,383 reviews (which are bug reports and feature requests) show that approximately 15{\%} appear on Twitter first. Of these 15{\%} tweets, 72{\%} are related to functional or behavioural aspects of the mobile app. Content analysis shows that user feedback in tweets mostly focuses on critical issues related to the feature failure and improper functionality. Conclusion: The results of this investigation show that the Twitter is not only a strong contender for useful information but also a faster source of information for mobile app improvement.},
author = {Deshpande, Gouri and Rokne, Jon},
booktitle = {Proceedings - 2018 5th International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2018},
doi = {10.1109/AIRE.2018.00008},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/deshpande2018.pdf:pdf},
isbn = {9781538684047},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,machine learning,mobile application improvement.,mobile apps,natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,social media,text analysis,user feedback},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {15--21},
title = {{User Feedback from Tweets vs App Store Reviews: An Exploratory Study of Frequency, Timing and Content}},
year = {2018}
}
@article{6823180,
abstract = {The design and development of process-aware information systems is often supported by specifying requirements as business process models. Although this approach is generally accepted as an effective strategy, it remains a fundamental challenge to adequately validate these models given the diverging skill set of domain experts and system analysts. As domain experts often do not feel confident in judging the correctness and completeness of process models that system analysts create, the validation often has to regress to a discourse using natural language. In order to support such a discourse appropriately, so-called verbalization techniques have been defined for different types of conceptual models. However, there is currently no sophisticated technique available that is capable of generating natural-looking text from process models. In this paper, we address this research gap and propose a technique for generating natural language texts from business process models. A comparison with manually created process descriptions demonstrates that the generated texts are superior in terms of completeness, structure, and linguistic complexity. An evaluation with users further demonstrates that the texts are very understandable and effectively allow the reader to infer the process model semantics. Hence, the generated texts represent a useful input for process model validation. {\textcopyright} 2014 IEEE.},
author = {Leopold, Henrik and Mendling, Jan and Polyvyanyy, Artem},
doi = {10.1109/TSE.2014.2327044},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Leopold, Henrik and Mendling, Jan and Polyvyanyy, Artem{\_} {\_}{\_}Supporting process model validation through natural language generation{\_}{\_} (2014).pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Business process model validation,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language text generation,verbalization},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
number = {8},
pages = {818--840},
title = {{Supporting process model validation through natural language generation}},
volume = {40},
year = {2014}
}
@inproceedings{7549329,
abstract = {Systems requirements are crucial to the proper functioning of a software and must be met for a project to be successful. Hence the need for its effective management. Implicit Requirements (IMRs) however are difficult to manage as a result of their nature-vague, unclear, and ambiguous amongst other characteristics. The process of requirement management is a continuous cycle as change in requirements and emergence of new requirements occur in a system. Hence the need for a tool/approach which identifies and manages requirements (implicit and explicit) effectively. However, most systems do not manage implicit requirements as a lot of attention is focused on explicit requirements. This research presents an approach for identification and management of IMRs using Analogy-based Reasoning in combination with two other core technologies (Ontology and Natural Language Processing). The approach is supported by a prototype tool, which was assessed by conducting a preliminary evaluation. The results indicate that the approach enables for early identification of IMRs when used with a good domain ontology and is potentially suitable for application in practice by experts.},
author = {Emebo, Onyeka and Olawande, Daramola and Charles, Ayo},
booktitle = {Proceedings - International Conference on Research Challenges in Information Science},
doi = {10.1109/RCIS.2016.7549329},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Emebo, Onyeka and Olawande, Daramola and Charles, Ayo{\_} {\_}{\_}An automated tool support for managing implicit requirements using Analogy-based Reasoning{\_}{\_} (2016).pdf:pdf},
isbn = {9781479987092},
issn = {21511357},
keywords = {analogy-based reasoning,ieee{\_}inc{\_}nlp{\_}x{\_}re,implicit requirement,natural language processing,ontology,requirement engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--6},
title = {{An automated tool support for managing implicit requirements using Analogy-based Reasoning}},
volume = {2016-Augus},
year = {2016}
}
@incollection{hutchison_design_2013,
abstract = {[Context and Motivation] Many a tool for finding ambiguities in natural language (NL) requirements specifications (RSs) is based on a parser and a parts-of-speech identifier, which are inherently imperfect on real NL text. Therefore, any such tool inherently has less than 100{\%} recall. Consequently, running such a tool on a NL RS for a highly critical system does not eliminate the need for a complete manual search for ambiguity in the RS. [Question/Problem] Can an ambiguity-finding tool (AFT) be built that has 100{\%} recall on the types of ambiguities that are in the AFT's scope such that a manual search in an RS for ambiguities outside the AFT's scope is significantly easier than a manual search of the RS for all ambiguities? [Principal Ideas/Results] This paper presents the design of a prototype AFT, SREE (Systemized Requirements Engineering Environment), whose goal is achieving a 100{\%} recall rate for the ambiguities in its scope, even at the cost of a precision rate of less than 100{\%}. The ambiguities that SREE searches for by lexical analysis are the ones whose keyword indicators are found in SREE's ambiguity-indicator corpus that was constructed based on studies of several industrial strength RSs. SREE was run on two of these industrial strength RSs, and the time to do a completely manual search of these RSs is compared to the time to reject the false positives in SREE's output plus the time to do a manual search of these RSs for only ambiguities not in SREE's scope. [Contribution] SREE does not achieve its goals. However, the time comparison shows that the approach to divide ambiguity finding between an AFT with 100{\%} recall for some types of ambiguity and a manual search for only the other types of ambiguity is promising enough to justify more work to improve the implementation of the approach. Some specific improvement suggestions are offered. {\textcopyright} 2013 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Tjong, Sri Fatimah and Berry, Daniel M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-37422-7_6},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard and Doerr, Joerg and Opdahl, Andreas L},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/tjong2013.pdf:pdf},
isbn = {9783642374210},
issn = {03029743},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {80--95},
publisher = {Springer Berlin Heidelberg},
title = {{The design of SREE - A prototype potential ambiguity finder for requirements specifications and lessons learned}},
url = {http://link.springer.com/10.1007/978-3-642-37422-7{\_}6},
volume = {7830 LNCS},
year = {2013}
}
@inproceedings{8491125,
abstract = {In this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers. Our results, based on an analysis of 11,537 apps from the Samsung Android and BlackBerry World app stores, indicate that the rating of 89{\%} of these apps can be predicted with 100{\%} accuracy. Our prediction model is built by using feature and rating information from the existing apps offered in the App Store and it yields highly accurate rating predictions, using only a few (11-12) existing apps for case-based prediction. These findings may have important implications for requirements engineering in app stores: They indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers.},
author = {Sarro, Federica and Harman, Mark and Jia, Yue and Zhang, Yuanyuan},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00018},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sarro, Federica and Harman, Mark and Jia, Yue and Zhang, Yuanyuan{\_} {\_}{\_}Customer rating reactions can be predicted purely using app features{\_}{\_} (2018).pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {App features extraction,App store analysis,Case based reasoning,Machine learning,Mobile applications,Natural language processing,Predictive modelling,Rating estimation,Requirements elicitation,Software analytics,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {76--87},
title = {{Customer rating reactions can be predicted purely using app features}},
year = {2018}
}
@conference{Emebo2018108,
abstract = {Implicit requirements (IMRs) in software requirements specifications (SRS) are subtle and need to be identified as users may not provide all information upfront. It is found that successful functioning of a software crucially depends on addressing its IMRs. This work presents a novel system called PROMIRAR with an integrated framework of Natural Language Processing, Ontology and Analogy based Reasoning for managing Implicit Requirements. It automates early identification and management of IMRs and is found helpful in targeted application domain. We present the PROMIRAR system with its architecture, demo and evaluation.},
annote = {cited By 1},
author = {Emebo, Onyeka and Olawande, Daramola and Charles, Ayo},
booktitle = {Lecture Notes in Engineering and Computer Science},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/emeboPromirar Tool for identifying and managing implicit Requirements in SRS documents.pdf:pdf},
isbn = {9789881404817},
issn = {20780958},
keywords = {Analogy-based reasoning,Implicit requirement,Natural language processing,Ontology,Requirement engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {108--112},
title = {{Promirar: Tool for identifying and managing implicit Requirements in SRS documents}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061941653{\&}partnerID=40{\&}md5=afbab6c71bb9104c72cdf781d12197aa},
volume = {2237},
year = {2018}
}
@article{hindle_topics_2015,
abstract = {Large organizations like Microsoft tend to rely on formal requirements documentation in order to specify and design the software products that they develop. These documents are meant to be tightly coupled with the actual implementation of the features they describe. In this paper we evaluate the value of high-level topic-based requirements traceability and issue report traceability in the version control system, using Latent Dirichlet Allocation (LDA). We evaluate LDA topics on practitioners and check if the topics and trends extracted match the perception that industrial Program Managers and Developers have about the effort put into addressing certain topics. We then replicate this study again on Open Source Developers using issue reports from issue trackers instead of requirements, confirming our previous industrial conclusions. We found that efforts extracted as commits from version control systems relevant to a topic often matched the perception of the managers and developers of what actually occurred at that time. Furthermore we found evidence that many of the identified topics made sense to practitioners and matched their perception of what occurred. But for some topics, we found that practitioners had difficulty interpreting and labelling them. In summary, we investigate the high-level traceability of requirements topics and issue/bug report topics to version control commits via topic analysis and validate with the actual stakeholders the relevance of these topics extracted from requirements and issues.},
author = {Hindle, Abram and Bird, Christian and Zimmermann, Thomas and Nagappan, Nachiappan},
doi = {10.1007/s10664-014-9312-1},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Hindle2015{\_}Article{\_}DoTopicsMakeSenseToManagersAnd.pdf:pdf},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {LDA,LDA topics,Requirements,Requirements topic,User study,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
number = {2},
pages = {479--515},
title = {{Do topics make sense to managers and developers?}},
url = {http://link.springer.com/10.1007/s10664-014-9312-1},
volume = {20},
year = {2015}
}
@conference{Uduwela2014370,
abstract = {Lack of Information Communication Technology (ICT) knowledge and the cost have been identified as challenges to Small and Medium Enterprises (SMEs) to adapt ICT. However there are tools and systems freely available to generate information systems automatically, but those require the database structure of the system. Database conceptualization based on the system requirement specification in natural language (SRS-NL) is the most significant landmark in the process of database design. Therefore it is desirable to have a tool, so that the non-technical people in SMEs could use to generate a quality conceptual database model (CDM) based on NL-SRS automatically. Comprehensive literature survey was conducted and evaluated the identified tools by analyzing usability, affordability and quality of the outcomes. Analysis showed that some limitations of the tools which cannot be used by non-technical people in SMEs.},
annote = {cited By 1},
author = {Uduwela, Wasana C. and Wijayarathna, Gamini},
booktitle = {IEEE International Conference on Industrial Engineering and Engineering Management},
doi = {10.1109/IEEM.2014.7058662},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/uduwela 07058662.pdf:pdf},
isbn = {9781479964109},
issn = {2157362X},
keywords = {ERD,Natural Language Processing (NLP),SME,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {370--373},
title = {{Survey on tools and systems to generate ER diagram from system requirement specification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988253839{\&}doi=10.1109{\%}2FIEEM.2014.7058662{\&}partnerID=40{\&}md5=35a8bcacaa1a9436ba3c73ebc5ba4b5e},
volume = {2015-Janua},
year = {2014}
}
@conference{Verma2013154,
abstract = {Software Requirements modeling is important for requirement understanding especially when they are expressed in Natural Language such as English, which is universally understood. Expressing requirement is such a way is natural and known to stakeholders. However, they are prone to ambiguity and poor understandability. This paper demonstrate how we can model software requirement expressed in natural language and represent them with a simple graph based structure using techniques of Natural Language Processing (NLP), this helps in understanding and correct interpretation of requirements. It can also represents knowledge about the requirement, which can be used to derive test case in early development phase. {\textcopyright} 2013 IEEE.},
annote = {cited By 1},
author = {Verma, Ravi Prakash and Beg, Md Rizwan},
booktitle = {International Conference on Emerging Trends in Engineering and Technology, ICETET},
doi = {10.1109/ICETET.2013.47},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Verma, Ravi Prakash and Beg, Md Rizwan{\_} {\_}{\_}Representation of knowledge from software requirements expressed in natural language{\_}{\_} (2013).pdf:pdf},
isbn = {9781479925605},
issn = {21570485},
keywords = {Knowledge acquisition Knowledge representation,Natural language processing,Software engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {154--158},
title = {{Representation of knowledge from software requirements expressed in natural language}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899008982{\&}doi=10.1109{\%}2FICETET.2013.47{\&}partnerID=40{\&}md5=6a3f830af6427717f5976eb348cbc633},
year = {2013}
}
@article{Bano20153,
abstract = {With the huge number of services that are available online, requirements analysts face an overload of choice when they have to select the most suitable service that satisfies a set of customer requirements. Both service descriptions and requirements are often expressed in natural language (NL), and natural language processing (NLP) tools that can match requirements and service descriptions, while filtering out irrelevant options, might alleviate the problem of choice overload faced by analysts. In this paper, we propose a NLP approach based on Knowledge Graphs that automates the process of service selection by ranking the service descriptions depending on their NL similarity with the requirements. To evaluate the approach, we have performed an experiment with 28 customer requirements and 91 service descriptions, previously ranked by a human assessor. We selected the top-15 services, which were ranked with the proposed approach, and found 53{\%} similar results with respect to top-15 services of the manual ranking. The same task, performed with the traditional cosine similarity ranking, produces only 13{\%} similar results. The outcomes of our experiment are promising, and new insights have also emerged for further improvement of the proposed technique.},
annote = {cited By 2},
author = {Bano, Muneera and Ferrari, Alessio and Zowghi, Didar and Gervasi, Vincenzo and Gnesi, Stefania},
doi = {10.1007/978-3-662-48634-4_1},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bano, Muneera and Ferrari, Alessio and Zowghi, Didar and Gervasi, Vincenzo and Gnesi, Stefania{\_} {\_}{\_}Automated service selection using natural language processing{\_}{\_} (2015).pdf:pdf},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Knowledge graphs,Natural language processing,Requirements engineering,Service selection,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {3--17},
title = {{Automated service selection using natural language processing}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983347911{\&}doi=10.1007{\%}2F978-3-662-48634-4{\_}1{\&}partnerID=40{\&}md5=b9d8c4e6221aa1565bc91fbbae4418cb},
volume = {558},
year = {2015}
}
@inproceedings{8920648,
abstract = {[Context] Digital transformation impacts an ever-increasing degree of everyone's business and private life. It is imperative to incorporate a wide audience of user requirements in the development process to design successful information systems (IS). Hence, requirements elicitation (RE) is increasingly performed by end-users that are novices at contributing requirements to IS development projects. [Objective] We need to develop RE systems that are capable of assisting a wide audience of end-users in communicating their needs and requirements. Prominent methods, such as elicitation interviews, are challenging to apply in such a context, as time and location constraints limit potential audiences. [Research Method] The presented dissertation project utilizes design science research to develop a requirements self-elicitation system, LadderBot. A conversational agent (CA) enables end-users to articulate needs and requirements on the grounds of the laddering method. The CA mimics a human interviewer's capability to rephrase questions and provide assistance in the process and allows users to converse in their natural language. Furthermore, the tool will assist requirements analysts with the subsequent aggregation and analysis of collected data. [Contribution] The dissertation project makes a practical contribution in the form of a ready-to-use system for wide audience end-user RE and subsequent analysis utilizing laddering as cognitive elicitation technique. A theoretical contribution is provided by developing a design theory for the application of conversational agents for RE, including the laboratory and field evaluation of design principles.},
author = {Rietz, Tim},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00061},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rietz, Tim{\_} {\_}{\_}Designing a conversational requirements elicitation system for end-users{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Conversational Agent,Design Science,End user,Laddering,Requirements Elicitation,Wide Audience,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {452--457},
title = {{Designing a conversational requirements elicitation system for end-users}},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{6480225,
abstract = {Open source projects requirements are mostly informal, text descriptions found in requests, forums, and other correspondence. Understanding of such requirements can provide insight into the nature of open source projects. Previously, we have demonstrated the Requirements Classifier for Natural Language (RCNL), which aids in NL requirements analysis. Herein, we demonstrate how the RCNL can aid in theory building. From its application to 16 open source projects, we conjecture a simple wave theory of requirements innovation: innovations expressed in requirements appear as a wave that is reflected in a subsequent wave of features that is reflected in a subsequent wave of product downloads. Although the theory is a conjecture, the process of its exploration demonstrates how RCNL can be used to explore theories about open source projects-theory exploration that would otherwise be intractable because of the difficulty in analyzing NL artifacts for requirements properties. {\textcopyright} 2012 IEEE.},
author = {Vlas, Radu and Robinson, William N.},
booktitle = {Proceedings of the Annual Hawaii International Conference on System Sciences},
doi = {10.1109/HICSS.2013.97},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Vlas, Radu and Robinson, William N.{\_} {\_}{\_}Applying a rule-based Natural Language Classifier to open source requirements{\_} A demonstration of theory exploration{\_}{\_} (2013).pdf:pdf},
isbn = {9780769548920},
issn = {15301605},
keywords = {formal specification,formal verification,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural l},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {jan},
pages = {3158--3167},
title = {{Applying a rule-based Natural Language Classifier to open source requirements: A demonstration of theory exploration}},
year = {2013}
}
@incollection{moreira_conflict_2013,
abstract = {Conflict identification in Aspect-Oriented Requirements Engineering (AORE) is an integral step toward resolving conflicting dependencies between requirements at an early stage of the software development. However, to date there has been no work supporting detection of conflicts in a large set of textual requirements without converting texts into an alternative representation (such as models or formal specification) or direct stakeholder involvement. Here, we present EA-Analyzer, an automated tool for identifying conflicts directly in aspect-oriented requirements specified in natural language text. This chapter is centered on a case study-based discussion of the accuracy of the tool. EA-Analyzer is applied to the Crisis Management System, a case study used as an established benchmark in several areas of aspect-oriented research.},
address = {Berlin, Heidelberg},
author = {Sardinha, Alberto and Chitchyan, Ruzanna and Ara{\'{u}}jo, Jo{\~{a}}o and Moreira, Ana and Rashid, Awais},
booktitle = {Aspect-Oriented Requirements Engineering},
doi = {10.1007/978-3-642-38640-4_11},
editor = {Moreira, Ana and Chitchyan, Ruzanna and Ara{\'{u}}jo, Jo{\~{a}}o and Rashid, Awais},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/sardinha Conflict identification with EA-analyzer.pdf:pdf},
isbn = {9783642386404},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {209--224},
publisher = {Springer Berlin Heidelberg},
title = {{Conflict identification with EA-analyzer}},
url = {http://link.springer.com/10.1007/978-3-642-38640-4{\_}11},
volume = {9783642386},
year = {2013}
}
@inproceedings{7397285,
abstract = {Requirements-based testing is a testing approach in which test cases are derived from requirements. Requirements represent the initial phase in software developments life cycle. Requirements are considered the basis of any software project. Therefore, any ambiguity in natural language requirements leads to major errors in the coming phases. Moreover, poorly defined requirements may cause software project failure. There exist many software development models as waterfall model, agile model, etc. In this paper, we propose a novel automated approach to generate test cases from requirements. Requirements can be gathered from different models either waterfall model (functional and non-functional) or agile model. SRS documents, non-functional requirements and user stories are parsed and used by the proposed approach to generate test cases in which requirements with different types are covered. The proposed approach uses text mining and symbolic execution methodology for test data generation and validation, where a knowledge base is developed for multi-disciplinary domains.},
author = {Elghondakly, Roaa and Moussa, Sherin and Badr, Nagwa},
booktitle = {2015 IEEE 7th International Conference on Intelligent Computing and Information Systems, ICICIS 2015},
doi = {10.1109/IntelCIS.2015.7397285},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Elghondakly, Roaa and Moussa, Sherin and Badr, Nagwa{\_} {\_}{\_}Waterfall and agile requirements-based model for automated test cases generation{\_}{\_} (2016).pdf:pdf},
isbn = {9781509019496},
keywords = {Agile,Requirements engineering,Requirements-based testing,Software testing,Test cases generation,Waterfall,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {607--612},
title = {{Waterfall and agile requirements-based model for automated test cases generation}},
year = {2016}
}
@inproceedings{8501306,
abstract = {We address the problem of extracting useful information contained in security and privacy breach reports. A breach report tells a short story describing how a breach happened and the follow-up remedial actions taken by the responsible parties. By predicting sentences that may follow a breach description using natural language processing, our goal is to suggest security and privacy requirements for practitioners and end users that can be used to prevent and recover from such breaches. We prepare a curated dataset of structured short breach stories using unstructured breach reports published by the U.S. Department of Health and Human Services. We propose a prediction model for inferring held-out sentences based on Paragraph Vector, a document embedding method, and Long Short-Term Memory networks. The predicted sentences can suggest natural language requirements. We evaluate our model on the curated dataset as well as the ROCStories corpus, a collection of five-sentence commonsense stories, and find that the presented model performs significantly better than the baseline of using average word vectors.},
author = {Guo, Hui and Kafali, Ozgur and Singh, Munindar},
booktitle = {Proceedings - 2018 5th International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2018},
doi = {10.1109/AIRE.2018.00009},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/guo Extraction of Natural Language Requirements from Breach Reports Using Event Inference.pdf:pdf},
isbn = {9781538684047},
keywords = {Breach reports,Event inference,Long Short-Term Memory architecture,Recurrent Neural Networks,Security and privacy requirements,Story Cloze Test,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {22--28},
title = {{Extraction of Natural Language Requirements from Breach Reports Using Event Inference}},
year = {2018}
}
@article{Okano201912,
abstract = {A requirement specification for software is usually described in a natural language and thus may include sentences containing ambiguity and contradiction. Problems due to the ambiguity often occur at the stage of the verification process of software development, and this forces developers to go back to the design process again. In order to prevent this kind of rework, a method of automatically converting a required specification written in Japanese to a state transition model is desired to help detect ambiguity and contradiction points of the specification. This paper proposes a method for this purpose, and reports on the result of applying the method to a specification example of an electric pot.},
annote = {cited By 1},
author = {Okano, Kozo and Takahashi, Kazuma and Ogata, Shinpei and Sekizawa, Toshifusa},
doi = {10.1007/978-3-319-97679-2_2},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Okano2019{\_}Chapter{\_}AnalysisOfSpecificationInJapan.pdf:pdf},
isbn = {9783319976785},
issn = {21903026},
journal = {Smart Innovation, Systems and Technologies},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {12--21},
title = {{Analysis of specification in Japanese using natural language processing}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051764855{\&}doi=10.1007{\%}2F978-3-319-97679-2{\_}2{\&}partnerID=40{\&}md5=3dff282862794e2b3e2713e8aa66f1c3},
volume = {108},
year = {2019}
}
@article{7366597,
abstract = {Adoption of Software Product Line Engineering (SPLE) to support systematic reuse of software-related artifacts within product families is challenging, time-consuming and error-prone. Analyzing the variability of existing artifacts needs to reflect different perspectives and preferences of stakeholders in order to facilitate decisions in SPLE adoption. Considering that requirements drive many development methods and activities, we introduce an approach to analyze variability of behaviors as presented in functional requirements. The approach, called semantic and ontological variability analysis (SOVA), uses ontological and semantic considerations to automatically analyze differences between initial states (preconditions), external events (triggers) that act on the system, and final states (post-conditions) of behaviors. The approach generates feature diagrams typically used in SPLE to model variability. Those diagrams are organized according to perspective profiles, reflecting the needs and preferences of the potential stakeholders for given tasks. We conducted an empirical study to examine the usefulness of the approach by comparing it to an existing tool which is mainly based on a latent semantic analysis measurement. SOVA appears to create outputs that are more comprehensible in significantly shorter times. These results demonstrate SOVA's potential to allow for flexible, behavior-oriented variability analysis.},
author = {Itzik, Nili and Reinhartz-Berger, Iris and Wand, Yair},
doi = {10.1109/TSE.2015.2512599},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Itzik, Nili and Reinhartz-Berger, Iris and Wand, Yair{\_} {\_}{\_}Variability Analysis of Requirements{\_} Considering Behavioral Differences and Reflecting Stakeholders' Perspectives{\_}{\_} (2016).pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Software product line engineering,feature diagrams,ieee{\_}inc{\_}nlp{\_}x{\_}re,ontology,requirements specifications,variability analysis},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
number = {7},
pages = {687--706},
title = {{Variability Analysis of Requirements: Considering Behavioral Differences and Reflecting Stakeholders' Perspectives}},
volume = {42},
year = {2016}
}
@inproceedings{6912260,
abstract = {Natural language artifacts, such as requirements specifications, often explicitly state the security requirements for software systems. However, these artifacts may also imply additional security requirements that developers may overlook but should consider to strengthen the overall security of the system. The goal of this research is to aid requirements engineers in producing a more comprehensive and classified set of security requirements by (1) automatically identifying security-relevant sentences in natural language requirements artifacts, and (2) providing context-specific security requirements templates to help translate the security-relevant sentences into functional security requirements. Using machine learning techniques, we have developed a tool-assisted process that takes as input a set of natural language artifacts. Our process automatically identifies security-relevant sentences in the artifacts and classifies them according to the security objectives, either explicitly stated or implied by the sentences. We classified 10,963 sentences in six different documents from healthcare domain and extracted corresponding security objectives. Our manual analysis showed that 46{\%} of the sentences were security-relevant. Of these, 28{\%} explicitly mention security while 72{\%} of the sentences are functional requirements with security implications. Using our tool, we correctly predict and classify 82{\%} of the security objectives for all the sentences (precision). We identify 79{\%} of all security objectives implied by the sentences within the documents (recall). Based on our analysis, we develop context-specific templates that can be instantiated into a set of functional security requirements by filling in key information from security-relevant sentences.},
author = {Riaz, Maria and King, Jason and Slankas, John and Williams, Laurie},
booktitle = {2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings},
doi = {10.1109/RE.2014.6912260},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Riaz, Maria and King, Jason and Slankas, John and Williams, Laurie{\_} {\_}{\_}Hidden in plain sight{\_} Automatically identifying security requirements from natural language artifacts{\_}{\_} (2014).pdf:pdf},
isbn = {9781479930333},
issn = {2332-6441},
keywords = {Security,access control,auditing,constraints,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language parsing,objectives,requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re,templates,text classification},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {183--192},
title = {{Hidden in plain sight: Automatically identifying security requirements from natural language artifacts}},
year = {2014}
}
@conference{Garigliano2018,
abstract = {Natural language processing has been proposed and applied to support a variety of tasks in requirements engineering. While shallow semantic allows to address many of the challenges, to further automatize requirements analysis a full understanding of textual requirements is needed. To this end, the future generation of natural language processing systems needs a deep semantics, that is a representation of the content independent of the surface description, which represents hidden casual, spatial, temporal and modal connections.},
annote = {cited By 1},
author = {Garigliano, Roberto and Perini, Dominic and Mich, Luisa},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/garig2018.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Which semantics for requirements engineering: From shallow to deep}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045456553{\&}partnerID=40{\&}md5=4cb8b02fff487487ea9c4d646435de4e},
volume = {2075},
year = {2018}
}
@inproceedings{8616992,
abstract = {User involvement in the process of discovering and shaping the product is the base of software systems. In recent years, however, a shift in the user feedback has been observed: repositories of user data have become increasingly more subjected to analysis for improvement purposes. Significant surge has been seen in feedback collected from users in the form of reviews and ratings along with app usage statistics. This led software engineering researchers to deploy big data analytics techniques in order to figure out the requirements that should be met in the future software system releases. While a variety of big data analytics methods exist, it is not clear which ones have been used and what are the benefits and disadvantages of these proposals. In this paper, we have aimed to outline the recently published proposals for big data analytics techniques for user feedback analysis. We found that the majority of the techniques rest on natural language processing concepts and visualization. Our findings also indicate that the majority of the proposals come from the United States, Germany and the United Kingdom. Moreover, we also found the proposed techniques perform well with the chosen data-sets however the generalizability and scalability of these method raised concerns as these methods are not evaluated based on real-world cases.},
author = {Bukhsh, Faiza Allah and Arachchige, Jeewanie Jayasinghe and Malik, Furqan},
booktitle = {Proceedings - 2018 International Conference on Frontiers of Information Technology, FIT 2018},
doi = {10.1109/FIT.2018.00043},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bukhsh, Faiza Allah and Arachchige, Jeewanie Jayasinghe and Malik, Furqan{\_} {\_}{\_}Analyzing excessive user feedback{\_} A big data challenge{\_}{\_} (2019).pdf:pdf},
isbn = {9781538693551},
issn = {2334-3141},
keywords = {Big data analytics,Feedback analysis,User Reviews,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {206--211},
title = {{Analyzing excessive user feedback: A big data challenge}},
year = {2019}
}
@article{10.1145/2735399.2735414,
abstract = {User satisfaction is recognized as an important contributor to the success of software applications. It is subjective and influenced by several factors that are linked to the non-functional requirements. Although non-functional requirements provide good criteria for selection of web service in Service-Oriented Architecture, specifying them during the discovery process is a difficult task. Dealing with the non-functional requirements that have an inherent ability to conflict is a complex task and modeling them can facilitate analyzing them. The aim of our research work is to propose a framework to improve design stage during web service selection so as to produce minimal conflicts and dependencies between requirements. We argue that the design structure that best supports and embodies the user constraints is the one that best meets a user's needs. Our framework is proposed after a detailed study of the issues and challenges identified by published literature by researchers working on non-functional requirement conflicts. To exemplify our work we consider a Remote Patient Monitoring System scenario and propose a complete framework that will take the user requirements as input and analyze how they interact using graph transformation rules. The output matrix of dependencies can help the designer to select the most desirable design solution. We make use of simple natural language processing techniques, graph transformation and the aspect oriented paradigm. By integrating the framework into the design phase, it is possible to limit the impact of changes and also to understand in advance the likely interdependencies.},
address = {New York, NY, USA},
author = {Phalnikar, Rashmi and Jinwala, Devesh},
doi = {10.1145/2735399.2735414},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Phalnikar, Rashmi and Jinwala, Devesh{\_} {\_}{\_}Analysis of Conflicting User Requirements in Web Applications Using Graph Transformation{\_}{\_} (2015).pdf:pdf},
issn = {0163-5948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {Conflicts and Dependencies,Graph Transformations,Non-Functional Requirements,Requirement Engineering,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
number = {2},
pages = {1--7},
publisher = {Association for Computing Machinery},
title = {{Analysis of Conflicting User Requirements in Web Applications Using Graph Transformation}},
url = {https://doi.org/10.1145/2735399.2735414},
volume = {40},
year = {2015}
}
@article{Wang2019547,
abstract = {Inconsistent specification are an inevitable intermediate product of a service requirements engineering process. In order to reduce requirements inconsistencies, we propose PASER, a Pattern-based Approach to Service Requirements analysis. The PASER approach first extracts the process information from service documents via natural language processing (NLP) techniques, then uses a requirements modeling language - Workflow-Patterns-based Process Language (WPPL) - to build the process model. Finally, through matching with workflow patterns, the inconsistencies in service requirements are identified and resolved by checking against a set of checking rules. We have conducted a preliminary experiment to evaluate it. An ATM service case study is presented as a running example to illustrate our approach.},
annote = {cited By 0},
author = {Wang, Ye and Wang, Ting and Sun, Jie},
doi = {10.1142/S0218194019500232},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Wang, Ye and Wang, Ting and Sun, Jie{\_} {\_}{\_}PASER{\_} A pattern-based approach to service requirements analysis{\_}{\_} (2019).pdf:pdf},
issn = {02181940},
journal = {International Journal of Software Engineering and Knowledge Engineering},
keywords = {Service requirements,natural language processing,requirement inconsistency,scopus{\_}inc{\_}nlp{\_}x{\_}re,service requirements analysis,workflow patterns,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {4},
pages = {547--576},
title = {{PASER: A pattern-based approach to service requirements analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064808850{\&}doi=10.1142{\%}2FS0218194019500232{\&}partnerID=40{\&}md5=49477ce98cd30b32e7aa9b6fec339fc8},
volume = {29},
year = {2019}
}
@article{Guzman2017387,
abstract = {Users of the Twitter microblogging platform share a considerable amount of information through short messages on a daily basis. Some of these so-called tweets discuss issues related to software and could include information that is relevant to the companies developing these applications. Such tweets have the potential to help requirements engineers better understand user needs and therefore provide important information for software evolution. However, little is known about the nature of tweets discussing software-related issues. In this paper, we report on the usage characteristics, content and automatic classification potential of tweets about software applications. Our results are based on an exploratory study in which we used descriptive statistics, content analysis, machine learning and lexical sentiment analysis to explore a dataset of 10,986,495 tweets about 30 different software applications. Our results show that searching for relevant information on software applications within the vast stream of tweets can be compared to looking for a needle in a haystack. However, this relevant information can provide valuable input for software companies and support the continuous evolution of the applications discussed in these tweets. Furthermore, our results show that it is possible to use machine learning and lexical sentiment analysis techniques to automatically extract information about the tweets regarding their relevance, authors and sentiment polarity.},
annote = {cited By 14},
author = {Guzman, Emitza and Alkadhi, Rana and Seyff, Norbert},
doi = {10.1007/s00766-017-0274-x},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Guzman, Emitza and Alkadhi, Rana and Seyff, Norbert{\_} {\_}{\_}An exploratory study of Twitter messages about software applications{\_}{\_} (2017).pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Content analysis,Requirements engineering,Software evolution,Textmining,User feedback,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {387--412},
title = {{An exploratory study of Twitter messages about software applications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757129{\&}doi=10.1007{\%}2Fs00766-017-0274-x{\&}partnerID=40{\&}md5=cbda7c25c5e3ab0bbedc09fbbdac5676},
volume = {22},
year = {2017}
}
@inproceedings{8491131,
abstract = {During the development or maintenance of an Android app, the app developer needs to determine the app's security and privacy requirements such as permission requirements. Permission requirements include two folds. First, what permissions (i.e., access to sensitive resources, e.g., location or contact list) the app needs to request. Second, how to explain the reason of permission usages to users. In this paper, we focus on the multiple challenges that developers face when creating permission-usage explanations. We propose a novel framework, CLAP, that mines potential explanations from the descriptions of similar apps. CLAP leverages information retrieval and text summarization techniques to find frequent permission usages. We evaluate CLAP on a large dataset containing 1.4 million Android apps. The evaluation results outperform existing state-of-the-art approaches, showing great promise of CLAP as a tool for assisting developers and permission requirements discovery.},
author = {Liu, Xueqing and Leng, Yue and Yang, Wei and Zhai, Chengxiang and Xie, Tao},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00024},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Liu, Xueqing and Leng, Yue and Yang, Wei and Zhai, Chengxiang and Xie, Tao{\_} {\_}{\_}Mining android app descriptions for permission requirements recommendation{\_}{\_} (2018).pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {Android permission,Natural language processing,Security requirement,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {147--158},
title = {{Mining android app descriptions for permission requirements recommendation}},
year = {2018}
}
@article{Zolotas2017791,
abstract = {During the last few years, the REST architectural style has drastically changed the way web services are developed. Due to its transparent resource-oriented model, the RESTful paradigm has been incorporated into several development frameworks that allow rapid development and aspire to automate parts of the development process. However, most of the frameworks lack automation of essential web service functionality, such as authentication or database searching, while the end product is usually not fully compliant to REST. Furthermore, most frameworks rely heavily on domain specific modeling and require developers to be familiar with the employed modeling technologies. In this paper, we present a Model-Driven Engineering (MDE) engine that supports fast design and implementation of web services with advanced functionality. Our engine provides a front-end interface that allows developers to design their envisioned system through software requirements in multimodal formats. Input in the form of textual requirements and graphical storyboards is analyzed using natural language processing techniques and semantics, to semi-automatically construct the input model for the MDE engine. The engine subsequently applies model-to-model transformations to produce a RESTful, ready-to-deploy web service. The procedure is traceable, ensuring that changes in software requirements propagate to the underlying software artefacts and models. Upon assessing our methodology through a case study and measuring the effort reduction of using our tools, we conclude that our system can be effective for the fast design and implementation of web services, while it allows easy wrapping of services that have been engineered with traditional methods to the MDE realm.},
annote = {cited By 5},
author = {Zolotas, Christoforos and Diamantopoulos, Themistoklis and Chatzidimitriou, Kyriakos C. and Symeonidis, Andreas L.},
doi = {10.1007/s10515-016-0206-x},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/ZOLOTA{\~{}}1.PDF:PDF},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Automated Software Engineering,Model-Driven Engineering,RESTful web services,Software requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {4},
pages = {791--838},
title = {{From requirements to source code: a Model-Driven Engineering approach for RESTful web services}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984844780{\&}doi=10.1007{\%}2Fs10515-016-0206-x{\&}partnerID=40{\&}md5=8bd9650aa74d0c817f0c41b23e2e3cd0},
volume = {24},
year = {2017}
}
@inproceedings{7949577,
abstract = {Software requirements analysis is crucial for any software project and it is the basis of requirements reuse within Software Product Line engineering. Software requirements specifications are usually expressed in natural language, which are informal, imprecise and ambiguous, thus analyzing them automatically is a challenging task. Although methods towards automatic analysis of software requirements have been studied before, many of them have limitations and effective researches in this area are still lacking. Therefore, in this paper a new approach was proposed to automatically extract structured information of functional requirements from Software Requirements Specifications in natural language. The methods of machine learning, natural language processing and semantic analysis were employed and combined in this approach. With a 10-fold cross validation setting, the method was evaluated on a manually annotated corpus. The experiments show that this approach achieves a good performance. Moreover, it was found that the model trained on the requirements dataset of the Ecommerce systems, can be used to extract semantic information from the requirements of auto-maker systems. The cross-domain extraction results show that the method of this paper is domain independent and robust to some extent.},
author = {Wang, Yinglin and Zhang, Jianzhang},
booktitle = {PIC 2016 - Proceedings of the 2016 IEEE International Conference on Progress in Informatics and Computing},
doi = {10.1109/PIC.2016.7949577},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Wang, Yinglin and Zhang, Jianzhang{\_} {\_}{\_}Experiment on automatic functional requirements analysis with the EFRFs semantic cases{\_}{\_} (2017).pdf:pdf},
isbn = {9781509034833},
keywords = {EFRF,Functional requirements extraction,Requirements engineering,Semantic analysis,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {636--642},
title = {{Experiment on automatic functional requirements analysis with the EFRFs semantic cases}},
year = {2017}
}
@inproceedings{10.1145/3167132.3167321,
abstract = {The1 growth of social networks impacts on several areas of our society. They can be used during software development, more specifically in the requirements elicitation activity, for identifying, complementing and validating the domain requirements. In this paper, we propose an approach that shows how social networks can be used as a source for capturing domain requirements. The aim is to perform the initial modeling of the system domain, providing a systematic methodology (process) for rapidly capturing relevant features that would not be straightforwardly elicited using traditional approaches. We apply this approach to the emergency systems domain (more specifically, floods in coastal areas), extracting information from the Twitter social network. The result is a domain model whose features can be reused in several applications of that domain. The application of the approach has been evaluated for its usefulness by domain experts and replicated to verify the generation of similar results at different time periods.},
address = {New York, NY, USA},
author = {Borges, Cl{\'{a}}udio and Ara{\'{u}}jo, Jo{\~{a}}o and Rodrigues, Armanda},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3167132.3167321},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/BORGES{\~{}}1.PDF:PDF},
isbn = {9781450351911},
keywords = {Domain model,Emergency systems,Feature model,Requirements elicitation,Social networks,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1772--1781},
publisher = {Association for Computing Machinery},
series = {SAC '18},
title = {{Towards an approach to elicit domain requirements from social networks: The case of emergency systems}},
url = {https://doi.org/10.1145/3167132.3167321},
year = {2018}
}
@conference{Iwama20121012,
abstract = {This paper describes a novel framework for creating a parser to process and analyze texts written in a "partially structured" natural language. In many projects, the contents of document artifacts tend to be described as a mixture of formal parts (i.e. the text constructs follow specific conventions) and parts written in arbitrary free text. Formal parsers, typically defined and used to process a description with rigidly defined syntax such as program source code are very precise and efficient in processing the formal part, while parsers developed for natural language processing (NLP) are good at robustly interpreting the free-text part. Therefore, combining these parsers with different characteristics can allow for more flexible and practical processing of various project documents. Unfortunately, conventional approaches to constructing a parser from multiple small parsers were studied extensively only for formal language parsers and are not directly applicable to NLP parsers due to the differences in the way the input text is extracted and evaluated. We propose a method to configure and generate a combined parser by extending an approach based on parser combinator, the operators for composing multiple formal parsers, to support both NLP and formal parsers. The resulting text parser is based on Parsing Expression Grammars, and it benefits from the strength of both parser types. We demonstrate an application of such combined parser in practical situations and show that the proposed approach can efficiently construct a parser for analyzing project-specific industrial specification documents. {\textcopyright} 2012 IEEE.},
annote = {cited By 5},
author = {Iwama, Futoshi and Nakamura, Taiga and Takeuchi, Hironori},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2012.6227119},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Iwama, Futoshi and Nakamura, Taiga and Takeuchi, Hironori{\_} {\_}{\_}Constructing parser for industrial software specifications containing formal and natural language description{\_}{\_} (2012).pdf:pdf},
isbn = {9781467310673},
issn = {02705257},
keywords = {Document Analysis,Parser Combinator,Parsing Expression Grammars,Requirement Engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1012--1021},
title = {{Constructing parser for industrial software specifications containing formal and natural language description}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864257712{\&}doi=10.1109{\%}2FICSE.2012.6227119{\&}partnerID=40{\&}md5=d5ee7b815c0f0925779b7fb66e2b3e56},
year = {2012}
}
@conference{Kashmira2018,
abstract = {An entity relationship data model is a high level conceptual model that describes information as entities, attributes relationships and constraints. Entity relationship diagrams to design the database of the software. It involves a sequence of tasks including extracting the requirements, identifying the entities, their attributes, the relationship between the entities, constraints and finally drawing the diagram. As such entity relationship diagram design has become a tedious task for novice designer. This research addresses the above issue, proposes a Natural Language Processing based tool which accepts requirement specification written in English language and generates entity relationship diagram.},
annote = {cited By 0},
author = {Kashmira, P. G.T.H. and Sumathipala, Sagara},
booktitle = {2018 3rd International Conference on Information Technology Research, ICITR 2018},
doi = {10.1109/ICITR.2018.8736146},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kashmira, P. G.T.H. and Sumathipala, Sagara{\_} {\_}{\_}Generating Entity Relationship Diagram from Requirement Specification based on NLP{\_}{\_} (2018).pdf:pdf},
isbn = {9781728114705},
keywords = {Entity relationship data model,Natural Language Processing,Requirement Specification,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Generating Entity Relationship Diagram from Requirement Specification based on NLP}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068436771{\&}doi=10.1109{\%}2FICITR.2018.8736146{\&}partnerID=40{\&}md5=a549e7bcdba021306a9c8d792f908bc1},
year = {2018}
}
@inproceedings{10.1145/2905055.2905179,
abstract = {The most significant phase in the development of a quality software project is Requirement engineering. The objective of the software requirement engineering is the elicitation of the requirements of the clients and their analysis. In general the requirements are expressed in natural languages which are ambiguous in nature. Ambiguity means the same word or sentence can be interpreted differently by different persons. The Word Sense Disambiguation (WSD) system assigns the correct meaning to the words having multiple interpretations, depending on the context of use. In this paper, we propose a framework, for removing ambiguities in an SRS (Software Requirement Specifications) document in an efficient way. This framework uses the WordNet and the concept of Association rule mining for assigning the correct interpretation of a word in given context.},
address = {New York, NY, USA},
author = {Husain, Mohd Shahid and {Akheela Khanum}, M.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2905055.2905179},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Husain, Mohd Shahid and {\_}Akheela Khanum{\_}, M.{\_} {\_}{\_}Word sense disambiguation in software requirement specifications using Wordnet and association mining rule{\_}{\_} (2016).pdf:pdf},
isbn = {9781450339629},
keywords = {Association Rule Mining,SRS,WSD,WordNet,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ICTCS '16},
title = {{Word sense disambiguation in software requirement specifications using Wordnet and association mining rule}},
url = {https://doi.org/10.1145/2905055.2905179},
volume = {04-05-Marc},
year = {2016}
}
@inproceedings{7765525,
abstract = {Natural language (NL) is still the predominant notation that practitioners use to represent software requirements. Albeit easy to read, NL does not readily highlight key concepts and relationships such as dependencies and conflicts. This contrasts with the inherent capability of graphical conceptual models to visualize a given domain in a holistic fashion. In this paper, we propose to automatically derive conceptual models from a concise and widely adopted NL notation for requirements: user stories. Due to their simplicity, we hypothesize that our approach can improve on the low accuracy of previous works. We present an algorithm that combines state-of-the-art heuristics and that is implemented in our Visual Narrator tool. We evaluate our work on two case studies wherein we obtained promising precision and recall results (between 80{\%} and 92{\%}). The creators of the user stories perceived the generated models as a useful artifact to communicate and discuss the requirements, especially for team members who are not yet familiar with the project.},
author = {Robeer, Marcel and Lucassen, Garm and {Van Der Werf}, Jan Martijn E.M. and Dalpiaz, Fabiano and Brinkkemper, Sjaak},
booktitle = {Proceedings - 2016 IEEE 24th International Requirements Engineering Conference, RE 2016},
doi = {10.1109/RE.2016.40},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/ROBEER{\~{}}1.PDF:PDF},
isbn = {9781509041213},
issn = {2332-6441},
keywords = {NLP,User stories,conceptual modeling,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {196--205},
title = {{Automated Extraction of Conceptual Models from User Stories via NLP}},
year = {2016}
}
@article{Younas2019,
abstract = {Functional and non-functional requirements are important equally in software development. Usually, the requirements are expressed in natural languages. The functional and non-functional requirements are written inter-mixed in software requirement document. The extraction of requirement from the software requirement document is a challenging task. Most of the recent studies adopted a supervised learning approach for the extraction of non-functional requirements. However, there is a drawback of supervised learning such as training of model and retrain if the domain changed. The proposed approach manipulates the textual semantic of functional requirements to identify the non-functional requirements. The semantic similarity is calculated based on co-occurrence of patterns in large human knowledge repositories of Wikipedia. This study finds the similarity distance between the popular indicator keywords and requirement statements to identify the type of non-functional requirement. The proposed approach is applied to PROMISE “NFR dataset.” The performance of the proposed approach is measured in terms of precision, recall and F-measure. Furthermore, the research applies three pre-processing approaches (traditional, part of speech tagging and word augmentation) to increase the performance of NFR extraction. The proposed approach outperforms the results of existing studies.},
annote = {cited By 0},
author = {Younas, Muhammad and Jawawi, D. N.A. and Ghani, Imran and Shah, Muhammad Arif},
doi = {10.1007/s00521-019-04226-5},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Younas, Muhammad and Jawawi, D. N.A. and Ghani, Imran and Shah, Muhammad Arif{\_} {\_}{\_}Extraction of non-functional requirement using semantic similarity distance{\_}{\_} (2019).pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Machine learning,Natural language processing,Non-functional requirement,Semantic similarity,Word2Vec,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Extraction of non-functional requirement using semantic similarity distance}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066890830{\&}doi=10.1007{\%}2Fs00521-019-04226-5{\&}partnerID=40{\&}md5=408051793ca49efdbb9a4f35f59d61ce},
year = {2019}
}
@conference{Slob2018,
abstract = {Requirements visualization can contribute to requirements comprehension through the creation of conceptual models. However, these models can become hard to read and current tool support is minimal. Applying the right visualization mechanisms can help construct models that are more readable. To such extent, we present the Interactive Narrator tool: a web application that helps practitioners analyze software requirements at an abstract level. Interactive Narrator uses Natural Language Processing to derive conceptual models from user stories, which are then translated into an interactive network diagram with zooming and filtering capabilities. Interactive Narrator facilitates discussion and aims to accelerate the understanding of large sets of software requirements.},
annote = {cited By 0},
author = {Slob, Govert Jan and Dalpiaz, Fabiano and Brinkkemper, Sjaak and Lucassen, Garm},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/slob.pdf:pdf},
issn = {16130073},
keywords = {Agile development,Requirements visualization,User stories,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{The interactive narrator tool: Effective requirements exploration and discussion through visualization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045417653{\&}partnerID=40{\&}md5=38b4c289ac0d9a8e7b2518058e56d39a},
volume = {2075},
year = {2018}
}
@inproceedings{7549323,
abstract = {Consumer choices are enormously influential in the success of the companies and organizations behind the highly competitive global service and product offerings of today. Consumer choice relates to preference, i.e. a set of assumptions a person creates around a service or a product such as convenience, utility or aesthetics. Furthermore, consumer preferences allow ranking of different assumptions about products or services based on the expected or to-be-experienced satisfaction of consuming them. In our previous work, we proposed a conceptualization of consumer preferences - the Consumer Preference Meta-Model (CPMM) - to enable a classification and ranking of the preferences that would be the basis for deciding which of would be considered to be developed into supporting information systems/services. In this study we collect consumer preferences through crowdsourcing, and in particular Twitter, because of its increasing popularity as a source of up-to-date comments and information about current services and products. The tweets of four major American airlines were processed using different techniques from natural language processing (NLP) that enabled the classification of their objectives, content, and importance within CPMM. By next mapping the highest-ranked results from CPMM to goal models enabled a model-based linkage from a corpus of preferences contained within short texts to high-level requirements for system/services.},
author = {Svee, Eric Oluf and Zdravkovic, Jelena},
booktitle = {Proceedings - International Conference on Research Challenges in Information Science},
doi = {10.1109/RCIS.2016.7549323},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Svee, Eric Oluf and Zdravkovic, Jelena{\_} {\_}{\_}A model-based approach for capturing consumer preferences from crowdsources{\_} The case of Twitter{\_}{\_} (2016).pdf:pdf},
isbn = {9781479987092},
issn = {21511357},
keywords = {consumer preference,consumer value,crowdsourcing,goal modeling,ieee{\_}inc{\_}nlp{\_}x{\_}re,model-based requirements,value},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--12},
title = {{A model-based approach for capturing consumer preferences from crowdsources: The case of Twitter}},
volume = {2016-Augus},
year = {2016}
}
@inproceedings{7148485,
abstract = {An acceptable solution is built only if the problem space is correctly defined, which is the prime reason to perform a Requirement Engineering process before the development of the project. Requirement Engineering is the most important phase of a software development and holds the ability to affect the entire software development actions if improperly implemented. However, the requirement engineering process faces many problems. Two major problems are inconsistencies and ambiguities, since requirements are elicited from multiple stakeholders and are specified in natural language in SRS. Consequently, the problems should be resolved timely so that it does not move to the next phase of software development process. The purpose of the study is to review and discuss some of the existing techniques to detect inconsistencies and ambiguities in software requirements. Later observations on the existing work are also discussed and a framework for the same is also suggested.},
author = {Sandhu, Geet and Sikka, Sunil},
booktitle = {International Conference on Computing, Communication and Automation, ICCCA 2015},
doi = {10.1109/CCAA.2015.7148485},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sandhu, Geet and Sikka, Sunil{\_} {\_}{\_}State-of-art practices to detect inconsistencies and ambiguities from software requirements{\_}{\_} (2015).pdf:pdf},
isbn = {9781479988907},
keywords = {Ambiguity,Inconsistency,Requirement Engineering,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {812--817},
title = {{State-of-art practices to detect inconsistencies and ambiguities from software requirements}},
year = {2015}
}
@inproceedings{6051627,
abstract = {Reuse of requirements leads to reduction in time spent for specification of new products. Variant management of requirement documents is an essential prerequisite in terms of a successful reuse of requirements. It supports the decisions if available requirements can be reused or not. One possibility to document the variability is feature modelling. One main challenge while introducing feature modelling in a grown environment is to extract product features from large natural language specifications. The current practice is a manual review of specifications conducted by domain experts. This procedure is very costly in terms of time. A promising approach to optimize feature identification is a semi-automatic identification of features in natural language specifications based on lexical analysis. This paper presents the current approaches used for handling variability in automotive specifications at Daimler passenger car development along with first experiences gained in using the optimized approach for feature identification using a lexical analysis. {\textcopyright} 2011 IEEE.},
author = {Boutkova, Ekaterina and Houdek, Frank},
booktitle = {Proceedings of the 2011 IEEE 19th International Requirements Engineering Conference, RE 2011},
doi = {10.1109/RE.2011.6051627},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/boutkova2011.pdf:pdf},
isbn = {9781457709234},
issn = {2332-6441},
keywords = {automotive,feature identification,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirements management,specifications,variability management},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {313--318},
title = {{Semi-automatic identification of features in requirement specifications}},
year = {2011}
}
@inproceedings{6636700,
abstract = {Businesses and organizations in jurisdictions around the world are required by law to provide their customers and users with information about their business practices in the form of policy documents. Requirements engineers analyze these documents as sources of requirements, but this analysis is a time-consuming and mostly manual process. Moreover, policy documents contain legalese and present readability challenges to requirements engineers seeking to analyze them. In this paper, we perform a large-scale analysis of 2,061 policy documents, including policy documents from the Google Top 1000 most visited websites and the Fortune 500 companies, for three purposes: (1) to assess the readability of these policy documents for requirements engineers; (2) to determine if automated text mining can indicate whether a policy document contains requirements expressed as either privacy protections or vulnerabilities; and (3) to establish the generalizability of prior work in the identification of privacy protections and vulnerabilities from privacy policies to other policy documents. Our results suggest that this requirements analysis technique, developed on a small set of policy documents in two domains, may generalize to other domains. {\textcopyright} 2013 IEEE.},
author = {Massey, Aaron K. and Eisenstein, Jacob and Anton, Annie I. and Swire, Peter P.},
booktitle = {2013 21st IEEE International Requirements Engineering Conference, RE 2013 - Proceedings},
doi = {10.1109/RE.2013.6636700},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Massey, Aaron K. and Eisenstein, Jacob and Anton, Annie I. and Swire, Peter P.{\_} {\_}{\_}Automated text mining for requirements analysis of policy documents{\_}{\_} (2013).pdf:pdf},
isbn = {9781467357654},
issn = {2332-6441},
keywords = {data mining,data privacy,formal verification,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,softw},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {4--13},
title = {{Automated text mining for requirements analysis of policy documents}},
year = {2013}
}
@article{7765062,
abstract = {A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.},
author = {Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank},
doi = {10.1109/TSE.2016.2635134},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel and Zimmer, Frank{\_} {\_}{\_}Automated Extraction and Clustering of Requirements Glossary Terms{\_}{\_} (2017).pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Requirements glossaries,case study research,clustering,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,term extraction},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
number = {10},
pages = {918--945},
title = {{Automated Extraction and Clustering of Requirements Glossary Terms}},
volume = {43},
year = {2017}
}
@conference{Çevikol2019,
abstract = {The ground segment constitutes the ground–based infrastructure necessary to support the operations of satellites, including the control of the spacecraft in orbit, and the acquisition, reception, processing and delivery of the data. Since the ground segment is one of the essential elements in satellite operations, the quality of the requirements are critically important for the success of the satellite missions. Similar to many other large-scale systems, requirements for the ground segment are documented in natural language, making them prone to ambiguity and vagueness, and making it difficult to check properties such as completeness and consistency. Due to these shortcomings, the review process of the requirements is expensive in terms of time and effort. Our aim is to provide automated support for detecting inconsistencies in the ground segment requirements. Our approach relies on natural language processing and machine learning techniques. Our plan is to validate our work on a real ground segment requirement set.},
annote = {cited By 0},
author = {{\c{C}}evikol, Sercan and Aydemir, Fatma Başak},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/cevikol Detecting inconsistencies of natural language requirements in satellite ground segment domain.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Detecting inconsistencies of natural language requirements in satellite ground segment domain}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068025155{\&}partnerID=40{\&}md5=384f8ee4f14752fa851d41ceda7285ed},
volume = {2376},
year = {2019}
}
@conference{Gnesi2019,
abstract = {QuARS (Quality Analyzer for Requirements Specifications) is a tool able to perform an analysis of Natural Language (NL) requirements in a systematic and an automatic way by means of natural language processing techniques with a focus on ambiguity detection. QuARS allows the requirements engineers to perform an early analysis of the requirements for automatically detecting potential linguistic defects.},
annote = {cited By 1},
author = {Gnesi, Stefania and Trentanni, Gianluca},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/gnesi QuARS A NLP tool for requirements analysis.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{QuARS: A NLP tool for requirements analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068049537{\&}partnerID=40{\&}md5=ebe401d150a132c9f61eb003bed3d0ec},
volume = {2376},
year = {2019}
}
@inproceedings{10.1145/3297280.3299745,
abstract = {Many problems in software systems can ultimately be traced to problematic system requirements. To mitigate this problem, some systems builders have adopted a more systematic approach to requirements elicitation and analysis. However, many of these approaches treat requirements as isolated entities having no direct connection to the actual system implementation. We propose VeriCCM, an approach that addresses potential problems both in the semantics and the syntax of requirements models. We also evaluate our approach with an empirical study.},
address = {New York, NY, USA},
author = {Gaither, Danielle and Madala, Kaushik and Do, Hyunsook and Bryant, Barrett R.},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3297280.3299745},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Gaither, Danielle and Madala, Kaushik and Do, Hyunsook and Bryant, Barrett R.{\_} {\_}{\_}VerICCM{\_} Improving the syntax and semantics of requirements models{\_}{\_} (2019).pdf:pdf},
isbn = {9781450359337},
keywords = {Machine learning,Requirements analysis,Validation,Verification,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1881--1884},
publisher = {Association for Computing Machinery},
series = {SAC '19},
title = {{VerICCM: Improving the syntax and semantics of requirements models}},
url = {https://doi.org/10.1145/3297280.3299745},
volume = {Part F1477},
year = {2019}
}
@inproceedings{8935711,
abstract = {Content less than two hundred words like comments or review statements is known as a short text. Short text classification is useful for automatically categorizing sentence into predefined group. There are several traditional short text classification methods by using bag-of-words with k nearest neighbors (k-NN), Na{\"{i}}ve Bayes, Maximum entropy, support vector machines (SVMs), and an algorithm based on statistics and rules. The deep learning method is outperformed other methods on classification of short text with normal size of dataset. Some researches classify requirements into functional and nonfunctional requirements. There is no research on multi-classification of functional requirements with a small dataset particularly for an accounting field. This paper presents an approach to classify short text for a small dataset into multiple categories of functional requirements on the accounting domain. The proposed approach uses an ontology to construct bag-of-words and uses Naive Bayes to classify for small dataset. The experiment is conducted using four hundred of datasets with 5folds and 10-folds cross-validation. The result shows that the method can correctly classify more than 80{\%}. Additionally, comparisons between the ontology-based Naive Bayes method and other methods are investigated.},
author = {Sangounpao, Ketkaew and Muenchaisri, Pornsiri},
booktitle = {Proceedings - 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2019},
doi = {10.1109/SNPD.2019.8935711},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sangounpao, Ketkaew and Muenchaisri, Pornsiri{\_} {\_}{\_}Ontology-Based Naive Bayes Short Text Classification Method for a Small Dataset{\_}{\_} (2019).pdf:pdf},
isbn = {9781728116518},
keywords = {accounting domain knowledge,ieee{\_}inc{\_}nlp{\_}x{\_}re,multi-classification,ontology,requirements engineering,short text classification,small dataset,traditional classification},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {53--58},
title = {{Ontology-Based Naive Bayes Short Text Classification Method for a Small Dataset}},
year = {2019}
}
@inproceedings{7320403,
abstract = {Requirements are subject to frequent changes as a way to ensure that they reflect the current best understanding of a system, and to respond to factors such as new and evolving needs. Changing one requirement in a requirements specification may warrant further changes to the specification, so that the overall correctness and consistency of the specification can be maintained. A manual analysis of how a change to one requirement impacts other requirements is time-consuming and presents a challenge for large requirements specifications. We propose an approach based on Natural Language Processing (NLP) for analyzing the impact of change in Natural Language (NL) requirements. Our focus on NL requirements is motivated by the prevalent use of these requirements, particularly in industry. Our approach automatically detects and takes into account the phrasal structure of requirements statements. We argue about the importance of capturing the conditions under which change should propagate to enable more accurate change impact analysis. We propose a quantitative measure for calculating how likely a requirements statement is to be impacted by a change under given conditions. We conduct an evaluation of our approach by applying it to 14 change scenarios from two industrial case studies.},
author = {Arora, Chetan and Sabetzadeh, Mehrdad and Goknil, Arda and Briand, Lionel C. and Zimmer, Frank},
booktitle = {2015 IEEE 23rd International Requirements Engineering Conference, RE 2015 - Proceedings},
doi = {10.1109/RE.2015.7320403},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Arora, Chetan and Sabetzadeh, Mehrdad and Goknil, Arda and Briand, Lionel C. and Zimmer, Frank{\_} {\_}{\_}Change impact analysis for Natural Language requirements{\_} An NLP approach{\_}{\_} (2015).pdf:pdf},
isbn = {9781467369053},
issn = {2332-6441},
keywords = {Change Impact Analysis,Natural Language Processing (NLP),Natural Language Requirements,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {6--15},
title = {{Change impact analysis for Natural Language requirements: An NLP approach}},
year = {2015}
}
@conference{Vogelsang2019,
abstract = {Software has become the driving force for innovations in any technical system that observes the environment with different sensors and influence it by controlling a number of actuators; nowadays called Cyber-Physical System (CPS). The development of such systems is inherently inter-disciplinary and often contains a number of independent subsystems. Due to this diversity, the majority of development information is expressed in natural language artifacts of all kinds. In this paper, we report on recent results that our group has developed to support engineers of CPSs in working with the large amount of information expressed in natural language. We cover the topics of automatic knowledge extraction, expert systems, and automatic requirements classification. Furthermore, we envision that natural language processing will be a key component to connect requirements with simulation models and to explain tool-based decisions. We see both areas as promising for supporting engineers of CPSs in the future.},
annote = {doesn't mention the techniques used},
author = {Vogelsang, Andreas and Hartig, Kerstin and Pudlitz, Florian and Schlutter, Aaron and Winkler, Jonas},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/vogelsang{\_}etal{\_}NLP4RE2019preprint.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Supporting the development of cyber-physical systems with natural language processing: A report}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068078920{\&}partnerID=40{\&}md5=183f6e5a429c910d882a191e6baf6955},
volume = {2376},
year = {2019}
}
@inproceedings{7330207,
abstract = {Privacy policies serve to inform consumers about a company's data practices, and to protect the company from legal risk due to undisclosed uses of consumer data. In addition, US and EU regulators require companies to accurately describe their practices in these policies, and some laws prescribe how companies should write these policies. Despite these aims, privacy policies are frequently criticized for being vague and uninformative. To support and improve the analysis of privacy policies, we report results from constructing an information type lexicon from manual, human annotations and an entity extractor based on part-of-speech tagging. The lexicon was constructed from 3,850 annotations obtained from crowd workers analyzing 15 privacy policies. An entity extractor was designed to extract entities from these annotations. The extractor succeeds at finding entities in 92{\%} of annotations and the lexicon consists of 725 unique entities. Finally, we measured the terminological reuse across all 15 policies and observed the lexicon has a 31-78{\%} chance of containing a word from any previously seen policy.},
author = {Bhatia, Jaspreet and Breaux, Travis D.},
booktitle = {8th International Workshop on Requirements Engineering and Law, RELAW 2015 - Proceedings},
doi = {10.1109/RELAW.2015.7330207},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bhatia, Jaspreet and Breaux, Travis D.{\_} {\_}{\_}Towards an information type lexicon for privacy policies{\_}{\_} (2015).pdf:pdf},
isbn = {9781509001040},
keywords = {crowdsourcing,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,privacy,requirements extraction,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {19--24},
title = {{Towards an information type lexicon for privacy policies}},
year = {2015}
}
@article{ISI:000480629700007,
abstract = {The paper is devoted to the development of an ontology-based intelligent agent (OBIA) for semantic parsing the natural language specifications of software requirements (SRS), which performs the parsing of the specification, determines the number and percentage of missing attributes. This allows to determine the non-functional characteristics-components of the software quality, reflect which attributes are missing for one or another sub-characteristics of the nonfunctional characteristic, and form a real ontology for the nonfunctional characteristics.},
address = {1505 SOFIA 39, MADRID BLVD, FLR 2, SOFIA, 00000, BULGARIA},
author = {Hovorushchenko, Tetiana and Boyarchuk, Artem and Pavlova, Olga},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/hovorushchenko.pdf:pdf},
issn = {1313-8251},
journal = {INTERNATIONAL JOURNAL ON INFORMATION TECHNOLOGIES AND SECURITY},
keywords = {Non-fun,Software requirements specification (SRS),wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {2},
pages = {59--70},
publisher = {UNION SCIENTISTS BULGARIA},
title = {{ONTOLOGY-BASED INTELLIGENT AGENT FOR SEMANTIC PARSING THE NATURAL LANGUAGE SPECIFICATIONS OF SOFTWARE REQUIREMENTS}},
type = {Article},
volume = {11},
year = {2019}
}
@inproceedings{6093363,
abstract = {Software requirements are typically captured in natural languages (NL) such as English and then analyzed by software engineers to generate a formal software design/model (such as UML model). However, English is syntactically ambiguous and semantically inconsistent. Hence, the English specifications of software requirements can not only result in erroneous and absurd software designs and implementations but the informal nature of English is also a main obstacle in machine processing of English specification of the software requirements. To address this key challenge, there is need to introduce a controlled NL representation for software requirements to generate accurate and consistent software models. In this paper, we report an automated approach to generate Semantic of Business Vocabulary and Rules (SBVR) standard based controlled representation of English software requirement specification. The SBVR based controlled representation can not only result in accurate and consistent software models but also machine process able because SBVR has pure mathematical foundation. We also introduce a java based implementation of the presented approach that is a proof of concept. {\textcopyright} 2011 IEEE.},
author = {Umber, Ashfa and Bajwa, Imran Sarwar},
booktitle = {2011 6th International Conference on Digital Information Management, ICDIM 2011},
doi = {10.1109/ICDIM.2011.6093363},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/umber Minimizing ambiguity in natural language software requirements specification.pdf:pdf},
isbn = {9781457715389},
keywords = {Natural Lanaague Processing,SBVR,Software Requirement Specifications,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {102--107},
title = {{Minimizing ambiguity in natural language software requirements specification}},
year = {2011}
}
@inproceedings{10.1145/3341105.3374050,
abstract = {Many times developers start with a high-level description of a use case which is then used in place of a more formal requirement. However, such a description fails to provide rich details that are typically part of a more structured use case. Having a varied level of details and degree of formalism among use cases, it is often difficult to comprehend and visualize functional dependencies among each of them in detail. Use Case Map (UCM) elaborates such dependencies in terms of relationships and responsibilities, and act as a bridge between specifications and design artifacts. However, there is no systematic transformation approach available for deriving UCMs from use case scenarios. In this work, we present a tool-support developed to automatically generate UCMs from the use case specification by identifying relationships, responsibilities, and related functional dependencies among them. Demonstration video of UC2Map - https://youtu.be/mnRwmVivbsM. Source code with other materials is available on Git - https://github.com/Rishabharora2773/BTP2019},
address = {New York, NY, USA},
author = {Tiwari, Saurabh and Arora, Rishab and Bharambe, Ashray},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3341105.3374050},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/tiwari2020.pdf:pdf},
isbn = {9781450368667},
keywords = {Requirement analysis,Systematic transformation approach,Tool support,Use case,Use case maps,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1650--1653},
publisher = {Association for Computing Machinery},
series = {SAC '20},
title = {{UC2Map: Automatic translation of use case maps from specification}},
url = {https://doi.org/10.1145/3341105.3374050},
year = {2020}
}
@inproceedings{10.1145/2593801.2593804,
abstract = {User requirement specification (URS) documents written in the form of free-form natural language text contain system use-case descriptions as one of the elements in the URS. For a few application domains, some of the system use-cases in SRS define services and functionality which needs to comply with law, rules and regulations pertaining to the application domain. In this paper, we present a multi-step approach to automatically extract system use-cases from URS and construct traceability links between system-uses and appropriate regulations in the regulatory documents. We define lexicon-based, syntactic and semantic features to discriminate system use-cases from other elements in the SRS. We investigate the application of five semantic similarity methods implemented in the SEMILAR semantic similarity toolkit to compute similarity between a given system usecase with regulations in a regulatory document. We conduct a series of experiments on real-world data obtained from software projects of a large global Information Technology (IT) services company to validate the proposed approach. Experimental results demonstrate effectiveness (accuracy of 83.3{\%} for system use-case extraction and 72{\%} for constructing traceability links) and limitations of the proposed approach.},
address = {New York, NY, USA},
author = {Jain, Ritika and Ghaisas, Smita and Sureka, Ashish},
booktitle = {3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2014 - Proceedings},
doi = {10.1145/2593801.2593804},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ritika SANAYOJAN A framework for traceability link recovery between use-cases in software requirement specification and regulatory documents.pdf:pdf},
isbn = {9781450328463},
keywords = {Mining software repositories,Natural language processing,Regulatory compliance,Requirements engineering,Text mining,Traceability link recovery,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {12--18},
publisher = {Association for Computing Machinery},
series = {RAISE 2014},
title = {{SANAYOJAN: A framework for traceability link recovery between use-cases in software requirement specification and regulatory documents}},
url = {https://doi.org/10.1145/2593801.2593804},
year = {2014}
}
@conference{Toews2018,
abstract = {In this report we describe the previous research done by our institute in the field of requirement analysis using different natural language processing methods. To represent the different degrees of similarity between words we implemented different methods that make use of synonyms and hyperonyms. We present the strengths of our methods and identify their weaknesses. For our future research we want to incorporate Word Embeddings as they solve most of the difficulties we faced with synonyms and hyperonyms. Copyright c 2018 by the paper's authors.},
annote = {cited By 0},
author = {Toews, D. and Heuss, T.},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/toews.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Research on NLP for RE at Fraunhofer FKIE: A report on grouping requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045427856{\&}partnerID=40{\&}md5=c2905f6ef88b341b2d238fca066a2cf2},
volume = {2376},
year = {2019}
}
@article{Ali20112718,
abstract = {Poor requirements analysis process results in incomplete software applications. Some requirements appear as scattered and tangled concerns within requirements document. Hence it is difficult to identify such requirements. A number of research approaches such as Theme/Doc, early aspects identification, information retrieval and aspects identification using UML have been developed to identify crosscutting concern at the requirements level. Nevertheless, these approaches are only supported by semiautomated tools whereby human intervention is required to achieve the desired results. This research focuses on developing a tool to automatically identify crosscutting concern at the requirements level. A model based on Theme/Doc and early aspects identification approaches is formulated as the basis of this tool, 3CI. 3CI adopts natural language processing (NLP) techniques such as verb frequency analysis, part-of-speech tagging and dominant verb analysis. The tool usability, efficiency and scalability are evaluated by comparing the performance of a requirements engineer conducting similar task manually. Our evaluation on 3CI demonstrates 75{\%} of accuracy. {\textcopyright} 2011 Academic Journals.},
annote = {cited By 7},
author = {Ali, Busyairah Syd and Kasirun, Zarinah M.D.},
doi = {10.5897/IJPS11.015},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/ali An approach for crosscutting concern identification at requirements level using NLP.pdf:pdf},
issn = {19921950},
journal = {International Journal of Physical Sciences},
keywords = {3CL,Aspects-oriented requirements engineering,Crosscutting concern,Dominant verb analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {11},
pages = {2718--2730},
title = {{An approach for crosscutting concern identification at requirements level using NLP}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960075256{\&}partnerID=40{\&}md5=4885b16bc9ce2d944b7789a75b51dfb1},
volume = {6},
year = {2011}
}
@inproceedings{10.1145/3385032.3385039,
abstract = {Specialized terms used in the requirements document should be defined in a glossary.We propose a technique for automated extraction and clustering of glossary terms from large-sized requirements documents.We use text chunking combined withWordNet removal to extract candidate glossary terms. Next, we apply a state-of-the art neural word embeddings model for clustering glossary terms based on semantic similarity measures. Word embeddings are capable of capturing the context of a word and compute its semantic similarity relation with other words used in a document. Its use for clustering ensures that terms that are used in similar ways belong to the same cluster.We apply our technique to the CrowdRE dataset, which is a large-sized dataset with around 3000 crowd-generated requirements for smart home applications. To measure the effectiveness of our extraction and clustering technique we manually extract and cluster the glossary terms from CrowdRE dataset and use it for computing precision, recall and coverage. Results indicate that our approach can be very useful for extracting and clustering of glossary terms from a large body of requirements.},
address = {New York, NY, USA},
author = {Bhatia, Kushagra and Mishra, Siba and Sharma, Arpit},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3385032.3385039},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bhatia, Kushagra and Mishra, Siba and Sharma, Arpit{\_} {\_}{\_}Clustering glossary terms extracted from large-sized software requirements using fasttext{\_}{\_} (2020).pdf:pdf},
isbn = {9781450375948},
keywords = {Clustering,Fasttext.,Glossary,Natural language processing,Requirements engineering,Word embeddings,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ISEC 2020},
title = {{Clustering glossary terms extracted from large-sized software requirements using fasttext}},
url = {https://doi.org/10.1145/3385032.3385039},
year = {2020}
}
@inproceedings{5460024,
abstract = {Scenarios are critical for requirement analysis and system design. In this paper, we present a lightweight framework for scenario elicitation from natural language requirements. First, the events are elicited from sentences using event templates. Then the elicited events are associated with event tree to constitute scenarios. {\textcopyright} 2010 IEEE.},
author = {Liu, Xiaoli},
booktitle = {2nd International Workshop on Education Technology and Computer Science, ETCS 2010},
doi = {10.1109/ETCS.2010.137},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/xiaoli Scenario elicitation from natural language requirements.pdf:pdf},
isbn = {9780769539874},
keywords = {Component,Event template,NLR analysis,Scenario elicitation,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {252--255},
title = {{Scenario elicitation from natural language requirements}},
volume = {2},
year = {2010}
}
@inproceedings{10.1145/2642937.2642958,
abstract = {Many software systems utilize forums to allow a broad set of stakeholders to request features. However the resulting mass of ideas and comments can make prioritization and management of feature requests challenging. In this paper we propose a novel approach for partially automating the creation of personas from a set of feature requests in open forums. Our approach utilizes topic clustering, classification, and association rules to identify meaningful groupings of feature requests and then uses them to guide the construction of personas. Once created, these personas are leveraged to coordinate feature requests, track changes, and to provide stakeholder communication mechanisms. We illustrate our approach with examples taken from the health-insurance domain and then evaluate it against feature requests in the SugarCRM project.},
address = {New York, NY, USA},
author = {Rahimi, Mona and Cleland-Huang, Jane},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
doi = {10.1145/2642937.2642958},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rahimi, Mona and Cleland-Huang, Jane{\_} {\_}{\_}Personas in the Middle{\_} Automated Support for Creating Personas as Focal Points in Feature Gathering Forums{\_}{\_} (2014).pdf:pdf},
isbn = {9781450330138},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,persona,requirements management,text mining},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {479--484},
publisher = {Association for Computing Machinery},
series = {ASE '14},
title = {{Personas in the Middle: Automated Support for Creating Personas as Focal Points in Feature Gathering Forums}},
url = {https://doi.org/10.1145/2642937.2642958},
year = {2014}
}
@inproceedings{8920535,
abstract = {In order to sustain, software systems have to evolve in favor of its main target users. Due to the pervasive adoption of online user forums and social media, collecting users feedbacks and comments become possible. However, such crowd generated data are often fragmented, with various viewpoints mentioned during a series of message exchange. The aim of the thesis is to propose an argumentation-based CrowdRE approach, which represents such group conversations as a user argumentation model with the original conversation structure reserved. Based on the argumentation model, we are able to identify new features proposed by the crowd-users or issues encountered, and their supporting and attacking arguments using argumentation theory. To accomplish this research, we adopted an abstract argumentation, bipolar argumentation framework, and coalition-based meta argumentation framework. In addition, to provided automated support to our proposed approach, algorithms will be developed for bipolar argumentation, coalition-based meta argumentation, and end-users voting mechanism. Finally, this thesis employees different machine learning algorithms to automatically classify crowd-users comments into rationale elements and identify conflict-free features or claims based on their supporting and attacking arguments. Initial results show that the proposed approach can identify features, issues and their supporting and attacking arguments with acceptable performance.},
author = {Khan, Javed Ali},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00059},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Khan, Javed Ali{\_} {\_}{\_}Mining requirements arguments from user forums{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Argumentation,Machine learning,Natural language processing,Requirements,User forum,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {440--445},
title = {{Mining requirements arguments from user forums}},
volume = {2019-Septe},
year = {2019}
}
@article{McZara20151721,
abstract = {Implementing the entire set of requirements for a software system is often not feasible owing to time and resource limitations. A key driver for successful delivery of any software system is the ability to prioritize the large number of requirements. Prioritization of requirements is a key challenge because current methods are not scalable to handle a realistic number of requirements. Current methods for requirements prioritization in market-driven software development projects are neither sufficient nor proven. A prioritization technique that is more time-efficient, accurate, and easier to implement for large-scale projects than current practices is needed. We address these challenges with a prioritization method that incorporates the use of a linguistic tool and constraint solver. In this paper we propose a method, referred to as SNIPR, for requirements prioritization and selection based on natural language processing and satisfiability modulo theories solvers. We present a controlled experiment in which 40 systems engineers prioritized and selected 20 requirements from a list of 100 using SNIPR and the weighted sum model. Results show that the SNIPR method consumes less time, improves selection accuracy, and is easier to perform than the weighted sum model. These results motivate further research using linguistic tools and constraint solvers for the prioritization of large sets of requirements.},
annote = {cited By 18},
author = {McZara, Jason and Sarkani, Shahryar and Holzer, Thomas and Eveleigh, Timothy},
doi = {10.1007/s10664-014-9334-8},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/MCZARA{\~{}}1.PDF:PDF},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Controlled experiment,NLP,Release planning,Requirements engineering,Requirements prioritization,SMT solver,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {6},
pages = {1721--1761},
title = {{Software requirements prioritization and selection using linguistic tools and constraint solvers—a controlled experiment}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944327311{\&}doi=10.1007{\%}2Fs10664-014-9334-8{\&}partnerID=40{\&}md5=eb35d4fc5bb15579b8f59110f5afc337},
volume = {20},
year = {2015}
}
@article{Müter2019109,
abstract = {[Context and motivation] In agile system development methods, product backlog items (or tasks) play a prominent role in the refinement process of software requirements. Tasks are typically defined manually to operationalize how to implement a user story; tasks formulation often exhibits low quality, perhaps due to the tedious nature of decomposing user stories into tasks. [Question/Problem] We investigate the process through which user stories are refined into tasks. [Principal ideas/results] We study a large collection of backlog items (N = 1,593), expressed as user stories and sprint tasks, looking for linguistic patterns that characterize the required feature of the user story requirement. Through a linguistic analysis of sentence structures and action verbs (the main verb in the sentence that indicates the task), we discover patterns of labeling refinements, and explore new ways for refinement process improvement. [Contribution] By identifying a set of 7 elementary action verbs and a template for task labels, we make first steps towards comprehending the refinement of user stories to backlog items.},
annote = {cited By 0},
author = {M{\"{u}}ter, Laurens and Deoskar, Tejaswini and Mathijssen, Max and Brinkkemper, Sjaak and Dalpiaz, Fabiano},
doi = {10.1007/978-3-030-15538-4_7},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/muter Refinement of User Stories into Backlog Items Linguistic Structure and Action Verbs Research Preview.pdf:pdf},
isbn = {9783030155377},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Backlog items,Natural language processing,Requirements engineering,Sprint tasks,User stories,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {109--116},
title = {{Refinement of User Stories into Backlog Items: Linguistic Structure and Action Verbs: Research Preview}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064038669{\&}doi=10.1007{\%}2F978-3-030-15538-4{\_}7{\&}partnerID=40{\&}md5=b9535284a83a58546b16ab8ae5e35adc},
volume = {11412 LNCS},
year = {2019}
}
@conference{Santos2019,
abstract = {Online user feedback contains information that is of interest to requirements engineering (RE). Natural language processing (NLP) techniques, especially classification algorithms, are a popular way of automatically classifying requirements-relevant contents. Research into this use of NLP in RE has sought to answer different research questions, often causing their classifications to be incompatible. Identifying and structuring these classifications is therefore urgently needed. We present a preliminary taxonomy that we constructed based on the findings from a systematic literature review, which places 78 classifications categories for user feedback into four groups: Sentiment, Intention, User Experience, and Topic. The taxonomy reveals the purposes for which user feedback is analyzed in RE, provides an initial harmonization of the vocabulary in this research area, and may inspire researchers to investigate classifications they had previously not considered. This paper intends to foster discussions among NLP experts and to identify further improvements to the taxonomy.},
annote = {cited By 0},
author = {Santos, Rubens and Groen, Eduard C. and Villela, Karina},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/santos A taxonomy for user feedback classifications.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{A taxonomy for user feedback classifications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068029496{\&}partnerID=40{\&}md5=55258d733d12086108a5bb082cd3083e},
volume = {2376},
year = {2019}
}
@conference{Park201011,
abstract = {When a system is developed, Requirements Document is generated by requirement analysts and then translated to formal specifications by specifiers. If a formal specification can be generated automatically from Natural Language Requirements Document, system development cost and system fault from experts' misunderstanding will be decreased. Also, if we would like to get the higher accuracy in analysis of Requirements Document automatically, antecedent decision of pronoun is very important for elicitation of formal requirements (i.e. component, action, statement and parameters etc.) automatically from Natural Language Requirements Document via Natural Language Processing. Pronoun can be classified in personal and demonstrative pronoun. In the Requirements Document, the personal pronoun is almost not occurred so we focused on antecedent decision for a demonstrative pronoun. The final goal of this research is to generate automatically formal specifications from natural language requirements document using natural language processing to develop systems. For this, This paper, based on previous research [I], proposes the Anaphora Resolution System to decide antecedent of pronoun using Natural Language Processing from Natural Language Requirements Document in Korean. {\textcopyright} 2010 IEEE.},
annote = {cited By 2},
author = {Park, Ki Seon and An, Dong Un and Lee, Yong Seok},
booktitle = {ICIC 2010 - 3rd International Conference on Information and Computing},
doi = {10.1109/ICIC.2010.9},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/park Anaphora resolution system for natural language requirements document in Korean.pdf:pdf},
isbn = {9780769540474},
keywords = {Anaphora resolution,Antecedent decision,Natural language processing,Requirements elicitation,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {11--14},
title = {{Anaphora resolution system for natural language requirements document in Korean}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955835345{\&}doi=10.1109{\%}2FICIC.2010.9{\&}partnerID=40{\&}md5=e13dd32f076a37219889baff7f7241e6},
volume = {1},
year = {2010}
}
@inproceedings{7815636,
abstract = {Service requirements documentation plays a crucial role on the quality of service-oriented systems to be developed. A large amount of service requirements are documented in the form of natural language, which are usually human-centric and therefore error-prone and inaccurate. In order to improve the quality of service requirements documents, we propose a service requirements modeling and validation method using workflow patterns. First, it extracts the process information using natural language processing tools. Then it formalizes the process information with a requirements modeling language - Workflow-Patterns-based Process Language (WPPL). Finally, the defects existed in service requirements are checked against a set of checking rules by matching with workflow patterns. A financial service example - Trade Order - was used to illustrate our approach.},
author = {Wang, Ye and Jiang, Bo and Wang, Ting},
booktitle = {2016 IEEE 24th International Requirements Engineering Conference Workshops (REW)},
doi = {10.1109/rew.2016.053},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Wang, Ye and Jiang, Bo and Wang, Ting{\_} {\_}{\_}Using Workflow Patterns to Model and Validate Service Requirements{\_}{\_} (2017).pdf:pdf},
keywords = {formal verification,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,se},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {281--288},
title = {{Using Workflow Patterns to Model and Validate Service Requirements}},
year = {2017}
}
@inproceedings{7489864,
abstract = {Domain analysis is important to core assets development in software product line (SPL) engineering, and in software-as-a-service (SaaS) engineering. Traditional methods of domain analysis, however, are heavily based on manual labor. They depend on domain experts' experience to analyze the commonality and variability of systems in a domain, which appears to be an obstacle for many organizations which tend to provide SaaS service or to launch a software product line (SPL). In this paper, we propose a new approach to automatically extract the semantic information from software requirements specifications (SRSs). We address this issue by combining techniques of semantic role labeling and domain knowledge modeling. In this approach, we selected frequent verbs from software requirement specification documents in the e-commerce domain, and built the semantic frames for those verbs. Then the selected sentences were labeled manually and the result was used as training examples for machine learning. To obtain the accurate features of the examples we correct the parsing result of Stanford Parser with the help of domain knowledge. During the labeling process, we adopt a sequential way in which the previous labeled results will be used to construct dynamic features for the identification of the subsequent semantic roles. The proposed approach was implemented and evaluated. The preliminary result shows the approach is effective and reliable. The extracted semantic information can be used to model the variability and commonality of functional requirements in the domain engineering.},
author = {Wang, Yinglin},
booktitle = {Proceedings of 2015 IEEE International Conference on Progress in Informatics and Computing, PIC 2015},
doi = {10.1109/PIC.2015.7489864},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Wang, Yinglin{\_} {\_}{\_}Semantic information extraction for software requirements using semantic role labeling{\_}{\_} (2016).pdf:pdf},
isbn = {9781467380867},
keywords = {Software requirement engineering,automatic analysis,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirement extraction,semantic analysis},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {332--337},
title = {{Semantic information extraction for software requirements using semantic role labeling}},
year = {2016}
}
@inproceedings{8054879,
abstract = {In this paper we apply self-labeling algorithms as Semi-Supervised Classification (SSC) techniques in order to automate the classification of functional and non-functional requirements contained in reviews in the App Store. In this domain, where it is easy collect a large number of reviews but difficult to manually annotate then, we found that SSC techniques can successfully perform this task and that only a small amount of data is needed to achieve results similar to classical supervised techniques. We also found that the models learned can properly assign labels to the collected data and can classify unseen future reviews. We believe SSC techniques can be of particular use during requirements classification.},
author = {Deocadez, Roger and Harrison, Rachel and Rodriguez, Daniel},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.58},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Deocadez, Roger and Harrison, Rachel and Rodriguez, Daniel{\_} {\_}{\_}Automatically classifying requirements from app stores{\_} A preliminary study{\_}{\_} (2017).pdf:pdf},
isbn = {9781538634882},
keywords = {Apps reviews,Mobile apps,Self-labeling algorithms,Semi-supervised learning,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {367--371},
title = {{Automatically classifying requirements from app stores: A preliminary study}},
year = {2017}
}
@article{Umber201130,
abstract = {This paper presents a novel approach to automate the process of software requirements elicitation and specification. The software requirements elicitation is perhaps the most important phase of software development as a small error at this stage can result in absurd software designs and implementations. The automation of the initial phase (such as requirement elicitation) phase can also contribute to a long standing challenge of automated software development. The presented approach is based on Semantic of Business Vocabulary and Rules (SBVR), an OMG's recent standard. We have also developed a prototype tool SR-Elicitor (an Eclipse plugin), which can be used by software engineers to record and automatically transform the natural language software requirements to SBVR software requirements specification. The major contribution of the presented research is to demonstrate the potential of SBVR based approach, implemented in a prototype tool, proposed to improve the process of requirements elicitation and specification. {\textcopyright} 2011 Springer-Verlag.},
annote = {cited By 11},
author = {Umber, Ashfa and Bajwa, Imran Sarwar and {Asif Naeem}, M.},
doi = {10.1007/978-3-642-22714-1_4},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/LGGFEBZQ/Umber e.a. - 2011 - NL-Based Automated Software Requirements Elicitati.pdf:pdf},
isbn = {9783642227134},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Natural Language Processing,Requirement Engineering,Requirements Elicitation,Requirements Specification,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {PART 2},
pages = {30--39},
title = {{NL-based automated software requirements elicitation and specification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051553451{\&}doi=10.1007{\%}2F978-3-642-22714-1{\_}4{\&}partnerID=40{\&}md5=a2e7edbc5b995e57fba15153d74a806a},
volume = {191 CCIS},
year = {2011}
}
@conference{Dalpiaz2019,
abstract = {[Team Overview] The Requirements Engineering Lab at Utrecht University conducts research on techniques and software tools that help people express better requirements in order to ultimately deliver better software products. [Past Research] We have focused on natural language processing-powered tools that analyze user stories to identify defects, to extract conceptual models that deliver an overview of the used concepts, and to pinpoint terminological ambiguity. Also, we studied how reviews for competing products can be analyzed via natural language processing (NLP) to identify new requirements. [Research Plan] The gained knowledge from our experience with NLP in requirements engineering (RE) triggers new research lines concerning the synergy between humans and NLP, including the use of intelligent chatbots to elicit requirements, the automated synthesis of creative requirements, and the maintenance of traceability via linguistic tooling.},
annote = {cited By 0},
author = {Dalpiaz, Fabiano and Brinkkemper, Sjaak},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/dalpiaz Research on NLP for RE at Utrecht University A report.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Research on NLP for RE at Utrecht University: A report}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068047828{\&}partnerID=40{\&}md5=aa328c67adc927bee51fda871cb518e2},
volume = {2376},
year = {2019}
}
@conference{Liu201983,
abstract = {Requirements analysis is the first point of information system development, which has a significant impact on the development. For the requirements of natural language description, automated requirement checking model cannot feasible. To verify the consistency of information system requirements, the paper builds a semantic model with tree nodes of natural language clauses. The model divides clauses into a representation of keywords set with seven-tuple. The paper not only proposed a dependency tree model to solve the problem that the refined tree cannot characterize the relationship between syntactic structure and keywords, but also put forward a dependency tagging algorithm and an algorithm to construct and update dependency parsing tree. The paper further put forward a semantic similarity calculation method to determine similarity among sub clause syntactic structures.},
annote = {cited By 0},
author = {Liu, Gang and Wang, Kai and Liu, Wangyang and Cao, Yang},
booktitle = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
doi = {10.1109/ICSESS47205.2019.9040816},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Liu, Gang and Wang, Kai and Liu, Wangyang and Cao, Yang{\_} {\_}{\_}The construction and measure method of dependency parsing tree model{\_}{\_} (2019).pdf:pdf},
isbn = {9781728109459},
issn = {23270594},
keywords = {Component,Dependency tree model,Dependent annotation algorithms,Information system,Requirement analysis,Semantic model,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {83--86},
title = {{The construction and measure method of dependency parsing tree model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082849681{\&}doi=10.1109{\%}2FICSESS47205.2019.9040816{\&}partnerID=40{\&}md5=58824829deef6d7fcdcd959a5f998cfb},
volume = {2019-Octob},
year = {2019}
}
@conference{Borrull2018,
abstract = {[Team Overview] The Software and Service Engineering Group (GESSI) of UPC has traditionally conducted research in many fields of software engineering. [Research Plan on NLP] As a result of our participation in the OpenReq project, natural language processing (NLP) has become one of our highest priority research fields. We are using NLP for interdependency detection and requirements reuse, being the center piece of both tasks the identification of similar requirements. Copyright c 2018 by the paper's authors.},
annote = {cited By 0},
author = {Borrull, R. and Costal, D. and Franch, X. and Quer, C.},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/borrul Research on NLP for RE at UPC A report.pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Research on NLP for RE at UPC: A report}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045427121{\&}partnerID=40{\&}md5=18caa0cf208e48dc0d6192047057f851},
volume = {2376},
year = {2019}
}
@article{li_enabling_2019,
abstract = {A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95{\%} on average and identify variabilities in textual requirements with high precision (over 97{\%} on average) and recall (over 94{\%} on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.},
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
doi = {10.1007/s10270-017-0641-6},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li{\_} {\_}{\_}Enabling automated requirements reuse and configuration{\_}{\_} (2019).pdf:pdf},
issn = {16191374},
journal = {Software and Systems Modeling},
keywords = {Configuration,Feature model,Product line,Requirements,Reuse,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {2177--2211},
title = {{Enabling automated requirements reuse and configuration}},
url = {http://link.springer.com/10.1007/s10270-017-0641-6},
volume = {18},
year = {2019}
}
@inproceedings{8491162,
abstract = {Software requirements are typically written in natural language, which need to be transformed into a more formal representation. Natural language processing techniques have been applied to aid in this transformation. Semantic parsing, for instance, adds semantic structure to text. It however requires supporting corpora which are still missing in requirements engineering. To address this gap, we developed FN-RE, a corpus of requirements documents, which was annotated based on semantic frames in FrameNet. Each requirement statement was manually labelled by two annotators by selecting suitable semantic frames and related frame elements. We obtained an average agreement of 72.85{\%} between the two annotators, measured by F-score, thus indicating that the annotations provided in our corpus are reliable.},
author = {Alhoshan, Waad and Batista-Navarro, Riza and Zhao, Liping},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00055},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Alhoshan, Waad and Batista-Navarro, Riza and Zhao, Liping{\_} {\_}{\_}Towards a corpus of requirements documents enriched with semantic frame annotations{\_}{\_} (2018).pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {FrameNet,Requirements corpus,Requirements documents,Semantic annotation,Semantic frames,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {428--431},
title = {{Towards a corpus of requirements documents enriched with semantic frame annotations}},
year = {2018}
}
@article{rago_assisting_2016,
abstract = {Textual requirements are very common in software projects. However, this format of requirements often keeps relevant concerns (e.g., performance, synchronization, data access, etc.) from the analyst's view because their semantics are implicit in the text. Thus, analysts must carefully review requirements documents in order to identify key concerns and their effects. Concern mining tools based on NLP techniques can help in this activity. Nonetheless, existing tools cannot always detect all the crosscutting effects of a given concern on different requirements sections, as this detection requires a semantic analysis of the text. In this work, we describe an automated tool called REAssistant that supports the extraction of semantic information from textual use cases in order to reveal latent crosscutting concerns. To enable the analysis of use cases, we apply a tandem of advanced NLP techniques (e.g, dependency parsing, semantic role labeling, and domain actions) built on the UIMA framework, which generates different annotations for the use cases. Then, REAssistant allows analysts to query these annotations via concern-specific rules in order to identify all the effects of a given concern. The REAssistant tool has been evaluated with several case-studies, showing good results when compared to a manual identification of concerns and a third-party tool. In particular, the tool achieved a remarkable recall regarding the detection of crosscutting concern effects.},
author = {Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andres},
doi = {10.1007/s10515-014-0156-0},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rago, Alejandro and Marcos, Claudia and Diaz-Pace, J. Andres{\_} {\_}{\_}Assisting requirements analysts to find latent concerns with REAssistant{\_}{\_} (2016).pdf:pdf},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Concern mining,Natural language processing,Semantic analysis,Tool support,Use case specifications,User assistance,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {2},
pages = {219--252},
title = {{Assisting requirements analysts to find latent concerns with REAssistant}},
url = {http://link.springer.com/10.1007/s10515-014-0156-0},
volume = {23},
year = {2016}
}
@article{Al-Hroob20181,
abstract = {Context: The automatic extraction of actors and actions (i.e., use cases) of a system from natural language-based requirement descriptions, is considered a common problem in requirements analysis. Numerous techniques have been used to resolve this problem. Examples include rule-based (e.g., inference), keywords, query (e.g., bi-grams), library maintenance, semantic business vocabularies, and rules. The question remains: can combination of natural language processing (NLP) and artificial neural networks (ANNs) perform this job successfully and effectively? Objective: This paper proposes a new approach to automatically identify actors and actions in a natural language-based requirements' description of a system. Included are descriptions of how NLP plays an important role in extracting actors and actions, and how ANNs can be used to provide definitive identification. Method: We used an NLP parser with a general architecture for text engineering, producing lexicons, syntaxes, and semantic analyses. An ANN was developed using five different use cases, producing different results due to their complexity and linguistic formation. Results: Binomial classification accuracy techniques were used to evaluate the effectiveness of this approach. Based on the five use cases, the results were 17–63{\%} for precision, 5–6100{\%} for recall, and 29–71{\%} for F-measure. Conclusion: We successfully used a combination of NLP and ANN artificial intelligence techniques to reveal specific domain semantics found in a software requirements specification. An Intelligent Technique for Requirements Engineering (IT4RE) was developed to provide a semi-automated approach, classified as Intelligent Computer Aided Software Engineering (I-CASE).},
annote = {cited By 0},
author = {Al-Hroob, Aysh and Imam, Ayad Tareq and Al-Heisa, Rawan},
doi = {10.1016/j.infsof.2018.04.010},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Al-Hroob, Aysh and Imam, Ayad Tareq and Al-Heisa, Rawan{\_} {\_}{\_}The use of artificial neural networks for extracting actions and actors from requirements document{\_}{\_} (2018).pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {ANN,GATE,I-CASE,MATLAB,NLP,Software requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--15},
title = {{The use of artificial neural networks for extracting actions and actors from requirements document}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047095670{\&}doi=10.1016{\%}2Fj.infsof.2018.04.010{\&}partnerID=40{\&}md5=87be8f2ae61255f05bc9b7b7edb2d240},
volume = {101},
year = {2018}
}
@inproceedings{5507386,
abstract = {Specific software requirements can be organized with different ways, such as user class oriented organization, functional hierarchy oriented organization, stimulus oriented organization, and so on. User class oriented software requirements specification is easy to understand for each class of users' behaviors, but difficult to understand functional hierarchy. We adopt a controlled requirements language named X-JRDL as a requirements language. We can analyze requirements specifications written with X-JRDL using natural language processing techniques. In this paper, we propose a transformation method between two software requirements specifications organized in different ways. {\textcopyright} 2010 IEEE.},
author = {Matsuo, Yusuke and Ogasawara, Kiyoshi and Ohnishi, Atsushi},
booktitle = {2010 4th International Conference on Research Challenges in Information Science - Proceedings, RCIS 2010},
doi = {10.1109/rcis.2010.5507386},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Matsuo, Yusuke and Ogasawara, Kiyoshi and Ohnishi, Atsushi{\_} {\_}{\_}Automatic transformation of organization of software requirements specifications{\_}{\_} (2010).pdf:pdf},
isbn = {9781424448401},
issn = {2151-1357},
keywords = {Organization of software requirements specificatio,Software requirements specification,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {269--278},
title = {{Automatic transformation of organization of software requirements specifications}},
year = {2010}
}
@inproceedings{10.1145/2723742.2723745,
abstract = {Automated machine analysis of natural language requirements poses several challenges. Complex requirements such as functional requirements and use cases are hard to parse and analyze, the language itself is un-constrained, the flow of requirements may be haphazard, and one requirement may contradict another - to name a few challenges. In this paper, we present a lightweight semantic modeling technique through natural language processing to filter requirements and create a semi-formal semantic network of requirement sentences. We employ novel techniques of classification of verbs used in requirements, semantic role labeling, discourse identification, and a few verb entailment and dependency relationships to generate a lightweight semantic network and critique the requirements. We discuss the design of the model and some early results obtained from analyzing real-life industrial requirements.},
address = {New York, NY, USA},
author = {Sengupta, Shubhashis and Ramnani, Roshni R. and Das, Subhabrata and Chandran, Anitha},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2723742.2723745},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sengupta, Shubhashis and Ramnani, Roshni R. and Das, Subhabrata and Chandran, Anitha{\_} {\_}{\_}Verb-based semantic modelling and analysis of textual requirements{\_}{\_} (2015).pdf:pdf},
isbn = {9781450334327},
keywords = {Requirement Engineering,Requirement Quality Analysis,Rule based Natural Language Processing,Semantic Modelling,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {30--39},
publisher = {Association for Computing Machinery},
series = {ISEC '15},
title = {{Verb-based semantic modelling and analysis of textual requirements}},
url = {https://doi.org/10.1145/2723742.2723745},
volume = {18-20-Febr},
year = {2015}
}
@incollection{maalej_using_2013,
abstract = {Requirements engineering is one of the most complex and at the same time most crucial aspects of software engineering. It typically involves different stakeholders with different backgrounds. Constant changes in both the problem and the solution domain make the work of the stakeholders extremely dynamic. New problems are discovered, additional information is needed, alternative solutions are proposed, several options are evaluated, and new hands-on experience is gained on a daily basis. The knowledge needed to define and implement requirements is immense, often interdisciplinary and constantly expanding. It typically includes engineering, management and collaboration information, as well as psychological aspects and best practices. This book discusses systematic means for managing requirements knowledge and its owners as valuable assets. It focuses on potentials and benefits of “lightweight,” modern knowledge technologies such as semantic Wikis, machine learning, and recommender systems applied to requirements engineering. The 17 chapters are authored by some of the most renowned researchers in the field, distilling the discussions held over the last five years at the MARK workshop series. They present novel ideas, emerging methodologies, frameworks, tools and key industrial experience in capturing, representing, sharing, and reusing knowledge in requirements engineering. While the book primarily addresses researchers and graduate students, practitioners will also benefit from the reports and approaches presented in this comprehensive work.},
address = {Berlin, Heidelberg},
author = {Daramola, O. and St{\aa}lhane, T. and Omoronyia, I. and Sindre, G.},
booktitle = {Managing Requirements Knowledge},
doi = {10.1007/978-3-642-34419-0_6},
editor = {Maalej, Walid and Thurimella, Anil Kumar},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/daramola2013.pdf:pdf},
isbn = {978-3-642-34418-3 978-3-642-34419-0},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {117--141},
publisher = {Springer Berlin Heidelberg},
title = {{Using Ontologies and Machine Learning for Hazard Identification and Safety Analysis}},
url = {http://link.springer.com/10.1007/978-3-642-34419-0{\_}6},
year = {2013}
}
@inproceedings{8337942,
abstract = {Software Requirements are the basis of high-quality software development process, each step is related to SR, these represent the needs and expectations of the software in a very detailed form. The software requirement classification (SRC) task requires a lot of human effort, specially when there are huge of requirements, therefore, the automation of SRC have been addressed using Natural Language Processing (NLP) and Information Retrieval (IR) techniques, however, generally requires human effort to analyze and create features from corpus (set of requirements). In this work, we propose to use Deep Learning (DL) to classify software requirements without labor intensive feature engineering. The model that we propose is based on Convolutional Neural Network (CNN) that has been state of art in other natural language related tasks. To evaluate our proposed model, PROMISE corpus was used, contains a set of labeled requirements in functional and 11 different categories of non-functional requirements. We achieve promising results on SRC using CNN even without handcrafted features.},
author = {Navarro-Almanza, Raul and Juurez-Ramirez, Reyes and Licea, Guillermo},
booktitle = {Proceedings - 2017 5th International Conference in Software Engineering Research and Innovation, CONISOFT 2017},
doi = {10.1109/CONISOFT.2017.00021},
isbn = {9781538639566},
keywords = {Convolutional Neural Network,Software Engineering,Software Requirement Classification,Word Embedding,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {116--120},
title = {{Towards Supporting Software Engineering Using Deep Learning: A Case of Software Requirements Classification}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{10.1145/3328833.3328837,
abstract = {Now a day‟s main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.},
address = {New York, NY, USA},
author = {Taj, Soonh and Arain, Qasim and Memon, Imran and Zubedi, Asma},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3328833.3328837},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Taj, Soonh and Arain, Qasim and Memon, Imran and Zubedi, Asma{\_} {\_}{\_}To apply data mining for classification of crowd sourced software requirements{\_}{\_} (2019).pdf:pdf},
isbn = {9781450361057},
keywords = {Crowdsourcing,Data mining,Functional Requirements,NonFunctional Requirements,Requirement classification,Requirement elicitation,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {42--46},
publisher = {Association for Computing Machinery},
series = {ICSIE '19},
title = {{To apply data mining for classification of crowd sourced software requirements}},
url = {https://doi.org/10.1145/3328833.3328837},
year = {2019}
}
@inproceedings{8054858,
abstract = {Natural language-based use cases remains the main means of requirements elicitation and specification, despite the well-known problems that accompany natural language specifications, namely their incompleteness, inconsistency and ambiguity. This paper presents a novel approach for tackling the textual description problems by using FrameNet frames, which are linguistics patterns for concept description. The proposed approach is demonstrated via an explorative example. We show that use of frames in use case-based elicitation has a promise to lead to clearer understanding of the domain concepts covered in a use-case and to help acquire more complete information.},
author = {Kundi, Mahwish and Chitchyan, Ruzanna},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.53},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kundi, Mahwish and Chitchyan, Ruzanna{\_} {\_}{\_}Use case elicitation with FrameNet frames{\_}{\_} (2017).pdf:pdf},
isbn = {9781538634882},
keywords = {Completeness,Ontology-based use case elicitation,Quality,Use case specification,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {224--231},
title = {{Use case elicitation with FrameNet frames}},
year = {2017}
}
@article{mahmoud_detecting_2016,
abstract = {In this paper, we describe a novel unsupervised approach for detecting, classifying, and tracing non-functional software requirements (NFRs). The proposed approach exploits the textual semantics of software functional requirements (FRs) to infer potential quality constraints enforced in the system. In particular, we conduct a systematic analysis of a series of word similarity methods and clustering techniques to generate semantically cohesive clusters of FR words. These clusters are classified into various categories of NFRs based on their semantic similarity to basic NFR labels. Discovered NFRs are then traced to their implementation in the solution space based on their textual semantic similarity to source code artifacts. Three software systems are used to conduct the experimental analysis in this paper. The results show that methods that exploit massive sources of textual human knowledge are more accurate in capturing and modeling the notion of similarity between FR words in a software system. Results also show that hierarchical clustering algorithms are more capable of generating thematic word clusters than partitioning clustering techniques. In terms of performance, our analysis indicates that the proposed approach can discover, classify, and trace NFRs with accuracy levels that can be adequate for practical applications.},
author = {Mahmoud, Anas and Williams, Grant},
doi = {10.1007/s00766-016-0252-8},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/anas Detecting, classifying, and tracing non-functional software requirements.pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Classification,Information retrieval,Non-functional requirements,Semantics,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {sep},
number = {3},
pages = {357--381},
title = {{Detecting, classifying, and tracing non-functional software requirements}},
url = {http://link.springer.com/10.1007/s00766-016-0252-8},
volume = {21},
year = {2016}
}
@article{belsis_pburc_2014,
abstract = {Agile software development methodologies are increasingly adopted by organizations because they focus on the client's needs, thus safeguarding business value for the final product. At the same time, as the economy and society move toward globalization, more organizations shift to distributed development of software projects. From this perspective, while adopting agile techniques seems beneficial, there are still a number of challenges that need to be addressed; among these notable is the effective cooperation between the stakeholders and the geographically distributed development team. In addition, data collection and validation for requirements engineering demands efficient processing techniques in order to handle the volume of data as well as to manage different inconsistencies, when the data are collected using online tools. In this paper, we present "PBURC," a patterns-based, unsupervised requirements clustering framework, which makes use of machine-learning methods for requirements validation, being able to overcome data inconsistencies and effectively determine appropriate requirements clusters for optimal definition of software development sprints. {\textcopyright} 2013 Springer-Verlag London.},
author = {Belsis, Petros and Koutoumanos, Anastasios and Sgouropoulou, Cleo},
doi = {10.1007/s00766-013-0172-9},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Belsis2014{\_}Article{\_}PBURCAPatterns-basedUnsupervis.pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Agile software process,Distributed software development,Requirements clustering,Requirements engineering,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {2},
pages = {213--225},
shorttitle = {PBURC},
title = {{PBURC: A patterns-based, unsupervised requirements clustering framework for distributed agile software development}},
url = {http://link.springer.com/10.1007/s00766-013-0172-9},
volume = {19},
year = {2014}
}
@incollection{hutchison_tackling_2010,
abstract = {[Context and motivation] Traceability is not as well established in the automobile industry as it is for instance in avionics. However, new standards require specifications to contain traces. Manually creating and maintaining traceability in large specifications is cumbersome and expensive. [Question/problem] This work investigates whether it is possible to semi-automatically recover traceability within natural language specifications (e.g. requirement and test specifications) using information retrieval algorithms. More specifically, this work deals with large, German specifications from the automobile industry. [Principal ideas/results] Using optimized algorithms, we are able to retrieve most of the traces. The remaining problem is the reduction of false-positive candidate traces. [Contribution] We identified optimizations that improve the retrieval quality: Use of meta-data, filtering of redundant texts, use of domain language, and dynamic identification of signals. {\textcopyright} 2010 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Leuser, J{\"{o}}rg and Ott, Daniel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-14192-8_19},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard and Wieringa, Roel and Persson, Anne},
isbn = {3642141919},
issn = {03029743},
keywords = {German Specifications,Information Retrieval,Large Specifications,Natural Language,Traceability,Traceability Recovery,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {203--217},
publisher = {Springer Berlin Heidelberg},
title = {{Tackling semi-automatic trace recovery for large specifications}},
url = {http://link.springer.com/10.1007/978-3-642-14192-8{\_}19},
volume = {6182 LNCS},
year = {2010}
}
@inproceedings{7105213,
abstract = {Ambiguity is a critical problem in the software requirement specifications. Requirements are typically expressed in a natural language. However, expressions in natural languages are likely to suffer from ambiguities. Hence, it is essential that when the requirements are analyzed, the ambiguities are resolved to the extent possible, so that the software specifications are free of any potential misinterpretations. One of the attractive alternatives in resolving ambiguities is to convert the informal natural language requirements into their formal or semi-formal counterpart that ensures precision and orthogonality. Towards meeting this goal, the Unified Modeling Language notations can be exploited to an advantage. Being graphical in nature, the UML notations can be easily comprehended by the user and at the same time, being driven by orthogonal syntactic/semantic conventions, the notations help reduce ambiguities greatly. The proposed work is aimed to support the text based information as the requirement specification. It resolves the uncertainty and find the UML components and the relationship among them to generate the accurate the UML Diagram.},
author = {Patel, Gaurav A. and Priya, A. Swathy},
booktitle = {2014 International Conference on Advances in Engineering and Technology, ICAET 2014},
doi = {10.1109/ICAET.2014.7105213},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Patel, Gaurav A. and Priya, A. Swathy{\_} {\_}{\_}Resolve the uncertainity in requirement specification to generate the UML diagram{\_}{\_} (2015).pdf:pdf},
isbn = {9781479949496},
keywords = {Ambiguity,Natural language processing (NLP),OO analysis/design,Requirement engineering,Software Requirement Specification,Unified Modeling Language,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1--7},
title = {{Resolve the uncertainity in requirement specification to generate the UML diagram}},
year = {2015}
}
@conference{Li2018388,
abstract = {Eliciting security requirements in early stage of system development has been widely recognized as an efficient way for minimizing security cost and avoiding recurring security problems. However, in many projects, security requirements are not explicitly specified but rather mixed with other requirements, requiring precise and fast identification of such security requirements. Although several probability-based approaches have been proposed to tackle this problem, they are either imprecise or domain-dependent. In this paper, we propose a tool-supported method to efficiently identify security requirements, which combines linguistic analysis with machine learning techniques. In particular, we apply a systematic approach to identify linguistic features of security requirements based on existing security requirements ontologies and linguistic knowledge. We automatically extract such features from textual requirements, which are then used to train security requirements classifiers using typical machine learning techniques. We have implemented a prototype tool to support our approach, and have systematically evaluated our approach based on three realistic requirements specifications. The evaluation results show that our approach has promising potential to train classifiers that can classify requirements specifications from different application domains.},
annote = {cited By 4},
author = {Li, Tong},
booktitle = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
doi = {10.1109/APSEC.2017.45},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/tong Identifying Security Requirements Based on Linguistic Analysis and Machine Learning.pdf:pdf},
isbn = {9781538636817},
issn = {15301362},
keywords = {linguistic analysis,machine learning,natural language processing,prototype,scopus{\_}inc{\_}nlp{\_}x{\_}re,security requirements classification,security requirements ontology},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {388--397},
title = {{Identifying Security Requirements Based on Linguistic Analysis and Machine Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045897286{\&}doi=10.1109{\%}2FAPSEC.2017.45{\&}partnerID=40{\&}md5=b155257bfb86262a49a00c5d5feac971},
volume = {2017-Decem},
year = {2018}
}
@inproceedings{8431221,
abstract = {Extracting concepts from the software requirements is one of the first step on the way to automating the software development process. This task is difficult due to the ambiguity of the natural language used to express the requirements specification. The methods used so far consist mainly of statistical analysis of words and matching expressions with a specific ontology of the domain in which the planned software will be applicable. This article proposes a method and a tool to extract concepts based on a grammatical analysis of requirements written in English without the need to refer to specialized ontology. These concepts can be further expressed in the class model, which then can be the basis for the object-oriented analysis of the problem. This method uses natural language processing (NLP) techniques to recognize parts of speech and to divide sentences into phrases and also the WordNet dictionary to search for known concepts and recognize relationships between them.},
author = {Kuchta, Jaroslaw and Padhiyar, Priti},
booktitle = {Proceedings - 2018 11th International Conference on Human System Interaction, HSI 2018},
doi = {10.1109/HSI.2018.8431221},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kuchta, Jaroslaw and Padhiyar, Priti{\_} {\_}{\_}Extracting concepts from the software requirements specification using natural language processing{\_}{\_} (2018).pdf:pdf},
isbn = {9781538650233},
keywords = {Natural Language Processing (NLP),Software Requirement Specification,WordNet ontology,domain ontology,ieee{\_}inc{\_}nlp{\_}x{\_}re,requirement engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {443--448},
title = {{Extracting concepts from the software requirements specification using natural language processing}},
year = {2018}
}
@inproceedings{10.1145/2381716.2381786,
abstract = {Most research works have found that an important root cause of software project failure comes from the requirements; their quality has an important impact over other artifacts. As the requirements are expressed in natural language, they can be an important source of defects. Aspects such as non ambiguity, completeness, and atomicity can be affected due the characteristics of natural language. Traditional practices focus on finding software bugs, as a corrective approach, until the project has been coded already, instead assuring quality since the beginning. By other hand, evaluating such quality attributes can be a difficult task. In this paper we propose some guidelines for a disciplined sentence structure for expressing the requirements, which allows natural language processing techniques to evaluate quality. We also propose a tool for automatic requirement evaluation based on the grammar structure of sentences expressed in natural language. With this tool we have a huge speed increase over manual evaluation. In order to validate our proposal we have implemented a set of experiments with real projects, assessing the impact of requirements quality over project results. Copyright 2012 ACM.},
address = {New York, NY, USA},
author = {Huertas, Carlos and Ju{\'{a}}rez-Ram{\'{i}}rez, Reyes},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2381716.2381786},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Huertas, Carlos and Ju{\_}{\_}'{\_}a{\_}{\_}rez-Ram{\_}{\_}'{\_}i{\_}{\_}rez, Reyes{\_} {\_}{\_}NLARE, a natural language processing tool for automatic requirements evaluation{\_}{\_} (2012).pdf:pdf},
isbn = {9781450311854},
keywords = {Natural language processing,Software bugs,Software engineering,Software quality,Software requirements,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {371--378},
publisher = {Association for Computing Machinery},
series = {CUBE '12},
title = {{NLARE, a natural language processing tool for automatic requirements evaluation}},
url = {https://doi.org/10.1145/2381716.2381786},
year = {2012}
}
@inproceedings{8491159,
abstract = {Creating glossaries for large corpora of requirments is an important but expensive task. Glossary term extraction methods often focus on achieving a high recall rate and, therefore, favor linguistic proecssing for extracting glossary term candidates and neglect the benefits from reducing the number of candidates by statistical filter methods. However, especially for large datasets a reduction of the likewise large number of candidates may be crucial. This paper demonstrates how to automatically extract relevant domain-specific glossary term candidates from a large body of requirements, the CrowdRE dataset. Our hybrid approach combines linguistic processing and statistical filtering for extracting and reducing glossary term candidates. In a twofold evaluation, we examine the impact of our approach on the quality and quantity of extracted terms. We provide a ground truth for a subset of the requirements and show that a substantial degree of recall can be achieved. Furthermore, we advocate requirements coverage as an additional quality metric to assess the term reduction that results from our statistical filters. Results indicate that with a careful combination of linguistic and statistical extraction methods, a fair balance between later manual efforts and a high recall rate can be achieved.},
author = {Gemkow, Tim and Conzelmann, Miro and Hartig, Kerstin and Vogelsang, Andreas},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00052},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Gemkow, Tim and Conzelmann, Miro and Hartig, Kerstin and Vogelsang, Andreas{\_} {\_}{\_}Automatic glossary term extraction from large-scale requirements specifications{\_}{\_} (2018).pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {CrowdRE,Glossary term extraction,Natural language processing,Requirements engineering,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {412--417},
title = {{Automatic glossary term extraction from large-scale requirements specifications}},
year = {2018}
}
@inproceedings{10.1145/3341105.3374028,
abstract = {A natural language-based requirements specification tends to be full of requirements that are ambiguous, unnecessarily complicated, missing, wrong, duplicated or conflicting. Poor quality requirements often compromise the subsequent software construction activities (e.g. planning, design, coding or testing). A strategy for requirements quality evaluation should enable a faster requirements analysis, highlight defect indicators and incorporate also fix recommendations to help practitioners to effectively improve their requirements. In this paper we brief describe a Natural Language Processing and Petri-Net strategy for automated analysis of scenario-driven requirements named C{\&}L prototype tool. The C{\&}L evaluates structural (Static analysis) aspects of scenarios and behavioral aspects (Dynamic analysis) of equivalent Petri-Nets. The feasibility of the C{\&}L is evaluated in four projects described as use cases, which indicates promising results (the overall precision was 93.5{\%} and the recalls were perfect).},
address = {New York, NY, USA},
author = {Sarmiento-Calisaya, Edgar and C{\'{a}}rdenas, Edward Hinojosa and Cornejo-Aparicio, Victor and Alzamora, Guina Sotomayor},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3341105.3374028},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/SARMIE{\~{}}2.PDF:PDF},
isbn = {9781450368667},
keywords = {Analysis,Automation,Quality,Requirement,Scenario,Use case,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1405--1413},
publisher = {Association for Computing Machinery},
series = {SAC '20},
title = {{Towards the improvement of natural language requirements descriptions: The C{\&}{\&}L tool}},
url = {https://doi.org/10.1145/3341105.3374028},
year = {2020}
}
@inproceedings{10.1145/1774088.1774148,
abstract = {Requirements analysis is an important phase in a software project. It is often performed in an informal way by specialists who review documents looking for ambiguities, technical inconsistencies and incompleteness. Automatic evaluation of Natural Language (NL) requirements documents has been proposed as a means to improve the quality of the system under development. We show how the tool QuARS Express, introduced in a quality analysis process, is able to manage complex and structured requirement documents containing metadata, and to produce an analysis report rich of categorized information that points out linguistic defects and indications about the writing style of NL requirements. In this paper we report our experience using this tool in the automatic analysis of a large collection of natural language requirements, produced inside the MODCONTROL project. {\textcopyright} 2010 ACM.},
address = {New York, NY, USA},
author = {Bucchiarone, Antonio and Gnesi, Stefania and Fantechi, Alessandro and Trentanni, Gianluca},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/1774088.1774148},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/BUCCHI{\~{}}1.PDF:PDF},
isbn = {9781605586380},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,natural language automated analysis,natural language processing,requirements analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {281--286},
publisher = {Association for Computing Machinery},
series = {SAC '10},
title = {{An experience in using a tool for evaluating a large set of natural language requirements}},
url = {https://doi.org/10.1145/1774088.1774148},
year = {2010}
}
@inproceedings{6495150,
abstract = {Ambiguity is the main problem for software requirements specifications written in natural language. This study describes the problems and ambiguities encountered in Thai software requirements specifications, and proposes a simple approach to writing software requirements specifications in Thai by using controlled syntax. Our approach uses Backus Naur Form to analyze the syntax. We evaluate our approach from the questionnaires received from clients and software engineers. The experiment reports that the average accuracy from clients and software engineers is 88.89{\%} and 86.67{\%} respectively. {\textcopyright} 2012 IEEE.},
author = {Thongglin, Kanjana and Cardey, Sylviane and Greenfield, Peter},
booktitle = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
doi = {10.1109/ICTAI.2012.136},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/06495150.pdf:pdf},
isbn = {9780769549156},
issn = {10823409},
keywords = {ambiguity,artificial intelligence,controlled language,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,scopus{\_}inc{\_}nlp{\_}x{\_}re,software requirements specification,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {nov},
pages = {964--969},
title = {{Controlled syntax for thai software requirements specification}},
volume = {1},
year = {2012}
}
@incollection{kamalrudin_constructing_2018,
abstract = {A goal model, which is one of the common requirements models, has advantages of formalizing and visualizing results of requirements analysis. The model regards a requirement as a goal, and the root goal that is achieved by system execution should be decomposed to precondition goals. Current systems are large and complexed, so that there are a lot of requirements to be implemented. Therefore it is difficult to extract all goals and construct an elaborated goal model manually. In this paper we propose a process to support constructing goal models from requirements descriptions written in a natural language. In the proposed process, extraction rules are used to extract goals from requirements descriptions and then to construct a goal model from the goals. To evaluate our process, we applied the process to two system descriptions to construct goal models. The results show that the proposed process extracted appropriate goals and successfully assembled these goals in a goal hierarchy. We also report preliminary results of automating the proposed process.},
address = {Singapore},
annote = {Series Title: Communications in Computer and Information Science},
author = {Shimada, Hironori and Nakagawa, Hiroyuki and Tsuchiya, Tatsuhiro},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-981-10-7796-8_14},
editor = {Kamalrudin, Massila and Ahmad, Sabrina and Ikram, Naveed},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Shimada, Hironori and Nakagawa, Hiroyuki and Tsuchiya, Tatsuhiro{\_} {\_}{\_}Constructing a goal model from requirements descriptions based on extraction rules{\_}{\_} (2018).pdf:pdf},
isbn = {9789811077951},
issn = {18650929},
keywords = {Goal models,Natural languages,Requirements analysis,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {175--188},
publisher = {Springer Singapore},
title = {{Constructing a goal model from requirements descriptions based on extraction rules}},
url = {http://link.springer.com/10.1007/978-981-10-7796-8{\_}14},
volume = {809},
year = {2018}
}
@inproceedings{8491129,
abstract = {[Context] Semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. Such metadata is important for the systematic analysis of legal requirements. [Objectives] Our work is motivated by two observations: (1) The existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) Automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (NLP). Our objective is to take steps toward addressing these limitations. [Methods] We review and reconcile the semantic legal metadata types proposed in RE. Subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [Results and Conclusions] We propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on NLP. We evaluate the extraction rules through a case study. Our results indicate that the rules generate metadata annotations with high accuracy.},
author = {Sleimi, Amin and Sannier, Nicolas and Sabetzadeh, Mehrdad and Briand, Lionel and Dann, John},
booktitle = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
doi = {10.1109/RE.2018.00022},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sleimi, Amin and Sannier, Nicolas and Sabetzadeh, Mehrdad and Briand, Lionel and Dann, John{\_} {\_}{\_}Automated extraction of semantic legal metadata using natural language processing{\_}{\_} (2018).pdf:pdf},
isbn = {9781538674185},
issn = {2332-6441},
keywords = {Legal requirements,Natural language processing,Semantic legal metadata,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {124--135},
title = {{Automated extraction of semantic legal metadata using natural language processing}},
year = {2018}
}
@article{Mezghani2018501,
abstract = {Requirements are usually “hand-written” and suffers from several problems like redundancy and inconsistency. These problems between requirements or sets of requirements impact negatively the success of final products. Manually processing these issues requires too much time and it is very costly. We propose in this paper to automatically handle redundancy and inconsistency issues in a classification approach. The main contribution of this paper is the use of k-means algorithm for redundancy and inconsistency detection in a new context, which is Requirements Engineering context. Also, we introduce a preprocessing step based on the Natural Language Processing techniques in order to see the impact of this latter to the k-means results. We use Part-Of-Speech (POS) tagging and noun chunking in order to detect technical business terms associated with the requirements documents that we analyze. We experiment this approach on real industrial datasets. The results show the efficiency of the k-means clustering algorithm, especially with the preprocessing.},
annote = {cited By 0},
author = {Mezghani, Manel and Kang, Juyeon and S{\`{e}}des, Florence},
doi = {10.1007/978-3-319-91947-8_52},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/J4XWJ2QX/Mezghani e.a. - 2018 - Using k-Means for Redundancy and Inconsistency Det.pdf:pdf},
isbn = {9783319919461},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Inconsistency,K-means,POS tagging,Redundancy,Requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {501--508},
title = {{Using k-means for redundancy and inconsistency detection: Application to industrial requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048058491{\&}doi=10.1007{\%}2F978-3-319-91947-8{\_}52{\&}partnerID=40{\&}md5=7094bda86fb29542463734bf0a29d598},
volume = {10859 LNCS},
year = {2018}
}
@inproceedings{10.1145/2993717.2993733,
abstract = {Software size estimation is a crucial step in project management. According to the Standish Chaos Report, 65{\%} of software projects are over budget or deadline; therefore, a good size estimation method is very important. However, existing estimation methods are complicated and human-effort consuming. In many industrial projects, project technical leads (PTLs) do not use these methods but just give a rough estimation based on their experience. To decrease human effort, we propose an early software size estimation (ESSE) method, which can extract semantic features from natural language requirements automatically, and build size estimation models for project. Firstly, ESSE makes a two-level semantic analysis of requirements specification documents by information extraction and activation spreading. Then, complexity-related features are extracted from the results of semantic analysis. Finally, a size estimation model is trained to predict size of new projects by regression algorithms. Experiments in real industrial datasets show that our method is effective and can be applied to real industrial projects.},
address = {New York, NY, USA},
author = {Zhang, Cheng and Tong, Shensi and Mo, Wenkai and Zhou, Yang and Xia, Yong and Shen, Beijun},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2993717.2993733},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/ZHANG{\_}{\~{}}1.PDF:PDF},
isbn = {9781450348294},
keywords = {Requirements analysis,Semantic analysis,Software size estimation,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {112--115},
publisher = {Association for Computing Machinery},
series = {Internetware '16},
title = {{ESSE: An early software size estimation method based on auto-extracted requirements features}},
url = {https://doi.org/10.1145/2993717.2993733},
volume = {18-Septemb},
year = {2016}
}
@incollection{malik_building_2019,
abstract = {Elicitation is the entry point of any digital transformation project. Domain knowledge is a critical factor for successful elicitation. Hence, elicitation is usually executed by subject matter experts (SME), who have fair amount of domain knowledge required for the project. However, such SME are scare and costly resources. Hence there is an urgent need to design a framework that can help build domain knowledge of the business analyst (BA) involved in elicitation in order to lower dependence on SME. Such framework should be programmatically implementable and should be able to work seamlessly with the existing techniques of elicitation. Several solutions based on organizational theories, cognitive models, and strategic frameworks have been proposed with varying results. This work is in progress. As elicitation generates a web of interactions in natural language, natural language processing (NLP) techniques can be utilized to extract and build domain knowledge. This paper investigates the application of NLP to extract working domain knowledge from the available organizational documents to assist analyst during elicitation phase. The paper proposes a conceptual framework to extract and build domain familiarity. This paper also analyzes the gaps and new areas of research in this direction.},
address = {Singapore},
annote = {Series Title: Advances in Intelligent Systems and Computing},
author = {Owais, Syed Taha and Ahmad, Tanvir and Abuzar, Md},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-981-13-1822-1_54},
editor = {Malik, Hasmat and Srivastava, Smriti and Sood, Yog Raj and Ahmad, Aamir},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/owais Building Domain Familiarity for Elicitation A Conceptual Framework Based on NLP Techniques.pdf:pdf},
isbn = {9789811318214 9789811318221},
issn = {21945357},
keywords = {Building,Elicitation,NLP techniques,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {577--587},
publisher = {Springer Singapore},
shorttitle = {Building {\{}Domain{\}} {\{}Familiarity{\}} for {\{}Elicitation{\}}},
title = {{Building domain familiarity for elicitation: A conceptual framework based on NLP techniques}},
url = {http://link.springer.com/10.1007/978-981-13-1822-1{\_}54},
volume = {697},
year = {2019}
}
@incollection{dregvaite_running_2016,
abstract = {While requirements focus on how the user interacts with the system, user stories concentrate on the purpose of software features. But in practice, functional requirements are also described in user stories. For this reason, requirements clarification is needed, especially when they are written in natural language and do not stick to any templates (e.g., “as an X, I want Y so that Z{\ldots}”). However, there is a lot of implicit knowledge that is not expressed in words. As a result, natural language requirements descriptions may suffer from incompleteness. Existing approaches try to formalize natural language or focus only on entirely missing and not on deficient requirements. In this paper, we therefore present an approach to detect knowledge gaps in user-generated software requirements for interactive requirement clarification: We provide tailored suggestions to the users in order to get more precise descriptions. For this purpose, we identify not fully instantiated predicate argument structures in requirements written in natural language and use context information to realize what was meant by the user.},
address = {Cham},
annote = {Series Title: Communications in Computer and Information Science},
author = {B{\"{a}}umer, Frederik S. and Geierhos, Michaela},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-319-46254-7_44},
editor = {Dregvaite, Giedre and Damasevicius, Robertas},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/B{\_}{\_}{\_}{\_}a{\_}{\_}umer, Frederik S. and Geierhos, Michaela{\_} {\_}{\_}Running out of words{\_} How similar user stories can help to elaborate individual natural language requirement descriptions{\_}{\_} (2016).pdf:pdf},
isbn = {9783319462530},
issn = {18650929},
keywords = {Compensatory user stories,Natural language requirements clarification,Syntactically incomplete requirements,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {549--558},
publisher = {Springer International Publishing},
shorttitle = {Running {\{}Out{\}} of {\{}Words{\}}},
title = {{Running out of words: How similar user stories can help to elaborate individual natural language requirement descriptions}},
url = {http://link.springer.com/10.1007/978-3-319-46254-7{\_}44},
volume = {639},
year = {2016}
}
@inproceedings{8920392,
abstract = {The modern software landscape is highly competitive. Software companies need to quickly fix reported bugs and release requested new features, or they risk negative reviews and reduced market share. The amount of online user feedback prevents manual analysis. Past research has investigated automated requirement mining techniques on online platforms like App Stores and Twitter, but online product forums have not been studied. In this paper, we show that online product forums are a rich source of user feedback that may be used to elicit product requirements. The information contained in forum questions is different from what has been described in the related work on App Stores or Twitter. Users often provide detailed context to specific problems they encounter with a software product and other users respond with workarounds or to confirm the problem. Through the analysis of two large forums, we identify 18 different types of information (classifications) contained in forums that can be relevant to maintenance and evolution tasks. We show that a state-of-the-art App Store tool is unable to accurately classify forum data, which may be due to the differences in content. Thus, specific techniques are likely needed to mine requirements from product forums. In an exploratory study, we developed classifiers with forum specific features. Promising results are achieved for all classifiers with f-measure scores ranging from 70.3{\%} to 89.8{\%}.},
author = {Tizard, James and Wang, Hechen and Yohannes, Lydia and Blincoe, Kelly},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00014},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Tizard, James and Wang, Hechen and Yohannes, Lydia and Blincoe, Kelly{\_} {\_}{\_}Can a conversation paint a picture{\_} Mining requirements in software forums{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Machine learning,Natural language processing,Requirements engineering,Software product forums,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {17--27},
title = {{Can a conversation paint a picture? Mining requirements in software forums}},
volume = {2019-Septe},
year = {2019}
}
@article{Saint-Dizier2018290,
abstract = {Requirements are designed to specify the features of systems. Even for a simple system, several thousands of requirements produced by different authors are needed. Overlap and incoherence problems are frequently observed. In this article, we propose a method to construct a corpus of various types of incoherences and a categorization that leads to the definition of patterns to mine incoherent requirements. We focus in this contribution on incoherences (1) which can be detected solely from linguistic factors and (2) which concern pairs of requirements. Together, these represent about 60{\%} of the different types of incoherences; the other types require extensive domain knowledge and reasoning. The second part of this article develops several language-based patterns to detect incoherent requirements in texts. An indicative evaluation of the results concludes this contribution. More generally, this contribution opens new perspectives on incoherence analysis in texts.},
annote = {cited By 0},
author = {Saint-Dizier, Patrick},
doi = {10.1016/j.datak.2018.05.006},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Saint-Dizier, Patrick{\_} {\_}{\_}Mining incoherent requirements in technical specifications{\_} Analysis and implementation{\_}{\_} (2018).pdf:pdf},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Incoherence analysis,Linguistics of requirements,Natural language processing,Requirement engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {290--306},
title = {{Mining incoherent requirements in technical specifications: Analysis and implementation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047824410{\&}doi=10.1016{\%}2Fj.datak.2018.05.006{\&}partnerID=40{\&}md5=ed47518c766aad1447a926cb62e61723},
volume = {117},
year = {2018}
}
@inproceedings{10.5555/2820704.2820713,
abstract = {Latent Semantic Indexing (LSI) is an accepted technique for information retrieval that is used in requirements tracing to recover links between artifacts, e.g., between requirements documents and test cases. However, configuring LSI is difficult, because the number of possible configurations is huge. The configuration of LSI, which depends on the underlying dataset, greatly influences the accuracy of the results. Therefore, one of the key challenges for applying LSI is finding an appropriate configuration. Evaluating results for each configuration is time consuming, and therefore, automatically determining an appropriate configuration for LSI improves the applicability of LSI based methods. We propose a fully automated technique to determine appropriate configurations for LSI to recover links between requirements artifacts. We evaluate our technique on six sets of requirements artifacts from industry and academia and show that the configurations selected by our approach yield results that are almost as accurate as results from configurations based on a ground truth like known links or expert knowledge. Our approach improves the applicability of LSI in industry and academia, as researchers and practitioners do not need to determine appropriate configurations manually or provide a ground truth.},
author = {Eder, Sebastian and Femmer, Henning and Hauptmann, Benedikt and Junker, Maximilian},
booktitle = {Proceedings - 2nd International Workshop on Requirements Engineering and Testing, RET 2015},
doi = {10.1109/RET.2015.13},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/eder Configuring Latent Semantic Indexing for Requirements Tracing.pdf:pdf},
isbn = {9781479919345},
keywords = {Latent Semantic Indexing,Requirements Tracing,Traceability Link Recovery,acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {27--33},
publisher = {IEEE Press},
series = {RET '15},
title = {{Configuring Latent Semantic Indexing for Requirements Tracing}},
year = {2015}
}
@article{VidyaSagar201425,
abstract = {Requirements analysts consider a conceptual model to be an important artifact created during the requirements analysis phase of a software development life cycle (SDLC). A conceptual, or domain model is a visual model of the requirements domain in focus. Owing to its visual nature, the model serves as a platform for the deliberation of requirements by stakeholders and enables requirements analysts to further refine the functional requirements. Conceptual models may evolve into class diagrams during the design and execution phases of the software project. Even a partially automated conceptual model can save significant time during the requirements phase, by quickening the process of graphical communication and visualization. This paper presents a system to create a conceptual model from functional specifications, written in natural language in an automated manner. Classes and relationships are automatically identified from the functional specifications. This identification is based on the analysis of the grammatical constructs of sentences, and on Object Oriented principles of design. Extended entity-relationship (EER) notations are incorporated into the class relationships. Optimizations are applied to the identified entities during a post-processing stage, and the final conceptual model is rendered. The use of typed dependencies, combined with rules to derive class relationships offers a granular approach to the extraction of the design elements in the model. The paper illustrates the model creation process using a standard case study, and concludes with an evaluation of the usefulness of this approach for the requirements analysis. The analysis is conducted against both standard published models and conceptual models created by humans, for various evaluation parameters. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
annote = {cited By 35},
author = {{Vidya Sagar}, Vidhu Bhala R. and Abirami, S.},
doi = {10.1016/j.jss.2013.08.036},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/vidya Conceptual modeling of natural language functional requirements.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Automated requirements analysis,Conceptual modeling,Natural language processing,Syntactic analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {1},
pages = {25--41},
title = {{Conceptual modeling of natural language functional requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891601603{\&}doi=10.1016{\%}2Fj.jss.2013.08.036{\&}partnerID=40{\&}md5=755850dc60bec09f29d103fbf299d17e},
volume = {88},
year = {2014}
}
@inproceedings{8719411,
abstract = {Machine learning (ML) has demonstrated practical impact in a variety of application domains. Software engineering is a fertile domain where ML is helping in automating different tasks. In this paper, our focus is the intersection of software requirement engineering (RE) and ML. To obtain an overview of how ML is helping RE and the research trends in this area, we have surveyed a large number of research articles. We found that the impact of ML can be observed in requirement elicitation, analysis and specification, validation and management. Furthermore, in these categories, we discuss the specific problem solved by ML, the features and ML algorithms used as well as datasets, when available. We outline lessons learned and envision possible future directions for the domain.},
author = {Iqbal, Tahira and Elahidoost, Parisa and Lucio, Levi},
booktitle = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
doi = {10.1109/APSEC.2018.00015},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/A Bird's Eye View on Requirements Engineering and Machine Learning.pdf:pdf},
isbn = {9781728119700},
issn = {15301362},
keywords = {Machine learning,Overview,Requirements Engineering,State of the art,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {11--20},
title = {{A Bird's Eye View on Requirements Engineering and Machine Learning}},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{8920599,
abstract = {While many apps include built-in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like Twitter. Compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue. To make this feedback actionable to developers, support teams engage in recurring, effortful conversations with aMartens, D., {\&} Maalej, W. (2019). Extracting and analyzing context information in user-support conversations on twitter. Proceedings of the IEEE International Conference on Requirements Engineering, 2019-Septe, 131–141. https://doi.org/10.1109/RE.2019.00024pp users to clarify missing context items. This paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version. Evaluated against a truthset of 3014 tweets from official Twitter support accounts of the 3 popular apps Netflix, Snapchat, and Spotify, our approach achieved precisions from 81{\%} to 99{\%} and recalls from 86{\%} to 98{\%} for the different context item types. Combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto-populating issue trackers with structured bug reports.},
archivePrefix = {arXiv},
arxivId = {1907.13395},
author = {Martens, Daniel and Maalej, Walid},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00024},
eprint = {1907.13395},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Martens, Daniel and Maalej, Walid{\_} {\_}{\_}Extracting and analyzing context information in user-support conversations on twitter{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Context Information,Twitter,User Feedback,User Support Conversations,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {131--141},
title = {{Extracting and analyzing context information in user-support conversations on twitter}},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{6671349,
abstract = {In this paper we describe the architecture and application of GaiusT, a multi-phase framework for extracting requirements from legal documents. GaiusT aims to cover the extraction process from the very first steps, pre-processing the input texts and supporting structural and semantic annotation of legal documents. The annotated information is recorded in a database facilitating both retrieval and evaluation of the results. Existing linguistic tools and resources have been applied whenever possible. A post-analysis of the ongoing implementation of the modules in the GaiusT architecture is given as a preliminary understanding of how much existing tools can be adopted and how much they had to be adapted. {\textcopyright} 2013 IEEE.},
author = {Zeni, Nicola and Mich, Luisa and Mylopoulos, John and Cordy, James R.},
booktitle = {2013 6th International Workshop on Requirements Engineering and Law, RELAW 2013 - Proceedings},
doi = {10.1109/RELAW.2013.6671349},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Zeni, Nicola and Mich, Luisa and Mylopoulos, John and Cordy, James R.{\_} {\_}{\_}Applying gaiust for extracting requirements from legal documents{\_}{\_} (2013).pdf:pdf},
isbn = {9781479909506},
keywords = {Annotation schema,Conceptual model,Requirements elicitation,Semantic annotation,Support systems,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {65--68},
title = {{Applying gaiust for extracting requirements from legal documents}},
year = {2013}
}
@inproceedings{7337625,
abstract = {Unified Modeling Language (UML) is the most popular modeling language for analysis, design and development of the software system. There has been a lot of research interest in generating these UML models, especially class diagrams, automatically from Natural Language requirements. The interest in class diagrams can be attributed to the fact that classes represent the abstractions present in the system to be developed. However, automated generation of UML class diagrams is a challenging task as it involves lot of pre-processing or manual intervention at times. In this paper, we present dependency analysis based approach to derive UML class diagrams automatically from Natural Language requirements. We transform the requirements statements to an intermediary frame-based structured representation using dependency analysis of requirements statements and the Grammatical Knowledge Patterns. The knowledge stored in the frame-based structured representation is used to derive class diagrams using rule-based algorithm. Our approach has generated similar class diagrams as reported in earlier works based on linguistic analysis with either annotation or manual intervention. We present the effectiveness of our approach in terms of recall and precision for the case-studies presented in earlier works.},
author = {Sharma, Richa and Srivastava, Pratyoush K. and Biswas, Kanad K.},
booktitle = {2nd International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2015 - Proceedings},
doi = {10.1109/AIRE.2015.7337625},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sharma, Richa and Srivastava, Pratyoush K. and Biswas, Kanad K.{\_} {\_}{\_}From natural language requirements to UML class diagrams{\_}{\_} (2015).pdf:pdf},
isbn = {9781509001255},
keywords = {Natural Language Processing,Requirements,UML Models,ieee{\_}inc{\_}nlp{\_}x{\_}re,patterns,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {25--32},
title = {{From natural language requirements to UML class diagrams}},
year = {2015}
}
@article{Li2018108,
abstract = {In order to make a software project succeed, it is necessary to determine the requirements for systems and to document them in a suitable manner. Many ways for requirements elicitation have been discussed. One way is to gather requirements with crowdsourcing methods, which has been discussed for years and is called crowdsourcing requirements engineering. User requests forums in open source communities, where users can propose their expected features of a software product, are common examples of platforms for gathering requirements from the crowd. Requirements collected from these platforms are often informal text descriptions and we name them user requests. In order to transform user requests into structured software requirements, it is better to know the class of requirements that each request belongs to so that each request can be rewritten according to corresponding requirement templates. In this paper, we propose an effective classification methodology by employing both project-specific and non-project-specific keywords and machine learning algorithms. The proposed strategy does well in achieving high classification accuracy by using keywords as features, reducing considerable manual efforts in building machine learning based classifiers, and having stable performance in finding minority classes no matter how few instances they have.},
annote = {cited By 14},
author = {Li, Chuanyi and Huang, Liguo and Ge, Jidong and Luo, Bin and Ng, Vincent},
doi = {10.1016/j.jss.2017.12.028},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Li, Chuanyi and Huang, Liguo and Ge, Jidong and Luo, Bin and Ng, Vincent{\_} {\_}{\_}Automatically classifying user requests in crowdsourcing requirements engineering{\_}{\_} (2018).pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Crowdsourcing requirements engineering,Machine learning,Natural language processing,Software requirements classification,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {108--123},
title = {{Automatically classifying user requests in crowdsourcing requirements engineering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039771338{\&}doi=10.1016{\%}2Fj.jss.2017.12.028{\&}partnerID=40{\&}md5=88c411b1b66a52d14926f96c629a2b9e},
volume = {138},
year = {2018}
}
@article{Alksasbeh20171182,
abstract = {Use case modeling is an important requirements engineering technique which plays an important role in describing the systems specifications and facilitating systems development. The use of linguistic representations of system requirements as a source of information for generating use case models is a challenging task and can be considered relatively a new field. This paper has tackled the problem of extracting the required elements that are needed to automatically generate use case diagrams from specification documents which are written in common natural language. Therefore, we have developed an automated system which employs the Natural Language Processing (NLP) techniques to parse specifications syntactically based on a predefined set of heuristic rules. Furthermore, our system incorporates the capability of analyzing and understanding the English text as a semantic unit to infer some important linguistic features such as reference, comparing and additive cohesive devices. The extracted information is then mapped into actors and use cases, which are the basic elements of use case diagrams. Our proposed approach was evaluated using both recall and precision performance measurements. The experiments revealed that our system has an average of 96{\%} recall and 84{\%} precision.},
annote = {cited By 1},
author = {Alksasbeh, Malek Zakarya and Alqaralleh, Bassam A.Y. and Alramadin, Tahseen A. and Alemerien, Khalid Ali},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/zakarya An automated use case diagrams generator from natural language requirements.pdf:pdf},
issn = {18173195},
journal = {Journal of Theoretical and Applied Information Technology},
keywords = {Automatic diagrams generation,Information extraction,Natural language processing,Use case diagrams,User requirements analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {5},
pages = {1182--1190},
title = {{An automated use case diagrams generator from natural language requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015383914{\&}partnerID=40{\&}md5=0f7cd2b85a504213163277b0a7e2bd98},
volume = {95},
year = {2017}
}
@incollection{fred_chunking_2015,
abstract = {In order to obtain a most effective return on a software project investment, then at least one requirements inspection shall be completed. A formal requirement inspection identifies low quality knowledge representation content in the requirements document. In software development projects where natural language requirements are produced, a requirements document summarizes the results of requirements knowledge analysis and becomes the basis for subsequent software development. In many cases, the knowledge content quality of the requirements documents dictates the success of the software development. The need for determining knowledge quality of requirements documents is particularly acute when the target applications are large, complicated, and mission critical. The goal of this research is to develop knowledge content quality indicators of requirements statements in a requirements document prior to informal inspections. To achieve the goal, knowledge quality properties of the requirements statements are adopted to represent the quality of requirements statements. A suite of complexity metrics for requirements statements is used as knowledge quality indicators and is developed based upon natural language knowledge research of noun phrase (NP) chunks. A formal requirements inspection identifies low quality knowledge representation content in the requirements document. The knowledge quality of requirements statements of requirements documents is one of the most important assets a project must inspect. An application of the metrics to improve requirements understandability and readability during requirements inspections can be built upon the metrics shown and suggested to be taken into account.},
address = {Berlin, Heidelberg},
annote = {Series Title: Communications in Computer and Information Science},
author = {Rine, David C. and Fraga, Anabel},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-662-46549-3_16},
editor = {Fred, Ana and Dietz, Jan L G and Liu, Kecheng and Filipe, Joaquim},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/rine Chunking complexity measurement for requirements quality knowledge representation.pdf:pdf},
isbn = {9783662465486},
issn = {18650929},
keywords = {Chunking and cognition,Cohesion,Complexity metrics,Coupling,Information retrieval,NP chunk,Natural language understanding and processing,Requirements,Requirements inspections,Software quality,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {245--259},
publisher = {Springer Berlin Heidelberg},
title = {{Chunking complexity measurement for requirements quality knowledge representation}},
url = {http://link.springer.com/10.1007/978-3-662-46549-3{\_}16},
volume = {454},
year = {2015}
}
@inproceedings{10.1109/ICSE.2019.00057,
abstract = {In many software development projects, analysts are required to deal with systems' requirements from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents. Accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario. Our approach is to mine real-time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation. We propose to use both generative and discriminating methods. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modelling of natural language processing tasks. We used an extended version of Support Vector Machines (SVMs) with variable-sized feature vectors to efficiently and dynamically extract and classify requirements-related knowledge from the existing documents. To evaluate the performance of our approach intuitively and quantitatively, we used edit distance and precision/recall metrics. We show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate. Furthermore, we found that statistical and linguistic parameters such as smoothing methods, and words contiguity and order features can impact the performance of both extraction and classification tasks.},
author = {{Shakeri Hossein Abad}, Zahra and Gervasi, Vincenzo and Zowghi, Didar and Far, Behrouz H.},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2019.00057},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/shakeri Supporting Analysts by Dynamic Extraction and Classification of Requirements-Related Knowledge.pdf:pdf},
isbn = {9781728108698},
issn = {02705257},
keywords = {Dynamic language models,Information extraction,Natural Language Processing,Requirements classification,Requirements elicitation,Weighted Finite State Transducer,acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {442--453},
publisher = {IEEE Press},
series = {ICSE '19},
title = {{Supporting Analysts by Dynamic Extraction and Classification of Requirements-Related Knowledge}},
url = {https://doi.org/10.1109/ICSE.2019.00057},
volume = {2019-May},
year = {2019}
}
@inproceedings{10.1145/2647908.2655966,
abstract = {Adoption of SPLE techniques is challenging and expensive. Hence, automation in the adoption process is desirable, especially with respect to variability management. Different methods have been suggested for (semi-)automatically generating feature models from requirements or textual descriptions of products. However, while there are different ways to represent the same SPL in feature models, addressing different stakeholders' needs and preferences, existing methods usually follow fixed, predefined ways to generate feature models. As a result, the generated feature models may represent perspectives less relevant to the given tasks. In this paper we suggest an ontological approach that measures the semantic similarity, extracts variability, and automatically generates feature models that represent structural (objects-related) or functional (actions-related) perspectives. The stakeholders are able to control the perspective of the generated feature models, considering their needs and preferences for given tasks. Copyright 2014 ACM.},
address = {New York, NY, USA},
author = {Itzik, Nili and Reinhartz-Berger, Iris},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2647908.2655966},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Itzik, Nili and Reinhartz-Berger, Iris{\_} {\_}{\_}Generating Feature Models from Requirements{\_} Structural vs. Functional Perspectives{\_}{\_} (2014).pdf:pdf},
isbn = {9781450327398},
keywords = {Feature models,Mining,Ontology,Reverse engineering,Semantic similarity,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {44--51},
publisher = {Association for Computing Machinery},
series = {SPLC '14},
title = {{Generating Feature Models from Requirements: Structural vs. Functional Perspectives}},
url = {https://doi.org/10.1145/2647908.2655966},
volume = {2},
year = {2014}
}
@inproceedings{8054883,
abstract = {In the software process, unresolved natural language (NL) ambiguities in the early requirements phases may cause problems in later stages of development. Although methods exist to detect domain-independent ambiguities, ambiguities are also influenced by the domain-specific background of the stakeholders involved in the requirements process. In this paper, we aim to estimate the degree of ambiguity of typical computer science words (e.g., system, database, interface) when used in different application domains. To this end, we apply a natural language processing (NLP) approach based on Wikipedia crawling and word embeddings, a novel technique to represent the meaning of words through compact numerical vectors. Our preliminary experiments, performed on five different domains, show promising results. The approach allows an estimate of the variation of meaning of the computer science words when used in different domains. Further validation of the method will indicate the words that need to be carefully defined in advance by the requirements analyst to avoid misunderstandings when editing documents and dealing with experts in the considered domains.},
author = {Ferrari, Alessio and Donati, Beatrice and Gnesi, Stefania},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.20},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ferrari, Alessio and Donati, Beatrice and Gnesi, Stefania{\_} {\_}{\_}Detecting domain-specific ambiguities{\_} An NLP approach based on wikipedia crawling and word embeddings{\_}{\_} (2017).pdf:pdf},
isbn = {9781538634882},
keywords = {formal specification,formal verification,ieee{\_}inc{\_}nlp{\_}x{\_}re,informati,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {393--399},
title = {{Detecting domain-specific ambiguities: An NLP approach based on wikipedia crawling and word embeddings}},
year = {2017}
}
@conference{Lash2012541,
abstract = {In the design process, the requirements serve as the benchmark for the entire product. Therefore, the quality of requirement statements is essential to the success of a design. Because of their ergonomic-nature, most requirements are written in natural language (NL). However, writing requirements in natural language presents many issues such as ambiguity, specification issues, and incompleteness. Therefore, identifying issues in requirements involves analyzing these NL statements. This paper presents a linguistic approach to requirement analysis, which utilizes grammatical elements of requirements statements to identify requirement statement issues. These issues are organized by the entity{\^{a}}word, sentence, or document{\^{a}}that they affect. The field of natural language processing (NLP) provides a core set of tools that can aid with this linguistic analysis and provide a method to create a requirement analysis support tool. NLP addresses requirements on processing levels: lexical, syntactic, semantic, and pragmatic. While processing on the lexical and syntactic level are well-defined, mining semantic and pragmatic data is performed in a number of different methods. This paper provides an overview of these current requirement analysis methods in light of the presented linguistic approach. This overview will be used to identify areas for further research and development. Finally, a prototype requirement analysis support tool will be presented. This tool seeks to demonstrate how the semantic processing level can begin to be addressed in requirement analysis. The tool will analyze a sample set of requirements from a family of military tactical vehicles (FMTV) requirements document. It implements NLP tools to semantically compare requirements statements based upon their grammatical subject. Copyright {\textcopyright} 2012 by ASME.},
annote = {cited By 4},
author = {Lash, Alex and Murray, Kevin and Mocko, Gregory},
booktitle = {Proceedings of the ASME Design Engineering Technical Conference},
doi = {10.1115/DETC2012-71084},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Lash, Alex and Murray, Kevin and Mocko, Gregory{\_} {\_}{\_}Natural language processing applications in requirements engineering{\_}{\_} (2012).pdf:pdf},
isbn = {9780791845011},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {PARTS A AND B},
pages = {541--549},
title = {{Natural language processing applications in requirements engineering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884646680{\&}doi=10.1115{\%}2FDETC2012-71084{\&}partnerID=40{\&}md5=29930c2cbc1b0693f88e6f2844485e6c},
volume = {2},
year = {2012}
}
@incollection{junqueira_barbosa_analysis_2014,
abstract = {As a novel market data, online reviews can manifest user demands in real contexts of use. Thereby, this paper proposes a demand-centered approach for requirements evolution by mining and analyzing online reviews. In our approach, it is challenging to improve the accuracy of opinion mining techniques for huge volume of noisy review data. Furthermore, how to quantitatively evaluate the economic impact of user opinions for determining candidate requirements changes is also a challenging problem. In this paper, an opinion mining method augmented with noise pruning techniques is presented to automatically extract user opinions. After automatic synthesizing the information extracted, a utility-oriented econometric model is employed to find causal influences between the system aspects frequently mentioned in user opinions and common user demands for revising current requirements. A case study shows that the presented method of opinion mining achieves good precision and recall even if there is a large amount of noisy review data. The case study also validates the effectiveness of our approach that it discovers the candidate requirements changes related to the software revenue, especially the ones that are ignored by software developers. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
address = {Berlin, Heidelberg},
annote = {Series Title: Communications in Computer and Information Science},
author = {Jiang, Wei and Ruan, Haibin and Zhang, Li},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-662-43610-3_4},
editor = {{Junqueira Barbosa}, Simone Diniz and Chen, Phoebe and Cuzzocrea, Alfredo and Du, Xiaoyong and Filipe, Joaquim and Kara, Orhun and Kotenko, Igor and Sivalingam, Krishna M and {\'{S}}l{\c{e}}zak, Dominik and Washio, Takashi and Yang, Xiaokang and Zowghi, Didar and Jin, Zhi},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/jiang2014 Analysis of Economic Impact of Online Reviews An Approach for Market-Driven Requirements Evolution.pdf:pdf},
isbn = {9783662436097},
issn = {18650929},
keywords = {Electronic market,econometric analysis,online reviews,opinion mining,requirements evolution,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {45--59},
publisher = {Springer Berlin Heidelberg},
shorttitle = {Analysis of {\{}Economic{\}} {\{}Impact{\}} of {\{}Online{\}} {\{}Revie}},
title = {{Analysis of Economic Impact of Online Reviews: An Approach for Market-Driven Requirements Evolution}},
url = {http://link.springer.com/10.1007/978-3-662-43610-3{\_}4},
volume = {432 CCIS},
year = {2014}
}
@inproceedings{10.1145/3299771.3299774,
abstract = {Use case modeling refers to the process of identifying scenarios written in some natural language text, particularly to capture interactions between the system and associated actors. Several approaches have been proposed to maintain the synergy of use cases with other software models, but no systematic transformation approach is available to extract use case scenarios from the textual requirements specification. In this paper, we propose a systematic transformation approach that automatically extracts various use case elements from textual problem specifications. The approach uses Natural Language (NL) parser to identify Parts-Of-Speech (POS) tags, Type Dependencies (TDs) and semantic roles from the input text specification to populate use case elements. It further makes use of the questionnaire-based approach to develop the remaining unpopulated parts of the use case template. The paper demonstrates the applicability of the proposed approach by applying both industry and research-level case studies. The results highlight that the generated output is correct, consistent, non-redundant and complete, and helpful to use case developers in further analysis and documentation.},
address = {New York, NY, USA},
author = {Tiwari, Saurabh and Ameta, Deepti and Banerjee, Asim},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3299771.3299774},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Tiwari, Saurabh and Ameta, Deepti and Banerjee, Asim{\_} {\_}{\_}An approach to identify use case scenarios from textual requirements specification{\_}{\_} (2019).pdf:pdf},
isbn = {9781450362153},
keywords = {Case study,Natural Language Processing (NLP),Questionnaire based analysis,Tool support,Use case modeling,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ISEC'19},
title = {{An approach to identify use case scenarios from textual requirements specification}},
url = {https://doi.org/10.1145/3299771.3299774},
year = {2019}
}
@article{8323229,
abstract = {A fundamental management responsibility is securing information systems. Almost all applications that deal with safety, privacy, or defense include some form of access control. There are a plethora of access control models in the information security realm such as role-based access control and attribute-based access control. However, the initial development of access control policies (ACPs) can be very challenging. Most organizations have high-level requirement specifications that include a set of ACPs, which describe allowable operations of the system. It is time consuming and error-prone to manually sift through these documents and extract ACPs. In this paper, we propose a new framework towards extracting ACPs from unrestricted natural language documents using semantic role labeling (SRL). We were able to correctly identify ACP elements with an average {\textless}formula{\textgreater}{\textless}tex{\textgreater}{\$}F{\_}1{\$}{\textless}/tex{\textgreater}{\textless}/formula{\textgreater} score of 75{\%}, which bested the previous work by 15{\%}. Furthermore, as SRL tools are often trained on publicly available corpora such as Wall Street Journal, we investigated the idea of improving SRL performance using domain-related knowledge. We utilized domain adaptation and semi-supervised learning techniques and were able to improve the SRL performance by 2{\%} using only a small amount of access control data.},
author = {Narouei, Masoud and Takabi, Hassan and Nielsen, Rodney},
doi = {10.1109/TDSC.2018.2818708},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/narouei Automatic Extraction of Access Control Policies from Natural Language Documents.pdf:pdf},
issn = {15455971},
journal = {IEEE Transactions on Dependable and Secure Computing},
keywords = {Access control,Access control policy,Natural languages,Organizations,Permission,Privacy,Semantics,Tools,domain adaptation,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,policy engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,semantic role labeling,semi-supervised learning,transfer learning},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {506--517},
title = {{Automatic Extraction of Access Control Policies from Natural Language Documents}},
volume = {17},
year = {2018}
}
@inproceedings{8054850,
abstract = {Usability and user experience (UUX) strongly affect software quality and success. User reviews allow software users to report UUX issues. However, this information can be difficult to access due to the varying quality of the reviews, its large numbers and unstructured nature. In this work we propose an approach to automatically detect the UUX strengths and issues of software features according to user reviews. We use a collocation algorithm for extracting the features, lexical sentiment analysis for uncovering users' satisfaction about a particular feature and machine learning for detecting the specific UUX issues affecting the software application. Additionally, we present two visualizations of the results. An initial evaluation of the approach against human judgement obtained mixed results.},
author = {Bakiu, Elsa and Guzman, Emitza},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.76},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bakiu, Elsa and Guzman, Emitza{\_} {\_}{\_}Which feature is unusable{\_} Detecting usability and user experience issues from user reviews{\_}{\_} (2017).pdf:pdf},
isbn = {9781538634882},
keywords = {Software evolution,Text mining,Usability,User experience,User feedback,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {182--187},
title = {{Which feature is unusable? Detecting usability and user experience issues from user reviews}},
year = {2017}
}
@inproceedings{6894851,
abstract = {For specifications, people use natural language. We show that processing natural language and combining this with intelligent deduction and reasoning with ontologies can possibly replace some manual processes associated with requirements engineering (RE). Our prior research shows that the software tools we developed can indeed solve problems in the RE process. This paper shows this does not only work in the software engineering domain, but also for embedded software in the automotive industry. We use artificial intelligence in the sense of combining semantic knowledge from ontologies and natural language processing. This enables computer systems to 'understand' requirement texts and process these with 'common sense'. Our specification improver RESI detects flaws in texts such as ambiguous words, incomplete process words, and erroneous quantifiers and determiners.},
author = {K{\"{o}}rner, Sven J. and Landh{\"{a}}u{\ss}er, Mathias and Tichy, Walter F.},
booktitle = {2014 IEEE 1st International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2014 - Proceedings},
doi = {10.1109/AIRE.2014.6894851},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/K{\_}{\_}{\_}{\_}o{\_}{\_}rner, Sven J. and Landh{\_}{\_}{\_}{\_}a{\_}{\_}u{\_}{\_}ss{\_}er, Mathias and Tichy, Walter F.{\_} {\_}{\_}Transferring research into the real world{\_} How to improve RE with AI in the automotive industry{\_}{\_} (2014).pdf:pdf},
isbn = {9781479963553},
keywords = {artificial intelligence,automobile industry,ieee{\_}inc{\_}nlp{\_}x{\_}re,natura,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {13--18},
title = {{Transferring research into the real world: How to improve RE with AI in the automotive industry}},
year = {2014}
}
@inproceedings{8843470,
abstract = {Clustering plays an important role in reusable requirements retrieval from the ever-growing software project repositories. The literature on requirements cluster labeling is still emerging. Researchers have investigated clustering to support various software engineering activities such as requirements prioritization, feature identification, automated tracing, and code navigation. The primary task in analyzing the clustering results is to 'label' the clusters by means of some representative words to summarize and comprehend the requirements data. Despite the development of automatic cluster labeling techniques for software requirements, very little is understood about enhancing the cluster labels using external knowledge sources such as Wikipedia. In this paper, we review the literature on enhancing cluster labeling, present a framework for requirements cluster labeling and conduct an experiment to evaluate how the Wikipedia-based enhancement performs in labeling requirements clusters. The results show that Wikipedia-based labeling outperforms traditional Information Retrieval (IR) techniques. Our work sheds light on improving automated ways to support information reuse and management in the context of requirements engineering (RE).},
author = {Reddivari, Sandeep},
booktitle = {Proceedings - 2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science, IRI 2019},
doi = {10.1109/IRI.2019.00031},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Reddivari, Sandeep{\_} {\_}{\_}Enhancing software requirements cluster labeling using wikipedia{\_}{\_} (2019).pdf:pdf},
isbn = {9781728113371},
keywords = {Clustering,Information retrieval,Information reuse,Labeling,Requirements engineering,Reuse,Wikipedia,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {123--126},
title = {{Enhancing software requirements cluster labeling using wikipedia}},
year = {2019}
}
@article{landhauser_requirements_2014,
abstract = {Software engineering is supposed to be a structured process, but manual tasks leave much leeway. Ideally, these tasks lie in the hands of skilled analysts and software engineers. This includes creating the textual specification of the envisioned system as well as creating models for the software engineers. Usually, there is quite a bit of erosion during the process due to requirement changes, implementation decisions, etc. To deliver the software as specified, textual requirements, models, and the actual software need to be synchronized. However, in practice, the cost of manually maintaining consistency is too high. Our requirements engineering feedback system automates the process of keeping textual specification and models consistent when the models change. To improve overall processing of natural language specifications, our approach finds flaws in natural language specifications. In addition to the already published workshop paper, we show how well our tools support even non-software-engineers in improving texts. The case studies show that we can speed up the process of creation texts with fewer flaws significantly. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Landh{\"{a}}u{\ss}er, Mathias and K{\"{o}}rner, Sven J. and Tichy, Walter F.},
doi = {10.1007/s11219-013-9210-6},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/landhauser From requirements to UML models and back How automatic processing of text can support requirements engineering.pdf:pdf},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Modeling,Natural language specification,Ontology,UML,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {1},
pages = {121--149},
shorttitle = {From requirements to {\{}UML{\}} models and back},
title = {{From requirements to UML models and back: How automatic processing of text can support requirements engineering}},
url = {http://link.springer.com/10.1007/s11219-013-9210-6},
volume = {22},
year = {2014}
}
@article{merugu_automated_2019,
abstract = {The scale of software is growing rapidly for organizations begin to deploy their business on internet. It is a need of avoid ambiguity between engineers and users and to avoid mistakes in software requirements. And provide automatic requirement analysis techniques for modeling and analyzing requirements formally and save manpower. In this paper proposed cloud service method for automated detection of quality requirement in software requirement specification. This paper also present novel approach for process of automatic classification of software quality requirements based on supervised machine learning technique applied for the classification of training document and predict target document software quality requirements.},
author = {Merugu, R. Raja Ramesh and Chinnam, Satyananda Reddy},
doi = {10.1007/s12065-019-00241-6},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Merugu, R. Raja Ramesh and Chinnam, Satyananda Reddy{\_} {\_}{\_}Automated cloud service based quality requirement classification for software requirement specification{\_}{\_} (2019).pdf:pdf},
issn = {18645917},
journal = {Evolutionary Intelligence},
keywords = {Automated,Cloud service,Quality requirement,Software requirements,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Automated cloud service based quality requirement classification for software requirement specification}},
url = {http://link.springer.com/10.1007/s12065-019-00241-6},
year = {2019}
}
@incollection{pham_semantic_2014,
abstract = {Many attempts have been made to apply Natural Language Processing to requirements specifications. However, typical approaches rely on shallow parsing to identify object-oriented elements of the specifications (e.g. classes, attributes, and methods). As a result, the models produced are often incomplete, imprecise, and require manual revision and validation. In contrast, we propose a deep Natural Language Understanding approach to create complete and precise formal models of requirements specifications. We combine three main elements to achieve this: (1) acquisition of lexicon from a user-supplied glossary requiring little specialised prior knowledge; (2) flexible syntactic analysis based purely on word-order; and (3) Knowledge-based Configuration unifies several semantic analysis tasks and allows the handling of ambiguities and errors. Moreover, we provide feedback to the user, allowing the refinement of specifications into a precise and unambiguous form. We demonstrate the benefits of our approach on an example from the PROMISE requirements corpus.},
address = {Cham},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Selway, Matt and Mayer, Wolfgang and Stumptner, Markus},
booktitle = {{\{}PRICAI{\}} 2014: {\{}Trends{\}} in {\{}Artificial{\}} {\{}Intelligence{\}}},
doi = {10.1007/978-3-319-13560-1_40},
editor = {Pham, Duc-Nghia and Park, Seong-Bae},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/W4STSAHY/Selway e.a. - 2014 - Semantic Interpretation of Requirements through Co.pdf:pdf},
isbn = {978-3-319-13559-5 978-3-319-13560-1},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {496--510},
publisher = {Springer International Publishing},
title = {{Semantic Interpretation of Requirements through Cognitive Grammar and Configuration}},
url = {http://link.springer.com/10.1007/978-3-319-13560-1{\_}40},
volume = {8862},
year = {2014}
}
@article{Arruda2019506,
abstract = {Goal-Oriented Requirements Engineering approaches, in which the KAOS framework plays a key role, have been widely used for eliciting software requirements because they provide an easier way of communicating among stakeholders. However, the goal-oriented requirements modeling is not an easy way for novice requirements engineers. These professionals need more support in creating KAOS models. Recent studies have focused on the applicability of Artificial Intelligence techniques (e.g., Natural Language Processing – NLP) to support Requirements Engineering activities. In this sense, this paper aims to describe a way to support requirements elicitation for novice requirements engineers through the use of NLP within a chatbot. The chatbot (KAOSbot) acts as a KAOS modeling assistant. To evaluate our hypotheses about perceived efficacy from the novice requirements engineers perspective, we performed a quasi-experiment concerning KAOSbot's perceived ease of use, perceived usefulness and intention to use. The results show that KAOSbot tool is a promising approach for specifying KAOS models because it was perceived as easy to use, useful, and the participants intend to use it in the future.},
annote = {cited By 0},
author = {Arruda, Danilo and Marinho, Matheus and Souza, Eric and Wanderley, Fernando},
doi = {10.1007/978-3-030-24305-0_38},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Arruda, Danilo and Marinho, Matheus and Souza, Eric and Wanderley, Fernando{\_} {\_}{\_}A Chatbot for Goal-Oriented Requirements Modeling{\_}{\_} (2019).pdf:pdf},
isbn = {9783030243043},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Chatbot,Goal-Oriented Requirements Engineering,KAOS modeling,Natural Language Processing,Quasi-experiment,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {506--519},
title = {{A Chatbot for Goal-Oriented Requirements Modeling}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068606397{\&}doi=10.1007{\%}2F978-3-030-24305-0{\_}38{\&}partnerID=40{\&}md5=057afa5bcd65f031d936d1a4f52ff004},
volume = {11622 LNCS},
year = {2019}
}
@inproceedings{10.1145/3325730.3325757,
abstract = {Deciphering human language by Requirement Analysts is the key issue in Software Development. Clients communicate their software requirements in raw form. In this paper, we are presenting certain techniques of Natural Language processing which work out greatly to extract information properly and minimizing the bugs that may generate in later parts of Software Development. In today's era, the latest technological development in Artificial Intelligence has enabled machines to process the text to a certain level. Natural Language understanding is so far the most critical problem; the Software community is facing today in requirements gathering. In this study, using the techniques of Natural Language Interpretation, Testers and Software Developers can chalk out the more exact requirements from customers which can improve the Quality of Software to a certain level.},
address = {New York, NY, USA},
author = {Memon, Kamran Ali and Xiaoling, Xia},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3325730.3325757},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Memon, Kamran Ali and Xiaoling, Xia{\_} {\_}{\_}Deciphering and analyzing software requirements employing the techniques of Natural Language processing{\_}{\_} (2019).pdf:pdf},
isbn = {9781450362580},
keywords = {Linguistics,NLP techniques,Natural language processing,Natural language understanding,Software requirements,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {153--156},
publisher = {Association for Computing Machinery},
series = {ICMAI 2019},
title = {{Deciphering and analyzing software requirements employing the techniques of Natural Language processing}},
url = {https://doi.org/10.1145/3325730.3325757},
year = {2019}
}
@article{Terawaki201158,
abstract = {This paper describes a method to ensure the quality requirements from service receiver in the requirement definition phase of system development. The proposed method measures the quality characteristics that are in the requirement document using the text-mining technique and concept dictionary and identifies requirements of document with quality characteristics of the International Standards Organization (ISO) / the International Electrical technical Commission (IEC) 9126-1:2001[1]. The case study shows that the quality characteristics are contained in the requirements document. {\textcopyright} 2011 Springer-Verlag.},
annote = {cited By 3},
author = {Terawaki, Yuki},
doi = {10.1007/978-3-642-21793-7_7},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Terawaki, Yuki{\_} {\_}{\_}Supporting of requirements elicitation for ensuring services of information systems used for education{\_}{\_} (2011).pdf:pdf},
isbn = {9783642217920},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {ISO/IEC 9126,Non-Functional Requirements,RE,Requirements Engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,text-mining approach},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {PART 1},
pages = {58--65},
title = {{Supporting of requirements elicitation for ensuring services of information systems used for education}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960331399{\&}doi=10.1007{\%}2F978-3-642-21793-7{\_}7{\&}partnerID=40{\&}md5=bd89ae3a42789dd50360936d4b217c6c},
volume = {6771 LNCS},
year = {2011}
}
@inproceedings{8054828,
abstract = {The design of large-scale complex systems requires their analysis from multiple perspectives, often through the use of requirements models. Diversely located experts with different backgrounds (e.g., safety, security, performance) create such models using different requirements modeling languages. One open challenge is how to align these models such that they cover the same parts of the domain. We propose a technique based on natural language processing (NLP) that analyzes several models included in a project and provides suggestions to modelers based on what is represented in the models that analyze other concerns. Unlike techniques based on meta-model alignment, ours is flexible and language agnostic. We report the results of a focus group session in which experts from the air traffic management domain discussed our approach.},
author = {Aydemir, Fatma Başak and Dalpiaz, Fabiano},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
doi = {10.1109/REW.2017.82},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Aydemir, Fatma Başak and Dalpiaz, Fabiano{\_} {\_}{\_}Towards aligning multi-concern models via NLP{\_}{\_} (2017).pdf:pdf},
isbn = {9781538634882},
keywords = {Alignment,Collaborative modeling,Model management,Natural language processing,Requirements models,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {46--50},
title = {{Towards aligning multi-concern models via NLP}},
year = {2017}
}
@inproceedings{7516136,
abstract = {In this paper, we aim to cover works that are related to the process of transforming requirements into UML diagrams, from the first works which were manual techniques in 1976, to automatic tools in 2015. In this context, we try to exhibit different approaches and to indicate their strength as well as their shortcomings. This work will help us to evaluate existing approaches and propose other alternatives for Requirement Engineering. The objective of this paper is to present an overview of various works dedicated to requirement analysis and a comparative study of these works. Also, we tried to discuss the combination of Artificial Intelligence with Requirement Engineering.},
author = {Abdouli, Mariem and Karaa, Wahiba Ben Abdessalem and Ghezala, Henda Ben},
booktitle = {2016 IEEE/ACIS 14th International Conference on Software Engineering Research, Management and Applications, SERA 2016},
doi = {10.1109/SERA.2016.7516136},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Abdouli, Mariem and Karaa, Wahiba Ben Abdessalem and Ghezala, Henda Ben{\_} {\_}{\_}Survey of works that transform requirements into UML diagrams{\_}{\_} (2016).pdf:pdf},
isbn = {9781509008094},
keywords = {UML diagrams,artificial intelligence,ieee{\_}inc{\_}nlp{\_}x{\_}re,learning,requirements,state of the art,transformation},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {117--123},
title = {{Survey of works that transform requirements into UML diagrams}},
year = {2016}
}
@inproceedings{6360073,
abstract = {Requirements Engineering (RE) is about achieving a shared understanding about the software system to be built. No withstanding the importance of other RE activities, requirements specification deserves special attention due to its documentation purposes: to communicate requirements, someone has to write them down. In this paper we present RSLingo, a linguistic approach for improving the quality of requirements specifications, which is based on two languages and the mapping between them: RSL-PL, an extensible language for dealing with information extraction from requirements written in natural language; and RSL-IL, a formal language with a fixed set of constructs for representing and conveying RE-specific concerns. Contrarily to other approaches, this decoupling allows one to deal with requirements as "white-box" items, enabling a deeper understanding at a semantic level. Thus, RSLingo enables the automation of some verification tasks that prevent common requirements quality problems and lays the foundation to better integrate RE with the Model-Driven Engineering paradigm through transformations of requirements representations into design models. {\textcopyright} 2012 IEEE.},
author = {{De Almeida Ferreira}, David and {Da Silva}, Alberto Rodrigues},
booktitle = {2012 2nd IEEE International Workshop on Model-Driven Requirements Engineering, MoDRE 2012 - Proceedings},
doi = {10.1109/MoDRE.2012.6360073},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/{\_}De Almeida Ferreira{\_}, David and {\_}Da Silva{\_}, Alberto Rodrigues{\_} {\_}{\_}RSLingo{\_} An information extraction approach toward formal requirements specifications{\_}{\_} (2012).pdf:pdf},
isbn = {9781467343893},
keywords = {Information Extraction,Requirements Modeling,Requirements Specification Language,Transformations,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {39--48},
title = {{RSLingo: An information extraction approach toward formal requirements specifications}},
year = {2012}
}
@article{pruski_tiqi_2015,
abstract = {Software traceability is a required element in the development and certification of safety-critical software systems. However, trace links, which are created at significant cost and effort, are often underutilized in practice due primarily to the fact that project stakeholders often lack the skills needed to formulate complex trace queries. To mitigate this problem, we present a solution which transforms spoken or written natural language queries into structured query language (SQL). TiQi includes a general database query mechanism and a domain-specific model populated with trace query concepts, project-specific terminology, token disambiguators, and query transformation rules. We report results from four different experiments exploring user preferences for natural language queries, accuracy of the generated trace queries, efficacy of the underlying disambiguators, and stability of the trace query concepts. Experiments are conducted against two different datasets and show that users have a preference for written NL queries. Queries were transformed at accuracy rates ranging from 47 to 93 {\%}.},
author = {Pruski, Piotr and Lohar, Sugandha and Goss, William and Rasin, Alexander and Cleland-Huang, Jane},
doi = {10.1007/s00766-015-0224-4},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Pruski, Piotr and Lohar, Sugandha and Goss, William and Rasin, Alexander and Cleland-Huang, Jane{\_} {\_}{\_}TiQi{\_} Answering unstructured natural language trace queries{\_}{\_} (2015).pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Natural language processing,Queries,Speech recognition,Traceability,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {sep},
number = {3},
pages = {215--232},
shorttitle = {TiQi},
title = {{TiQi: Answering unstructured natural language trace queries}},
url = {http://link.springer.com/10.1007/s00766-015-0224-4},
volume = {20},
year = {2015}
}
@inproceedings{6894853,
abstract = {Assuring quality in software development processes is often a complex task. In many cases there are numerous needs which cannot be fulfilled with the limited resources given. Consequently it is crucial to identify the set of necessary requirements for a software project which needs to be complete and conflict-free. Additionally, the evolution of single requirements (artifacts) plays an important role because the quality of these artifacts has an impact on the overall quality of the project. To support stakeholders in mastering these tasks there is an increasing interest in AI techniques. In this paper we presents two content-based recommendation approaches that support the Requirements Engineering (RE) process. First, we propose a Keyword Recommender to increase requirements reuse. Second, we define a thesaurus enhanced Dependency Recommender to help stakeholders finding complete and conflict-free requirements. Finally, we present studies conducted at the Graz University of Technology to evaluate the applicability of the proposed recommendation technologies.},
author = {Ninaus, Gerald and Reinfrank, Florian and Stettinger, Martin and Felfernig, Alexander},
booktitle = {2014 IEEE 1st International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2014 - Proceedings},
doi = {10.1109/AIRE.2014.6894853},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ninaus, Gerald and Reinfrank, Florian and Stettinger, Martin and Felfernig, Alexander{\_} {\_}{\_}Content-based recommendation techniques for requirements engineering{\_}{\_} (2014).pdf:pdf},
isbn = {9781479963553},
keywords = {artificial intelligence,formal specification,ieee{\_}inc{\_}nlp{\_}x{\_}re,quali},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {27--34},
title = {{Content-based recommendation techniques for requirements engineering}},
year = {2014}
}
@inproceedings{7365796,
abstract = {End-users play an integral role in identifying requirements, validating software features' usefulness, locating defects, and in software product evolution in general. Their role in these activities is especially prominent in online application distribution platforms (OADPs), where software is developed for many potential users, and for which the traditional processes of requirements gathering and negotiation with a single group of end-users do not apply. With such vast access to end-users, however, comes the challenge of how to prioritize competing requirements in order to satisfy previously unknown user groups, especially with early releases of a product. One highly successful product that has managed to overcome this challenge is the Android Operating System (OS). While the requirements of early versions of the Android OS likely benefited from market research, new features in subsequent releases appear to have benefitted extensively from user reviews. Thus, lessons learned about how Android developers have managed to satisfy the user community over time could usefully inform other software products. We have used data mining and natural language processing (NLP) techniques to investigate the issues that were logged by the Android community, and how Google's remedial efforts correlated with users' requests. We found very strong alignment between end-users' top feature requests and Android developers' responses, particularly for the more recent Android releases. Our findings suggest that effort spent responding to end-users' loudest calls may be integral to software systems' survival, and a product's overall success.},
author = {Licorish, Sherlock A. and Tahir, Amjed and Bosu, Michael Franklin and MacDonell, Stephen G.},
booktitle = {2015 24th Australasian Software Engineering Conference},
doi = {10.1109/aswec.2015.19},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Licorish, Sherlock A. and Savarimuthu, Bastin Tony Roy and Keertipati, Swetha{\_} {\_}{\_}Attributes that predict which features to fix{\_} Lessons for app store mining{\_}{\_} (2017).pdf:pdf},
issn = {1530-0803},
keywords = {Android (operating system),data mining,ieee{\_}inc{\_}nlp{\_}x{\_}re,mobile comp},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {78--87},
title = {{On Satisfying the Android OS Community: User Feedback Still Central to Developers' Portfolios}},
year = {2015}
}
@inproceedings{10.1145/2554850.2555057,
abstract = {Lately, organizations have been subject to regulation promoting information transparency; one example of this is the Brazilian Information Access Law. This paper presents a novel way of performing requirements elicitation using both the law and a Non-Functional Requirements Patterns catalog as the information sources. Since organizations must follow the law, its information systems must also implement the law as requirements. Our process is guided by pattern matching, text mining and grounded analysis. We examine the special case of the Brazilian Access Law using our approach, which compares a previously encoded transparency knowledge base with the law.},
address = {New York, NY, USA},
author = {Engiel, Priscila and Cappelli, Claudia and {do Prado Leite}, Julio Cesar Sampaio},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
doi = {10.1145/2554850.2555057},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Engiel, Priscila and Cappelli, Claudia and {\_}do Prado Leite{\_}, Julio Cesar Sampaio{\_} {\_}{\_}Eliciting Concepts from the Brazilian Access Law Using a Combined Approach{\_}{\_} (2014).pdf:pdf},
isbn = {9781450324694},
keywords = {NFR framework,acm{\_}inc{\_}nlp{\_}x{\_}re,knowledge extraction,non-functional requirement,requirement pattern,text mining,transparency},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1001--1006},
publisher = {Association for Computing Machinery},
series = {SAC '14},
title = {{Eliciting Concepts from the Brazilian Access Law Using a Combined Approach}},
url = {https://doi.org/10.1145/2554850.2555057},
year = {2014}
}
@article{Mohanan2018,
abstract = {The user's software requirements are represented in natural language or speech language such as English. Translating these requirements into the object oriented models is a tough process for the designers. This paper proposes a neoteric approach to generate Unified Modeling Language (UML) class models instantly from software requirement specifications (SRS). Here we make use of the Open Natural language processing tool (OpenNLP) for lexical analysis and to generate the necessary parts of speech (POS) tags from these requirement specifications. Then Semantics of Business Vocabulary and Rules (SBVR) standard is used to extract the object oriented elements from the natural language (NL) processed SRS. From this, we generate the UML class models. Our prototype tool can generate accurate models in less time. This automated system for designing object oriented models from SRS reduces the cost and budget for both the designers and the users.},
annote = {cited By 0},
author = {Mohanan, Murali and Samuel, Philip},
doi = {10.1142/S0218213018500276},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Mohanan, Murali and Samuel, Philip{\_} {\_}{\_}Natural Language Processing Approach for UML Class Model Generation from Software Requirement Specifications via SBVR{\_}{\_} (2018).pdf:pdf},
issn = {17936349},
journal = {International Journal on Artificial Intelligence Tools},
keywords = {Requirement analysis,UML class diagrams,natural language processing,object oriented modeling,scopus{\_}inc{\_}nlp{\_}x{\_}re,software requirement specification},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {6},
title = {{Natural Language Processing Approach for UML Class Model Generation from Software Requirement Specifications via SBVR}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054319545{\&}doi=10.1142{\%}2FS0218213018500276{\&}partnerID=40{\&}md5=e2364ba79be26c866389cbd7a8ab22e9},
volume = {27},
year = {2018}
}
@inproceedings{9006086,
abstract = {Over the past few years, we have worked on pioneering an approach that employs Commonsense Knowledge (CSK) to automate the identification of Implicit Requirements (IMRs) from text in large Software Requirements Specifications (SRS) documents. This paper builds on our IMR-identification approach by adding CNN-based deep learning to detect IMRs from complex SRS big data such as images and tables.},
author = {Onyeka, Emebo and Anu, Vaibhav and Varde, Aparna S.},
booktitle = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
doi = {10.1109/BigData47090.2019.9006086},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/emeboIdentifying Implicit Requirements in SRS Big Data.pdf:pdf},
isbn = {9781728108582},
keywords = {Commonsense Knowledge,Domain Ontology,IMRs,Requirements Engineering,Text Mining,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {6169--6171},
title = {{Identifying Implicit Requirements in SRS Big Data}},
year = {2019}
}
@incollection{moreira_aspect_2013,
abstract = {This chapter presents a methodology for identification of crosscutting concerns in textual requirements along with its supporting tool EA-Miner. This chapter discusses how EA-Miner uses natural language processing techniques in aspect identification and structuring using a requirements level feature model as an example. The process is illustrated using the Car Crash case study.},
address = {Berlin, Heidelberg},
author = {Weston, Nathan and Chitchyan, Ruzanna and Sampaio, Americo and Rashid, Awais and Greenwood, Phil},
booktitle = {Aspect-Oriented Requirements Engineering},
doi = {10.1007/978-3-642-38640-4_1},
editor = {Moreira, Ana and Chitchyan, Ruzanna and Ara{\'{u}}jo, Jo{\~{a}}o and Rashid, Awais},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Weston, Nathan and Chitchyan, Ruzanna and Sampaio, Americo and Rashid, Awais and Greenwood, Phil{\_} {\_}{\_}Aspect identification in textual requirements with EA-miner{\_}{\_} (2013).pdf:pdf},
isbn = {9783642386404},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {3--21},
publisher = {Springer Berlin Heidelberg},
title = {{Aspect identification in textual requirements with EA-miner}},
url = {http://link.springer.com/10.1007/978-3-642-38640-4{\_}1},
volume = {9783642386},
year = {2013}
}
@inproceedings{10.1145/1985394.1985396,
abstract = {There is a gap between system requirements described with natural language and system design models described with formal language. In this paper, we present a framework for automatically mapping textual use cases to service component models from a model-based point of view. The generated models capture service component signatures and language independent dynamic behaviors. We have implemented our framework and demonstrated the benefits via a case study. {\textcopyright} 2011 ACM.},
address = {New York, NY, USA},
author = {Ding, Zuohua and Jiang, Mingyue and Palsberg, Jens},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/1985394.1985396},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ding, Zuohua and Jiang, Mingyue and Palsberg, Jens{\_} {\_}{\_}From textual use cases to service component models{\_}{\_} (2011).pdf:pdf},
isbn = {9781450305914},
issn = {02705257},
keywords = {Model transformation,Natural language processing,Service component model,System requirement,Textual use case,acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {8--14},
publisher = {Association for Computing Machinery},
series = {PESOS '11},
title = {{From textual use cases to service component models}},
url = {https://doi.org/10.1145/1985394.1985396},
year = {2011}
}
@article{Jurkiewicz2015110,
abstract = {Context: Use cases are a popular method of expressing functional requirements. One contains a main scenario and a set of extensions, each consisting of an event and an alternative sequence of activities. Events omitted in requirements specification can lead to rework. Unfortunately, as it follows from the previous research, manual identification of events is rather ineffective (less than 1/3 of events are identified) and it is slow. Objective: The goal of this paper is to propose an automatic method of identification of events in use cases and evaluate its quality. Method: Each step of a main scenario is analyzed by a sequence of NLP tools to identify its performer, activity type and information object. It has been observed that performer, activity type and some attributes of information objects determine types of events that can occur when that activity is performed. That empirical knowledge is represented as a set of axioms and two inference rules have been proposed which allow to identify types of possible events. For each event type an NLG pattern is proposed which allows to generate description of the event type in natural language. The proposed method was compared with two manual approaches to identification of events: ad hoc and HAZOP-based. Also a kind of Turing test was performed to evaluate linguistic quality of generated descriptions. Results: Accuracy of the proposed method is about 80{\%} (for manual approaches it is less than 1/3) and its speed is about 11 steps/minute (ad hoc approach is 4 times slower, and HAZOP-based approach is 20 times slower). Understandability of the generated event descriptions was not worse than understandability of the descriptions written by humans. Conclusions: The proposed method could be used to enhance contemporary tools for managing use cases.},
annote = {cited By 2},
author = {Jurkiewicz, J. and Nawrocki, J.},
doi = {10.1016/j.infsof.2014.09.011},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Jurkiewicz, J. and Nawrocki, J.{\_} {\_}{\_}Automated events identification in use cases{\_}{\_} (2015).pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Functional requirements,Natural language processing,Requirements engineering,Use cases,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {110--122},
title = {{Automated events identification in use cases}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84913554456{\&}doi=10.1016{\%}2Fj.infsof.2014.09.011{\&}partnerID=40{\&}md5=53aba0132ebde2c70b1ee83218a7cb04},
volume = {58},
year = {2015}
}
@inproceedings{8874920,
abstract = {Understanding interdependency among requirements is one of the success factors in software development. Information on requirements interdependency explicitly and implicitly resided various design artifacts or diagrams. A software requirements specification document is the artifact delivered in the early phase of development. It drives its following development processes. It also contains information on interdependencies among the requirements, such as similar, part-of, and elaborate. This study proposes an approach to model the requirement dependency graph for a software requirements specification document. There is an extraction process for Text Preprocessing, which includes of Tokenization, Stopword Removal, and Stemming. Besides, there is a process of measuring semantic similarity through WS4J (WordNet Similarity for Java). The results of the extraction process, combined with Greedy Algorithms as the optimum value solution approach. Besides, a method for calculating similarity was used through the practices of Wu Palmer and Levenshtein. At the end of this process, Reliability is performed using the Gwet's AC1 formula.},
author = {Priyadi, Yudi and Djunaidy, Arif and Siahaan, Daniel},
booktitle = {2019 1st International Conference on Cybernetics and Intelligent System, ICORIS 2019},
doi = {10.1109/ICORIS.2019.8874920},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Priyadi, Yudi and Djunaidy, Arif and Siahaan, Daniel{\_} {\_}{\_}Requirements Dependency Graph Modeling on Software Requirements Specification Using Text Analysis{\_}{\_} (2019).pdf:pdf},
isbn = {9781728114729},
keywords = {dependency type,ieee{\_}inc{\_}nlp{\_}x{\_}re,reliability,requirements dependency graph,scopus{\_}inc{\_}nlp{\_}x{\_}re,similarity,software requirements specifications,text preprocessing},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {221--226},
title = {{Requirements Dependency Graph Modeling on Software Requirements Specification Using Text Analysis}},
volume = {1},
year = {2019}
}
@article{10.1145/2815021.2815032,
abstract = {Requirements Engineering is one of the most vital activities in the entire Software Development Life Cycle. The success of the software is largely dependent on how well the users' requirements have been understood and converted into appropriate functionalities in the software. Typically, the users convey their requirements in natural language statements that initially appear easy to state. However, being stated in natural language, the statement of requirements often tends to suffer from misinterpretations and imprecise inferences. As a result, the requirements specified thus, may lead to ambiguities in the software specifications. One can indeed find numerous approaches that deal with ensuring precise requirement specifications. Naturally, an obvious approach to deal with ambiguities in natural language software specifications is to eliminate ambiguities altogether i.e. to use formal specifications. However, the formal methods have been observed to be cost-effective largely for the development of mission-critical software. Due to the technical sophistication required, these are yet to be accepted in the mainstream. Hence, the other alternative is to let the ambiguities exist in the natural language requirements but deal with the same using proven techniques viz. using approaches based on machine learning, knowledge and ontology to resolve them. One can indeed find numerous automated and semi-automated tools to resolve specific types of natural language software requirement ambiguities. However, to the best of our knowledge there is no published literature that attempts to compare and contrast the prevalent approaches to deal with ambiguities in natural language software requirements. Hence, in this paper, we attempt to survey and analyze the prevalent approaches that attempt to resolve ambiguities in natural language software requirements. We focus on presenting a state-of-the-art survey of the currently available tools for ambiguity resolution. The objective of this paper is to disseminate, dissect and analyze the research work published in the area, identify metrics for a comparative evaluation and eventually do the same. At the end, we identify open research issues with an aim to spark new interests and developments in this field.},
address = {New York, NY, USA},
author = {Shah, Unnati S. and Jinwala, Devesh C.},
doi = {10.1145/2815021.2815032},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Shah, Unnati S. and Jinwala, Devesh C.{\_} {\_}{\_}Resolving Ambiguities in Natural Language Software Requirements{\_}{\_} (2015).pdf:pdf},
issn = {0163-5948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {Ambiguity,Natural Language Processing,Requirements Engineering,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
month = {sep},
number = {5},
pages = {1--7},
publisher = {Association for Computing Machinery},
title = {{Resolving Ambiguities in Natural Language Software Requirements}},
url = {https://doi.org/10.1145/2815021.2815032},
volume = {40},
year = {2015}
}
@conference{Li2015582,
abstract = {Requirements engineering is crucial for software projects, but formal requirements engineering is often ignored in scientific software projects. Scientists do not often see the benefit of directing their time and effort towards documenting requirements. Additionally, there is a lack of requirements engineering knowledge amongst scientists who develop software. We aim at helping scientists to easily recover and reuse requirements without acquiring prior requirements engineering knowledge. We apply an automated approach to extract requirements for scientific software from available knowledge sources, such as user manuals and project reports. The approach employs natural language processing techniques to match defined patterns in input text. We have evaluated the approach in three different scientific domains, namely seismology, building performance and computational fluid dynamics. The evaluation results show that 78-97{\%} of the extracted requirement candidates are correctly extracted as early requirements.},
annote = {cited By 10},
author = {Li, Yang and Guzman, Emitza and Tsiamoura, Konstantina and Schneider, Florian and Bruegge, Bernd},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2015.05.326},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/li yang guzman Automated{\_}Requirements{\_}Extraction{\_}for{\_}Sc.pdf:pdf},
issn = {18770509},
keywords = {Natural language processing,Requirements engineering,Scientific software,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {1},
pages = {582--591},
title = {{Automated requirements extraction for scientific software}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939128546{\&}doi=10.1016{\%}2Fj.procs.2015.05.326{\&}partnerID=40{\&}md5=30f44debe5e2174cabad591d68d1b2b6},
volume = {51},
year = {2015}
}
@article{Morales-Ramirez2017159,
abstract = {Feedback about software applications and services that end-users express through web-based communication platforms represents an invaluable knowledge source for diverse software engineering tasks, including requirements elicitation. Research work on automated analysis of textual messages in app store reviews, open source software (OSS) mailing-lists and user forums has been rapidly increasing in the last five years. NLP techniques are applied to filter out irrelevant data, text mining and automated classification techniques are then used to classify messages into different categories, such as bug report and feature request. Our research focuses on online discussions that take place in user forums and OSS mailing-lists, and aims at providing automated analysis techniques to discover contained requirements. In this paper, we present a speech-acts based analysis technique, and experimentally evaluate it on a dataset taken from a widely used OSS project.},
annote = {cited By 8},
author = {Morales-Ramirez, Itzel and Kifetew, Fitsum Meshesha and Perini, Anna},
doi = {10.1007/978-3-319-59536-8_11},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Morales-Ramirez, Itzel and Kifetew, Fitsum Meshesha and Perini, Anna{\_} {\_}{\_}Analysis of online discussions in support of requirements discovery{\_}{\_} (2017).pdf:pdf},
isbn = {9783319595351},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automated classification techniques,Linguistic analysis,Requirements engineering,Sentiment analysis,Speech-acts,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {159--174},
title = {{Analysis of online discussions in support of requirements discovery}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021190655{\&}doi=10.1007{\%}2F978-3-319-59536-8{\_}11{\&}partnerID=40{\&}md5=cf477f416075f621335db51fa12b8782},
volume = {10253 LNCS},
year = {2017}
}
@article{Mohanan2016197,
abstract = {Software requirements are usually written in natural language or speech language which is asymmetric and irregular. This paper presents a suitable method for transforming user software requirement specifications (SRS) and business designs written in natural language into useful object oriented models. For sentence detection, tokenization, parts of speech tagging and parsing of requirement specifications we incorporate an open natural language processing (OpenNLP)tool. It provides very relevant parts of speech (POS) tags. This parts of speech tagging of the SRS is quite useful for further identification of object oriented elements like classes, objects, attributes, relationships etc. After obtaining the required and relative information, Semantic Business Vocabulary and Rules (SBVR) are applied to identify and to extract the object oriented elements from the requirement specification.},
annote = {cited By 3},
author = {Mohanan, Murali and Samuel, Philip},
doi = {10.1007/978-3-319-28031-8_17},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Mohanan, Murali and Samuel, Philip{\_} {\_}{\_}Software requirement elicitation using natural language processing{\_}{\_} (2016).pdf:pdf},
isbn = {9783319280301},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Class model generation,OpenNLP,Requirement elicitation,SBVR,Software requirement specification,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {197--208},
title = {{Software requirement elicitation using natural language processing}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951987701{\&}doi=10.1007{\%}2F978-3-319-28031-8{\_}17{\&}partnerID=40{\&}md5=5677c7d6ff1d92ddb12243e460678e0c},
volume = {424},
year = {2016}
}
@article{Bozyigit2019453,
abstract = {Software requirements include description of the features for the target system and express the expectations of users. In the analysis phase, requirements are transformed into easy-to-understand conceptual models that facilitate communication between stakeholders. Although creating conceptual models using requirements is mostly implemented manually by analysts, the number of models that automate this process has increased recently. Most of the models and tools are developed to analyze requirements in English, and there is no study for agglutinative languages such as Turkish or Finnish. In this study, we propose an automatic concept identification model that transforms Turkish requirements into Unified Modeling Language class diagrams to ease the work of individuals on the software team and reduce the cost of software projects. The proposed work is based on natural language processing techniques and a new rule-set containing twenty-six rules is created to find object-oriented design elements from requirements. Since there is no publicly available dataset on the online repositories, we have created a well-defined dataset containing twenty software requirements in Turkish and have made it publicly available on GitHub to be used by other researchers. We also propose a novel evaluation model based on an analytical hierarchy process that considers the experts' views and calculate the performance of the overall system as 89{\%}. We can state that this result is promising for future works in this domain.},
annote = {cited By 0},
author = {Bozyigit, Fatma and Aktaş, {\"{O}}zlem and Kilin{\c{c}}, Deniz},
doi = {10.3906/elk-1803-172},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bozyigit, Fatma and Aktaş, {\_}{\_}{\_}{\_}O{\_}{\_}zlem and Kilin{\_}{\_}c{\_}c{\_}{\_}, Deniz{\_} {\_}{\_}Automatic concept identification of software requirements in Turkish{\_}{\_} (2019).pdf:pdf},
issn = {13036203},
journal = {Turkish Journal of Electrical Engineering and Computer Sciences},
keywords = {Analytical hierarchy process-based evaluation,Class diagram,Conceptual model,Natural language processing,Rule-based model,Software requirements,Unified Modeling Language,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {1},
pages = {453--470},
title = {{Automatic concept identification of software requirements in Turkish}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062993311{\&}doi=10.3906{\%}2Felk-1803-172{\&}partnerID=40{\&}md5=e3e38654657ba44e0125f089b05bf31a},
volume = {27},
year = {2019}
}
@inproceedings{5636558,
abstract = {Natural language is the preferred form for writing use cases. While a few linguistic techniques exist that extract or validate structured information from unstructured natural language use case, they often cannot be extended beyond their primary language. Extending linguistic analysis and automated validation capabilities across multiple languages is necessary not only for widespread industrial adoption but it helps in analyzing a collection of multilingual use cases (quite frequent in multi-national projects) that need to be aggregated. We have published a UIMA (Unstructured Information Management Architecture) based linguistic engine for analyzing English use cases. In this paper, we report on extension of our linguistic technique to Japanese and effect of such an extension on the automated requirement validation suite. {\textcopyright} 2010 IEEE.},
author = {Sinha, Avik and Paradkar, Amit and Takeuchi, Hironori and Nakamura, Taiga},
booktitle = {Proceedings of the 2010 18th IEEE International Requirements Engineering Conference, RE2010},
doi = {10.1109/RE.2010.52},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sinha, Avik and Paradkar, Amit and Takeuchi, Hironori and Nakamura, Taiga{\_} {\_}{\_}Extending automated analysis of natural language use cases to other languages{\_}{\_} (2010).pdf:pdf},
isbn = {9780769541624},
issn = {2332-6441},
keywords = {extending,ieee{\_}inc{\_}nlp{\_}x{\_}re,linguistics,natural language processing},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {364--369},
title = {{Extending automated analysis of natural language use cases to other languages}},
year = {2010}
}
@article{Do201947,
abstract = {Increasingly competitive software industry, where multiple systems serve the same application domain and compete for customers, favors software with creative features. To promote software creativity, research has proposed multi-day workshops with experienced facilitators, and semi-automated tools to provide a limited support for creative thinking. Such approach is either time-consuming and demands substantial involvement from analysts with creative abilities, or useful only for existing large-scale software with a rich issue tracking system. In this paper, we present a novel framework, useful for both new and existing systems, providing an end-to-end automation to support creativity. In particular, the framework reuses freely available requirements for similar software, leverages state-of-the-art natural language processing and machine learning techniques, and generates candidate creative requirements. We apply the framework on three application domains: Antivirus, Web Browser, and File Sharing, and further report a human subject evaluation. The results demonstrate our framework's ability to generate creative features and provoke innovative thinking among developers with various experience levels.},
annote = {cited By 0},
author = {Do, Quoc Anh and Chekuri, Surendra Raju and Bhowmik, Tanmay},
doi = {10.1007/978-3-030-22888-0_4},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Do, Quoc Anh and Chekuri, Surendra Raju and Bhowmik, Tanmay{\_} {\_}{\_}Automated Support to Capture Creative Requirements via Requirements Reuse{\_}{\_} (2019).pdf:pdf},
isbn = {9783030228873},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Creativity,Language model,Natural language processing,Requirement boilerplate,Requirements engineering,Requirements reuse,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {47--63},
title = {{Automated Support to Capture Creative Requirements via Requirements Reuse}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068253503{\&}doi=10.1007{\%}2F978-3-030-22888-0{\_}4{\&}partnerID=40{\&}md5=5e740778bc69922370df468610d8a965},
volume = {11602 LNCS},
year = {2019}
}
@article{Ferrari201334,
abstract = {[Context and motivation] System requirements are normally provided in the form of natural language documents. Such documents need to be properly structured, in order to ease the overall uptake of the requirements by the readers of the document. A structure that allows a proper understanding of a requirements document shall satisfy two main quality attributes: (i) requirements relatedness: each requirement is conceptually connected with the requirements in the same section; (ii) sections independence: each section is conceptually separated from the others. [Question/Problem] Automatically identifying the parts of the document that lack requirements relatedness and sections independence may help improve the document structure. [Principal idea/results] To this end, we define a novel clustering algorithm named Sliding Head-Tail Component (S-HTC). The algorithm groups together similar requirements that are contiguous in the requirements document. We claim that such algorithm allows discovering the structure of the document in the way it is perceived by the reader. If the structure originally provided by the document does not match the structure discovered by the algorithm, hints are given to identify the parts of the document that lack requirements relatedness and sections independence. [Contribution] We evaluate the effectiveness of the algorithm with a pilot test on a requirements standard of the railway domain (583 requirements). {\textcopyright} 2013 Springer-Verlag.},
annote = {cited By 16},
author = {Ferrari, Alessio and Gnesi, Stefania and Tolomei, Gabriele},
doi = {10.1007/978-3-642-37422-7_3},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ferrari, Alessio and Gnesi, Stefania and Tolomei, Gabriele{\_} {\_}{\_}Using clustering to improve the structure of natural language requirements documents{\_}{\_} (2013).pdf:pdf},
isbn = {9783642374210},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Requirements analysis,lexical clustering,requirements documents structure,requirements quality,scopus{\_}inc{\_}nlp{\_}x{\_}re,similarity-based clustering,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {34--49},
title = {{Using clustering to improve the structure of natural language requirements documents}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875836582{\&}doi=10.1007{\%}2F978-3-642-37422-7{\_}3{\&}partnerID=40{\&}md5=6147d211ee2c4963a847499e6a81d814},
volume = {7830 LNCS},
year = {2013}
}
@article{Gupta201947,
abstract = {While Agile methodologies are used in software development, researchers have identified many issues related to requirements engineering in Agile approaches. Some of these issues relate to ambiguity in user stories, which is a widely-used requirements specification mechanism in Agile methodologies. This research proposes the use of conceptual models while developing user stories. We posit that the use of conceptual models helps reducing ambiguity in user stories. An important aspect of our research is the creation of an algorithm for automatic generation of such models while developing the user stories.},
annote = {cited By 0},
author = {Gupta, Abhimanyu and Poels, Geert and Bera, Palash},
doi = {10.1007/978-3-030-34146-6_5},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/CreationOfMultipleConceptualModelsFromUserStories-GuptaEtAl-MREBAworkshop@ERconference-2019.pdf:pdf},
isbn = {9783030341459},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Agile development,Behavior driven development,Conceptual models,Natural language processing,User stories,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {47--57},
title = {{Creation of multiple conceptual models from user stories – a natural language processing approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077693301{\&}doi=10.1007{\%}2F978-3-030-34146-6{\_}5{\&}partnerID=40{\&}md5=163bc78cb0fca3ee4ab1336d9edd72ea},
volume = {11787 LNCS},
year = {2019}
}
@article{sundaram_assessing_2010,
abstract = {The generation of traceability links or traceability matrices is vital to many software engineering activities. It is also person-power intensive, time-consuming, error-prone, and lacks tool support. The activities that require traceability information include, but are not limited to, risk analysis, impact analysis, criticality assessment, test coverage analysis, and verification and validation of software systems. Information Retrieval (IR) techniques have been shown to assist with the automated generation of traceability links by reducing the time it takes to generate the traceability mapping. Researchers have applied techniques such as Latent Semantic Indexing (LSI), vector space retrieval, and probabilistic IR and have enjoyed some success. This paper concentrates on examining issues not previously widely studied in the context of traceability: the importance of the vocabulary base used for tracing and the evaluation and assessment of traceability mappings and methods using secondary measures. We examine these areas and perform empirical studies to understand the importance of each to the traceability of software engineering artifacts. {\textcopyright} 2010 Springer-Verlag London Limited.},
author = {Sundaram, Senthil Karthikeyan and Hayes, Jane Huffman and Dekhtyar, Alex and Holbrook, E. Ashlee},
doi = {10.1007/s00766-009-0096-6},
file = {:C$\backslash$:/Users/aaberkan/Zotero/storage/L8M7DN89/Sundaram e.a. - 2010 - Assessing traceability of software engineering art.pdf:pdf},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Automated tracing,Candidate link generation,Requirements traceability,Secondary measures,Vocabulary base,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
month = {sep},
number = {3},
pages = {313--335},
title = {{Assessing traceability of software engineering artifacts}},
url = {http://link.springer.com/10.1007/s00766-009-0096-6},
volume = {15},
year = {2010}
}
@article{Misra2016183,
abstract = {Context: Terminological inconsistencies owing to errors in usage of terms in requirements specifications could result into subtle yet critical problems in interpreting and applying these specifications into various phases of SDLC. Objective: In this paper, we consider special class of terminological inconsistencies arising from term-aliasing, wherein multiple terms spread across a corpus of natural language text requirements may be referring to the same entity. Identification of such alias entity-terms is a difficult problem for manual analysis as well as for developing tool support. Method: We consider the case of syntactic as well as semantic aliasing and propose a systematic approach for identifying these. Identification of syntactic aliasing involves automated generation of patterns for identifying syntactic variances of terms including abbreviations and introduced-aliases. Identification of semantic aliasing involves extracting multidimensional features (linguistic, statistical, and locational) from given requirement text to estimate semantic relatedness among terms. Based upon the estimated relatedness and standard language database based refinement, clusters of potential semantic aliases are generated. Results of these analyses with user refinement lead to generation of entity-term alias glossary and unification of term usage across requirements. Results: A prototype tool was developed to assess the effectiveness of the proposed approach for an automated analysis of term-aliasing in the requirements given as plain English language text. Experimental results suggest that approach is effective in identifying syntactic as well as semantic aliases, however, when aiming for higher recall on larger corpus, user selection is necessary to eliminate false positives. Conclusion: This proposed approach reduces the time-consuming and error-prone task of identifying multiple terms which might be referring to the same entity to a process of tool assisted identification of such term-aliases.},
annote = {cited By 4},
author = {Misra, Janardan},
doi = {10.1016/j.infsof.2015.11.006},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Misra, Janardan{\_} {\_}{\_}Terminological inconsistency analysis of natural language requirements{\_}{\_} (2016).pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Alias identification,Entity disambiguation,Latent semantic analysis,Requirements analysis,Requirements management,Terminological inconsistency analysis,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {183--193},
title = {{Terminological inconsistency analysis of natural language requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949644579{\&}doi=10.1016{\%}2Fj.infsof.2015.11.006{\&}partnerID=40{\&}md5=38b53d9b7112eef973670df8688027f9},
volume = {74},
year = {2016}
}
@inproceedings{8880431,
abstract = {Modeling the system's specifications from the functionality perspective is an important step in analyzing the software requirements. UML use case diagram is one of the most used functional modeling techniques in the software development process. This paper provides an approach to generate the UML use case diagrams from the requirements text using natural language processing. The approach consists of several steps to process the requirements text. Starting with filtering the text from the mistakes and going through natural language processes till generating the use case diagrams. Experimental evaluations have been done on several public software projects to demonstrate the accuracy of the proposed approach.},
author = {Hamza, Zahra Abdulkarim and Hammad, Mustafa},
booktitle = {2019 8th International Conference on Modeling Simulation and Applied Optimization, ICMSAO 2019},
doi = {10.1109/ICMSAO.2019.8880431},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Hamza, Zahra Abdulkarim and Hammad, Mustafa{\_} {\_}{\_}Generating UML use case models from software requirements using natural language processing{\_}{\_} (2019).pdf:pdf},
isbn = {9781538676844},
issn = {2573-5276},
keywords = {Functional modeling,NLP,Requirements,Software engineering,UML,Use case,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
pages = {1--6},
title = {{Generating UML use case models from software requirements using natural language processing}},
year = {2019}
}
@article{guo_tackling_2017,
abstract = {Software systems operating in any type of safety or security critical domains must comply with an increasingly large and complex set of regulatory standards. Compliance is partially demonstrated through establishing trace links between requirements and regulatory codes. Such links can be constructed manually or through semi-automated techniques in which the text in the regulatory code is used to formulate an information retrieval query. However, trace retrieval solutions are not effective when significant vocabulary mismatches exist between regulatory codes and product level requirements. This paper describes and compares three query augmentation techniques for addressing the term mismatch problem and improving the quality of trace links generated between regulatory codes and requirements. The first trains a classifier to replace the original query with terms learned from a training set of regulation-to-requirements trace links. The second, replaces the original query with terms learned through web-mining; and the third utilizes a domain ontology to augment query terms. The ontology is constructed manually using a guided approach that leverages existing traceability knowledge. All three techniques were evaluated against security regulations from the USA government's Health Insurance Privacy and Portability Act (HIPAA) traced against ten healthcare related requirements specifications. The classification approach returned the best results; however, improvements were observed with both the classification and ontology based solutions. The web-mining technique showed improvements in only a subset of queries. The three query augmentation techniques offer tradeoffs in terms of performance, cost and effort, and usage viability within a specific project context},
author = {Guo, Jin and Gibiec, Marek and Cleland-Huang, Jane},
doi = {10.1007/s10664-016-9479-8},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Guo, Jin and Gibiec, Marek and Cleland-Huang, Jane{\_} {\_}{\_}Tackling the term-mismatch problem in automated trace retrieval{\_}{\_} (2017).pdf:pdf},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Query augmentation,Requirements engineering,Semantic traceability,Traceability,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {1103--1142},
title = {{Tackling the term-mismatch problem in automated trace retrieval}},
url = {http://link.springer.com/10.1007/s10664-016-9479-8},
volume = {22},
year = {2017}
}
@article{10.1145/2907942,
abstract = {Privacy policies describe high-level goals for corporate data practices; regulators require industries to make available conspicuous, accurate privacy policies to their customers. Consequently, software requirements must conform to those privacy policies. To help stakeholders extract privacy goals from policies, we introduce a semiautomated framework that combines crowdworker annotations, natural language typed dependency parses, and a reusable lexicon to improve goal-extraction coverage, precision, and recall. The framework evaluation consists of a five-policy corpus governing web and mobile information systems, yielding an average precision of 0.73 and recall of 0.83. The results show that no single framework element alone is sufficient to extract goals; however, the overall framework compensates for elemental limitations. Human annotators are highly adaptive at discovering annotations in new texts, but those annotations can be inconsistent and incomplete; dependency parsers lack sophisticated, tacit knowledge, but they can perform exhaustive text search for prospective requirements indicators; and while the lexicon may never completely saturate, the lexicon terms can be reliably used to improve recall. Lexical reuse reduces false negatives by 41{\%}, increasing the average recall to 0.85. Last, crowd workers were able to identify and remove false positives by around 80{\%}, which improves average precision to 0.93.},
address = {New York, NY, USA},
author = {Bhatia, Jaspreet and Breaux, Travis D. and Schaub, Florian},
doi = {10.1145/2907942},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Bhatia, Jaspreet and Breaux, Travis D. and Schaub, Florian{\_} {\_}{\_}Mining privacy goals from privacy policies using hybridized task recomposition{\_}{\_} (2016).pdf:pdf},
issn = {15577392},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {Crowdsourcing,Natural language processing,Privacy,Requirements extraction,acm{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
publisher = {Association for Computing Machinery},
title = {{Mining privacy goals from privacy policies using hybridized task recomposition}},
url = {https://doi.org/10.1145/2907942},
volume = {25},
year = {2016}
}
@inproceedings{8049171,
abstract = {In this paper, we take up the second RE17 data challenge: The identification of requirements types using the 'Quality attributes (NFR)' dataset provided. We studied how accurately we can automatically classify requirements as functional (FR) and non-functional (NFR) in the dataset with supervised machine learning. Furthermore, we assessed how accurately we can identify various types of NFRs, in particular usability, security, operational, and performance requirements. We developed and evaluated a supervised machine learning approach employing meta-data, lexical, and syntactical features. We employed under-and over-sampling strategies to handle the imbalanced classes in the dataset and cross-validated the classifiers using precision, recall, and F1 metrics in a series of experiments based on the Support Vector Machine classifier algorithm. We achieve a precision and recall up to {\~{}}92{\%} for automatically identifying FRs and NFRs. For the identification of specific NFRs, we achieve the highest precision and recall for security and performance NFRs with {\~{}}92{\%} precision and {\~{}}90{\%} recall. We discuss the most discriminating features of FRs and NFRs as well as the sampling strategies used with an additional dataset and their impact on the classification accuracy.},
author = {Kurtanovic, Zijad and Maalej, Walid},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.82},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kurtanovic, Zijad and Maalej, Walid{\_} {\_}{\_}Automatically Classifying Functional and Non-functional Requirements Using Supervised Machine Learning{\_}{\_} (2017).pdf:pdf},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {Classification,Imbalanced Data,Machine Learning,Requirements,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {490--495},
title = {{Automatically Classifying Functional and Non-functional Requirements Using Supervised Machine Learning}},
year = {2017}
}
@article{Knauss201439,
abstract = {Context and motivation: Requirements of todays industry specifications need to be categorized for multiple reasons, including analysis of certain requirement types (like non-functional requirements) and identification of dependencies among requirements.This is a pre-requisite for effective communication and prioritization of requirements in industry-size specifications. Question/problem: Because of the size and complexity of these specifications, categorization tasks must be specifically supported in order to minimize manual efforts and to ensure a high classification accuracy. Approaches that make use of (supervised) automatic classification algorithms have to deal with the problem to provide enough training data with excellent quality. Principal ideas/results: In this paper, we discuss the requirements engineering team and their requirements management tool as a socio-technical system that allows consistent classification of requirements with a focus on organizational learning. We compare a manual, a semi-automatic, and a fully-automatic approach for the classification of requirements in this environment. We evaluate performance of these approaches by measuring effort and accuracy of automatic classification recommendations and combined performance of user and tool, and capturing the opinion of the expert-participants in a questionnaire. Our results show that a semiautomatic approach is most promising, as it offers the best ratio of quality and effort and the best learning performance. Contribution: Our contribution is the definition of a socio-technical system for requirements classification and its evaluation in an industrial setting at Mercedes-Benz with a team of ten practitioners. {\textcopyright} 2014 Springer International Publishing Switzerland.},
annote = {cited By 7},
author = {Knauss, Eric and Ott, Daniel},
doi = {10.1007/978-3-319-05843-6_4},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Knauss, Eric and Ott, Daniel{\_} {\_}{\_}Semi- automatic categorization of natural language requirements{\_}{\_} (2014).pdf:pdf},
isbn = {9783319058429},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {categorization,classification,natural language,requirements,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {39--54},
title = {{Semi- automatic categorization of natural language requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958535855{\&}doi=10.1007{\%}2F978-3-319-05843-6{\_}4{\&}partnerID=40{\&}md5=727abbb8253efeb4e863449ead0d4b7d},
volume = {8396 LNCS},
year = {2014}
}
@conference{Alhoshan2019,
abstract = {In requirements engineering (RE), software requirements typically come in the form of unstructured text, written in natural language. Consequently, identifying related requirements becomes a time-consuming task. In this paper, we propose a novel method for measuring semantic relatedness between software requirements, with the aim to develop RE tools that can automate the process of detecting traceability links in requirements documents. The proposed method is underpinned by an embedding-based representation of semantic frames in FrameNet, trained on a large corpus of user requirements. Applying the method to the task of detecting semantically related software requirements, the performance of the proposed method was evaluated against a manually labelled corpus and baseline system. Our method obtained satisfactory performance in terms of F-score (86.36{\%}) against a manually labelled data set of software requirements, and outperformed the baseline system by 24 percentage points. These encouraging results demonstrate the potential of our method to be integrated with RE tools for facilitating software requirement analysis and traceability tasks.},
annote = {cited By 0},
author = {Alhoshan, Waad and Batista-Navarro, Riza and Zhao, Liping},
booktitle = {CEUR Workshop Proceedings},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Alhoshan, Waad and Batista-Navarro, Riza and Zhao, Liping{\_} {\_}{\_}Towards a corpus of requirements documents enriched with semantic frame annotations{\_}{\_} (2018).pdf:pdf},
issn = {16130073},
keywords = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
title = {{Using frame embeddings to identify semantically related software requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068085263{\&}partnerID=40{\&}md5=f69694214eacec0d57a8097e862dfe5f},
volume = {2376},
year = {2019}
}
@inproceedings{6602667,
abstract = {Software requirements are traditionally documented in natural language (NL). However, despite being easy to understand and having high expressivity, this approach often leads to well-known requirements quality problems. In turn, dealing with these problems warrants a significant amount of human effort, causing requirements development activities to be error-prone and time-consuming. This paper introduces RSL-PL, a language that enables the definition of linguistic patterns typically found in well-formed individual NL requirements, according to the field's best practices. The linguistic features encoded within RSL-PL patterns enable the usage of information extraction techniques to automatically perform the linguistic analysis of NL requirements. Thus, in this paper we argue that RSL-PL can improve the quality of requirements specifications, as well as the productivity of requirements engineers, by mitigating the continuous effort that is often required to ensure requirements quality criteria, such as clearness, consistency, and completeness. {\textcopyright} 2013 IEEE.},
author = {{De Almeida Ferreira}, David and {Da Silva}, Alberto Rodrigues},
booktitle = {2013 3rd International Workshop on Requirements Patterns, RePa 2013 - Proceedings},
doi = {10.1109/RePa.2013.6602667},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/{\_}De Almeida Ferreira{\_}, David and {\_}Da Silva{\_}, Alberto Rodrigues{\_} {\_}{\_}RSL-PL{\_} A linguistic pattern language for documenting software requirements{\_}{\_} (2013).pdf:pdf},
isbn = {9781479909483},
keywords = {Information Extraction,Linguistic Analysis,Requirements Engineering,Requirements Linguistic Patterns,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {17--24},
title = {{RSL-PL: A linguistic pattern language for documenting software requirements}},
year = {2013}
}
@incollection{king_requirements_2011-1,
abstract = {Natural language is the main presentation means in industrial requirements documents. In addition, communication between the different stakeholders is often insufficient, therefore requirements documents are frequently incomplete and inconsistent. This causes problems during modeling or programming. The aim of the presented paper is to make deficiencies in behavior specifications apparent in the early project stage. The basic idea is to model the required system behavior and to generate feedback for human analysts, based on the deficiencies of the resulting models. The presented feedback generation was evaluated in an experiment. It was found that it can address genuine problems of requirements documents. {\textcopyright} 2011 Springer-Verlag.},
address = {Cham},
annote = {Series Title: Notes on Numerical Fluid Mechanics and Multidisciplinary Design},
author = {Kof, Leonid and Penzenstadler, Birgit},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-21640-4_9},
editor = {King, Rudibert},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kof, Leonid and Penzenstadler, Birgit{\_} {\_}{\_}From requirements to models{\_} Feedback generation as a result of formalization{\_}{\_} (2011).pdf:pdf},
isbn = {9783642216398},
issn = {03029743},
keywords = {Feedback Generation,Model Extraction,Requirements Engineering,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {93--107},
publisher = {Springer International Publishing},
shorttitle = {From {\{}Requirements{\}} to {\{}Models{\}}},
title = {{From requirements to models: Feedback generation as a result of formalization}},
url = {http://link.springer.com/10.1007/978-3-642-21640-4{\_}9},
volume = {6741 LNCS},
year = {2011}
}
@inproceedings{10.5555/2337223.2337407,
abstract = {Online social networks are now common place in day-to-day lives. They are also increasingly used to drive social action initiatives, either led by government or communities themselves (e.g., SeeClickFix, LoveLewisham.org, mumsnet). However, such initiatives are mainly used for crowd sourcing community views or coordinating activities. With the changing global economic and political landscape, there is an ever pressing need to engage citizens on a large-scale, not only in consultations about systems that affect them, but also involve them directly in the design of these very systems. In this paper we present the UDesignIt platform that combines social media technologies with software engineering concepts to empower communities to discuss and extract high-level design features. It combines natural language processing, feature modelling and visual overlays in the form of "image clouds" to enable communities and software engineers alike to unlock the knowledge contained in the unstructured and unfiltered content of social media where people discuss social problems and their solutions. By automatically extracting key themes and presenting them in a structured and organised manner in near real-time, the approach drives a shift towards large-scale engagement of community stakeholders for system design. {\textcopyright} 2012 IEEE.},
author = {Greenwood, Phil and Rashid, Awais and Walkerdine, James},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2012.6227089},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Greenwood, Phil and Rashid, Awais and Walkerdine, James{\_} {\_}{\_}UDesignIt{\_} Towards social media for community-driven design{\_}{\_} (2012).pdf:pdf},
isbn = {9781467310673},
issn = {02705257},
keywords = {End-user software engineering,Humans and social aspects,Requirements engineering,Software design,acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
pages = {1321--1324},
publisher = {IEEE Press},
series = {ICSE '12},
title = {{UDesignIt: Towards social media for community-driven design}},
year = {2012}
}
@inproceedings{6912254,
abstract = {One of the surprising observations of traceability in practice is the under-utilization of existing trace links. Organizations often create links in order to meet compliance requirements, but then fail to capitalize on the potential benefits of those links to provide support for activities such as impact analysis, test regression selection, and coverage analysis. One of the major adoption barriers is caused by the lack of accessibility to the underlying trace data and the lack of skills many project stakeholders have for formulating complex trace queries. To address these challenges we introduce TiQi, a natural language approach, which allows users to write or speak trace queries in their own words. TiQi includes a vocabulary and associated grammar learned from analyzing NL queries collected from trace practitioners. It is evaluated against trace queries gathered from trace practitioners for two different project environments.},
author = {Pruski, Piotr and Lohar, Sugandha and Aquanette, Rundale and Ott, Greg and Amornborvornwong, Sorawit and Rasin, Alexander and Cleland-Huang, Jane},
booktitle = {2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings},
doi = {10.1109/RE.2014.6912254},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/PRUSKI{\~{}}1.PDF:PDF},
isbn = {9781479930333},
issn = {2332-6441},
keywords = {Natural Language Processing,Queries,Speech Recognition,Traceability,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {123--132},
title = {{TiQi: Towards natural language trace queries}},
year = {2014}
}
@inproceedings{7480109,
abstract = {One of the major problems in software development process is managing software artefacts. While software evolves, inconsistencies between the artefacts do evolve as well. To resolve the inconsistencies in change management, a tool named Software Artefacts Traceability Analyzer (SAT-Analyzer) was introduced as the previous work of this research. Changes in software artefacts in requirement specification, Unified Modelling Language (UML) diagrams and source codes can be tracked with the help of Natural Language Processing (NLP) by creating a structured format of those documents. Therefore, in this research we aim at adding an NLP support as an extension to SAT-Analyzer. Enhancing the traceability links created in the SAT-analyzer tool is another focus due to artefact inconsistencies. This paper includes the research methodology and relevant research carried out in applying NLP for improved traceability management. Tool evaluation with multiple scenarios resulted in average Precision 72.22{\%}, Recall 88.89{\%} and F1 measure of 78.89{\%} suggesting high accuracy for the domain.},
author = {Arunthavanathan, A. and Shanmugathasan, S. and Ratnavel, S. and Thiyagarajah, V. and Perera, I. and Meedeniya, D. and Balasubramaniam, D.},
booktitle = {2nd International Moratuwa Engineering Research Conference, MERCon 2016},
doi = {10.1109/MERCon.2016.7480109},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/ARUNTH{\~{}}1.PDF:PDF},
isbn = {9781509006458},
keywords = {Artefacts,Natural Language Processing,Taxonomy,Traceability Links,Traceability Visualization,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
pages = {18--23},
title = {{Support for traceability management of software artefacts using Natural Language Processing}},
year = {2016}
}
@inproceedings{7416336,
abstract = {The mobile applications industry is booming and growing rapidly. Mobile software companies and applications' stores are in a continuous need for developing new applications that meet and satisfy the users' requirements. Users' reviews posted in mobile applications' stores after installing the applications by the users can give useful information for developers; they contain good, bad, or recommended features. So the analysis of these reviews is important for the requirements engineering processes in the mobile applications industry. The use of users' feedback in finding software requirements is new in this research field. This paper is a review one, focuses on mobile applications users' reviews sentiment analysis (SA) in order to extract user requirements for building new applications or enhancing existing ones; i.e. requirements evolution. Users' reviews usually contain not only applications' features, but also users' feelings or sentiments about these features. We conducted an investigation of approaches used in the literature, during the period from 2009 to 2015 to answer three main research questions. Results show the significance of using SA for analyzing users' reviews and reports automated methods and tools for analyzing the reviews to features and their corresponding sentiments.},
author = {Rizk, Nancy M. and Ebada, Amr and Nasr, Eman S.},
booktitle = {2015 11th International Computer Engineering Conference: Today Information Society What's Next?, ICENCO 2015},
doi = {10.1109/ICENCO.2015.7416336},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Rizk, Nancy M. and Ebada, Amr and Nasr, Eman S.{\_} {\_}{\_}Investigating mobile applications' requirements evolution through sentiment analysis of users' reviews{\_}{\_} (2016).pdf:pdf},
isbn = {9781509002757},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,mobile applications,requirements evolution,sentiment analysis,users' requirements,users' reviews},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {dec},
pages = {123--130},
title = {{Investigating mobile applications' requirements evolution through sentiment analysis of users' reviews}},
year = {2016}
}
@inproceedings{6779538,
abstract = {Business Rule identification is an important task of Requirements Engineering process. However, the task is challenging as business rules are often not explicitly stated in the requirements documents. In case business rules are explicit, they may not be atomic in nature or, may be vague. In this paper, we present an approach for identifying business rules in the available requirements documentation. We first identify various business rules categories and, then examine requirements documentation (including requirements specifications, domain knowledge documents, change request, request for proposal) for the presence of these rules. Our study aims at finding how effectively business rules can be identified and classified into one of the categories of business rules using machine learning algorithms. We report on the results of the experiments performed. Our observations indicate that in terms of overall result, support vector machine algorithm performed better than other classifiers. Random Forest algorithm had a higher precision than support vector machine algorithm but relatively low recall. Na{\"{i}}ve Bayes algorithm had a higher recall than support vector machine. We also report on evaluation study of our requirements corpus using stop-words and stemming the requirements statements. {\textcopyright} 2014 IEEE.},
author = {Sharma, Richa and Bhatia, Jaspreet and Biswas, K. K.},
booktitle = {Souvenir of the 2014 IEEE International Advance Computing Conference, IACC 2014},
doi = {10.1109/IAdCC.2014.6779538},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Sharma, Richa and Bhatia, Jaspreet and Biswas, K. K.{\_} {\_}{\_}Automated identification of business rules in requirements documents{\_}{\_} (2014).pdf:pdf},
keywords = {Business Rules,Machine Learning,Requirements,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {feb},
pages = {1442--1447},
title = {{Automated identification of business rules in requirements documents}},
year = {2014}
}
@article{Mahmud2017332,
abstract = {Due to the increasing complexity of embedded systems, early detection of software/hardware errors has become desirable. In this context, effective yet flexible specification methods that support rigorous analysis of embedded systems requirements are needed. Current specification methods such as pattern-based, boilerplates normally lack meta-models for extensibility and flexibility. In contrast, formal specification languages, like temporal logic, Z, etc., enable rigorous analysis, however, they usually are too mathematical and difficult to comprehend by average software engineers. In this paper, we propose a specification representation of requirements, which considers thematic roles and domain knowledge, enabling deep semantic analysis. The specification is complemented by our constrained natural language specification framework, ReSA, which acts as the interface to the representation. The representation that we propose is encoded in description logic, which is a decidable and computationally-tractable ontology language. By employing the ontology reasoner, Hermit, we check for consistency and completeness of requirements. Moreover, we propose an automatic transformation of the ontology-based specifications into Timed Computation Tree Logic formulas, to be used further in model checking embedded systems.},
annote = {cited By 1},
author = {Mahmud, Nesredin and Seceleanu, Cristina and Ljungkrantz, Oscar},
doi = {10.1007/978-3-319-66197-1_21},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/MAHMUD{\~{}}1.PDF:PDF},
isbn = {9783319661964},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Description logic,Embedded systems,Event-based semantics,Ontology,Requirements analysis,Requirements specification,Thematic roles,Timed computation tree logic,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {332--348},
title = {{Specification and semantic analysis of embedded systems requirements: From description logic to temporal logic}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029003286{\&}doi=10.1007{\%}2F978-3-319-66197-1{\_}21{\&}partnerID=40{\&}md5=ce1549d67c17f3fe774869849a6566a0},
volume = {10469 LNCS},
year = {2017}
}
@inproceedings{10.1145/2652524.2652537,
abstract = {Context: Real-time speech translation technology is today available but still lacks a complete understanding of how such technology may affect communication in global software projects. Goal: To investigate the adoption of combining speech recognition and machine translation in order to overcome language barriers among stakeholders who are remotely negotiating software requirements. Method: We performed an empirical simulation-based study including: Google Web Speech API and Google Translate service, two groups of four subjects, speaking Italian and Brazilian Portuguese, and a test set of 60 technical and non-technical utterances. Results: Our findings revealed that, overall: (i) a satisfactory accuracy in terms of speech recognition was achieved, although significantly affected by speaker and utterance differences; (ii) adequate translations tend to follow accurate transcripts, meaning that speech recognition is the most critical part for speech translation technology. Conclusions: Results provide a positive albeit initial evidence towards the possibility to use speech translation technologies to help globally distributed team members to communicate in their native languages.},
address = {New York, NY, USA},
author = {Calefato, Fabio and Lanubile, Filippo and Prikladnicki, Rafael and Pinto, Jo{\~{a}}o Henrique S.},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1145/2652524.2652537},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/CALEFA{\~{}}1.PDF:PDF},
isbn = {9781450327749},
issn = {19493789},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,controlled experiment,global software engineering,machine translation,requirements meetings},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
publisher = {Association for Computing Machinery},
series = {ESEM '14},
title = {{An empirical simulation-based study of real-time speech translation for multilingual global project teams}},
url = {https://doi.org/10.1145/2652524.2652537},
year = {2014}
}
@incollection{moschitti_robust_2013,
abstract = {Requirement Analysis (RA) is a relevant application for Semantic Technologies focused on the extraction and exploitation of knowledge derived from technical documents. Language processing technologies are useful for the automatic extraction of concepts as well as norms (e.g. constraints on the use of devices) that play a key role in knowledge acquisition and design processes. A distributional method to train a kernel-based learning algorithm is here proposed, as a cost-effective approach for the validation stage in RA of Complex Systems, i.e. Naval Combat Systems. The targeted application of Requirement Identification and Information Extraction techniques is here discussed in the realm of robust search processes that allows to suitably locate software functionalities within large collections of requirements written in natural language. {\textcopyright} Springer-Verlag Berlin Heidelberg 2013.},
address = {Berlin, Heidelberg},
annote = {Series Title: Communications in Computer and Information Science},
author = {Garzoli, Francesco and Croce, Danilo and Nardini, Manuela and Ciambra, Francesco and Basili, Roberto},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-642-45260-4_4},
editor = {Moschitti, Alessandro and Plank, Barbara},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/garzoli Using{\_}Machine{\_}Learning{\_}and{\_}Information{\_}R.pdf:pdf},
isbn = {9783642452598},
issn = {18650929},
keywords = {springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {44--58},
publisher = {Springer Berlin Heidelberg},
title = {{Robust Requirements Analysis in Complex Systems through Machine Learning}},
url = {http://link.springer.com/10.1007/978-3-642-45260-4{\_}4},
volume = {379 CCIS},
year = {2013}
}
@misc{jordan2015machine,
abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
author = {Jordan, M I and Mitchell, T M},
booktitle = {Science},
doi = {10.1126/science.aaa8415},
file = {:C$\backslash$:/Users/aaberkan/Desktop/cheatsheets/255.full.pdf:pdf},
issn = {10959203},
number = {6245},
pages = {255--260},
pmid = {26185243},
title = {{Machine learning: Trends, perspectives, and prospects}},
url = {https://science.sciencemag.org/content/349/6245/255.abstract?casa{\_}token=ztR1{\_}HtmFv8AAAAA:oV7UFVY2rqAYcsBhIWiaABGCbfX4Sy{\_}K926mKDK-rj7WPf-kShTkcQj4gf{\_}vw7vhG1lgdLRUgoWG},
volume = {349},
year = {2015}
}
@book{recker2012scientific,
abstract = {Summary: This book is designed to introduce doctoral and other higher-degree research students to the process of scientific research in the fields of Information Systems as well as fields of Information Technology, Business Process Management and other related disciplines within the social sciences. It guides research students in their process of learning the life of a researcher. In doing so, it provides an understanding of the essential elements, concepts and challenges of the journey into research studies. It also provides a gateway for the student to inquire deeper about each element covered​. Comprehensive and broad but also succinct and compact, the book is focusing on the key principles and challenges for a novice doctoral student.},
address = {Berlin, Heidelberg},
author = {Recker, Jan},
booktitle = {Scientific Research in Information Systems},
doi = {10.1007/978-3-642-30048-6},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/Zelfontwikkeling/Boeken/Jan Recker/Scientific Research in Information S (22)/Scientific Research in Informat - Jan Recker.pdf:pdf},
isbn = {978-3-642-30047-9},
publisher = {Springer Berlin Heidelberg},
title = {{Scientific Research in Information Systems}},
url = {https://link.springer.com/978-3-642-30048-6 http://link.springer.com/10.1007/978-3-642-30048-6},
year = {2013}
}
@inproceedings{8933613,
abstract = {Sentiment analysis tools are becoming increasingly more prevalent in the software engineering research community. In this data showcase paper, we present a set of twenty-two software requirements specification (SRS) documents that have been preprocessed and subsequently analyzed using the Senti4SD sentiment analysis tool. As part of our preliminary research, we compared the result of the sentiment analysis of the SRS documents and other non-related documents and found that the SRS documents were 6{\%} more neutral than other non-related documents. Finally, we also present a number of research questions that we believe the research community might be able to answer using our published data.},
author = {Werner, Colin and Li, Ze Shi and Ernst, Neil},
booktitle = {Proceedings - 2019 IEEE 27th International Requirements Engineering Conference Workshops, REW 2019},
doi = {10.1109/REW.2019.00022},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Werner, Colin and Li, Ze Shi and Ernst, Neil{\_} {\_}{\_}What can the sentiment of a software requirements specification document tell us{\_}{\_}{\_} (2019).pdf:pdf},
isbn = {9781728151656},
keywords = {Emotion,Requirements engineering,Sentiment analysis,Software requirements specifications,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {106--107},
title = {{What can the sentiment of a software requirements specification document tell us?}},
year = {2019}
}
@inproceedings{ISI:000405493000005,
abstract = {Over 50 papers present natural language processing tools for improving the quality of requirements. However, few of these are adopted by industry. Even worse, most of them are no longer publicly available or supported by their creators. The few available and actively maintained tools exhibit some outstanding features, but also include sub-optimal functionalities. In this paper, we compare the performance of 3 existing tools on how well they automatically detect ambiguity and atomicity defects and deviations in 4 real-world natural language requirements sets. Next, we show how to design a superior tool by combining the best performing approaches of these three. Finally, we introduce a research roadmap toward automatically generating NLP RE tool mashups through the assembly of modular components taken from existing tools.},
address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
annote = {24th IEEE International Requirements Engineering Conference Workshops
(RWE), Beijing, PEOPLES R CHINA, SEP 12-16, 2016},
author = {Arendse, Brian and Lucassen, Garm},
booktitle = {Proceedings - 2016 IEEE 24th International Requirements Engineering Conference Workshops, REW 2016},
doi = {10.1109/REW.2016.44},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Arendse, Brian and Lucassen, Garm{\_} {\_}{\_}Toward Tool Mashups{\_} Comparing and Combining NLP RE Tools{\_}{\_} (2017).pdf:pdf},
isbn = {9781509036943},
keywords = {Mashups,Natural language processing,Quality of requirements,Requirements engineering,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
organization = {IEEE; IEEE Comp Soc; Intel; Huawei; CVIC SE; Minist Educ, Key Lab High Confidence Software Technologies; Springer; Int Requirements Engn Board; Peking Univ},
pages = {26--31},
publisher = {IEEE},
title = {{Toward tool mashups: Comparing and combining NLP RE tools}},
type = {Proceedings Paper},
year = {2017}
}
@article{8491267,
abstract = {When Considering What and how to reuse, one must understand the differences and similarities of the systems being developed; this activity is part of the domain analysis. Among the several ways to perform domain analysis, identifying equivalent requirements (that is, the common elements in the domain) is scalable and noninvasive, and it supports the consolidation of requirements.},
author = {Falessi, Davide and Cantone, Giovanni},
doi = {10.1109/MS.2018.2874620},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/falessi The Effort Savings from Using NLP to Classify Equivalent Requirements.pdf:pdf},
issn = {19374194},
journal = {IEEE Software},
keywords = {Empirical software engineering,NLP technique,Requirements consolidation,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {jan},
number = {1},
pages = {48--55},
title = {{The Effort Savings from Using NLP to Classify Equivalent Requirements}},
volume = {36},
year = {2019}
}
@inproceedings{8498203,
abstract = {CONTEXT: Architecture and design of systems are sensitive to non-functional requirements (NFRs). Identifying NFRs and their categories at early phase is an essential task for project success. Automatic classification methods for that purpose have been studied for supporting requirement analysis. The past studies used simple vectorization methods and might miss semantics and interactions among words in requirements. OBJECTIVE: To examine whether different vectorization methods lead to differences in the classification performance of NFRs and their categories. METHOD: Comparative experiments were conducted with open data. Five vectorization methods including document embedding methods and four supervised classification methods were supplied. RESULTS: Some advanced methods could achieve better performance than traditional ones. The preference was dependent on classification methods. CONCLUSIONS: It is beneficial to consider using advanced methods for classifying non-functional requirements categories.},
author = {Amasaki, Sousuke and Leelaprute, Pattara},
booktitle = {Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018},
doi = {10.1109/SEAA.2018.00036},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/amasaki The effects of vectorization methods on non-functional requirements classification.pdf:pdf},
isbn = {9781538673829},
keywords = {Comparative study,Requirements classification,Vectorization methods,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
month = {aug},
pages = {175--182},
title = {{The effects of vectorization methods on non-functional requirements classification}},
year = {2018}
}
@article{Tóth2019432,
abstract = {Software systems are to be developed based on expectations of the customers. These expectations are expressed using natural languages. To design software meeting the needs of the customer and the stakeholders, the intentions, feedback and reviews are to be understood accurately and without ambiguity. These textual inputs often contain inaccuracies, contradictions and are seldom given in a well-structured form. The issues mentioned in the previous thought frequently result in the program not satisfying the expectation of the stakeholders. In particular, for non-functional requirements, clients rarely emphasize these specifications as much as they might be justified. Identifying, classifying and reconciling the requirements is one of the main duty of the System Analyst, which without using a proper tool, can be very demanding and time-consuming. Tools which support text processing are expected to improve the accuracy of identification and classification of requirements even in an unstructured set of inputs. System Analysts can also use them in document archeology tasks where many documents, regulations, standards, etc. have to be processed. Methods elaborated in natural language processing and machine learning offer a solid basis. However, their usability and the possibility to improve the performance utilizing the specific knowledge from the domain of the software engineering are to be examined thoroughly. In this paper, we present the results of our work adapting natural language processing and machine learning methods for handling and transforming textual inputs of software development. The major contribution of our work is providing a comparison of the performance and applicability of the stateof-the-art techniques used in natural language processing and machine learning in software engineering. Based on the results of our experiments, tools which can support System Analysts working on textual inputs can be designed.},
annote = {cited By 0},
author = {T{\'{o}}th, L{\'{a}}szl{\'{o}} and Vid{\'{a}}cs, L{\'{a}}szl{\'{o}}},
doi = {10.5755/j01.itc.48.3.21973},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/T{\_}{\_}'{\_}o{\_}{\_}th, L{\_}{\_}'{\_}a{\_}{\_}szl{\_}{\_}'{\_}o{\_}{\_} and Vid{\_}{\_}'{\_}a{\_}{\_}cs, L{\_}{\_}'{\_}a{\_}{\_}szl{\_}{\_}'{\_}o{\_}{\_}{\_} {\_}{\_}Study of the performance of various classifiers in labeling non-functional requirements{\_}{\_} (2019).pdf:pdf},
issn = {2335884X},
journal = {Information Technology and Control},
keywords = {Feedback processing,Machine Learning,Natural Language Processing,Requirements Engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {432--445},
title = {{Study of the performance of various classifiers in labeling non-functional requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073571594{\&}doi=10.5755{\%}2Fj01.itc.48.3.21973{\&}partnerID=40{\&}md5=2e124b93021099de6750c6870f7cc05a},
volume = {48},
year = {2019}
}
@inproceedings{7765522,
abstract = {Communication about requirements is often handled in issue tracking systems, especially in a distributed setting. As issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify. This paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems. It compares traditional linguistic machine learning features, such as "bag of words", with more advanced features, such as subject-action-object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta-data. Our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system meta-data outperform traditional approaches. We show that issues or data fields (e.g. descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence. Finally, we show that the choice of machine learning algorithms should depend on the goal, e.g. maximization of the detection rate or balance between detection rate and precision. In addition, the paper contributes a double coded gold standard and an open-source implementation to further pursue this topic.},
author = {Merten, Thorsten and Falis, Mat{\'{u}}{\v{s}} and H{\"{u}}bner, Paul and Quirchmayr, Thomas and B{\"{u}}rsner, Simone and Paech, Barbara},
booktitle = {Proceedings - 2016 IEEE 24th International Requirements Engineering Conference, RE 2016},
doi = {10.1109/RE.2016.8},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/MERTEN{\~{}}1.PDF:PDF},
isbn = {9781509041213},
issn = {2332-6441},
keywords = {Machine Learning,Mining Software Repositories,Natural Language Processing,Software Feature Request Detection,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {166--175},
title = {{Software Feature Request Detection in Issue Tracking Systems}},
year = {2016}
}
@article{Femmer202087,
abstract = {Semantic similarity information supports requirements tracing and helps to reveal important requirements quality defects such as redundancies and inconsistencies. Previous work has applied semantic similarity algorithms to requirements, however, we do not know enough about the performance of machine learning and deep learning models in that context. Therefore, in this work we create the largest dataset for analyzing the similarity of requirements so far through the use of Amazon Mechanical Turk, a crowd-sourcing marketplace for micro-tasks. Based on this dataset, we investigate and compare different types of algorithms for estimating semantic similarities of requirements, covering both relatively simple bag-of-words and machine learning models. In our experiments, a model which relies on averaging trained word and character embeddings as well as an approach based on character sequence occurrences and overlaps achieve the best performances on our requirements dataset.},
annote = {cited By 0},
author = {Femmer, Henning and M{\"{u}}ller, Axel and Eder, Sebastian},
doi = {10.1007/978-3-030-35510-4_6},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Femmer, Henning and M{\_}{\_}{\_}{\_}u{\_}{\_}ller, Axel and Eder, Sebastian{\_} {\_}{\_}Semantic Similarities in Natural Language Requirements{\_}{\_} (2020).pdf:pdf},
isbn = {9783030355098},
issn = {18651356},
journal = {Lecture Notes in Business Information Processing},
keywords = {Machine learning,Requirements engineering,Similarity detection,scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re,springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {87--105},
title = {{Semantic Similarities in Natural Language Requirements}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078479877{\&}doi=10.1007{\%}2F978-3-030-35510-4{\_}6{\&}partnerID=40{\&}md5=7b2e0f3fff33af183287893b6d01bf74},
volume = {371 LNBIP},
year = {2020}
}
@article{Kamaruddin2019351,
abstract = {Requirements elicitation is an important task before any development of system repository can be conducted. Typically, traditional methods such as interview, questionnaire and observation are made to gauge the users' needs. However, the users may not be able to spell out specifically of their need especially if there is no available system to compare resulting to outrageous demands and unrealistic expectations to the repository developer. An alternative approach to gauge the user needs from users' reviews of the on-the-shelf software may be a good starting point. In this paper we attempt to extract requirements from the users' independent reviews gathered from the internet using text analytics approach. The keywords are visualized based on its relevance and importance to the user. Then, it is used as a benchmark for the user to alter to their specific repository needs. From the experimental results, it is observed that there are functions that are very much needed by the user and yet there are also functions that are not used at all. Hence, this proposed approach may give insight to the user and developer about the actual needs of the respective system. It is envisaged that such approach can be a guide to the novice user and the developer in order to shorten the time to agree on the development of the repository system.},
annote = {cited By 0},
author = {Kamaruddin, Norhaslinda and Wahab, Abdul and Bakri, Mohammad and Hamiz, Muhammad},
doi = {10.1007/978-981-15-0399-3_28},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Kamaruddin, Norhaslinda and Wahab, Abdul and Bakri, Mohammad and Hamiz, Muhammad{\_} {\_}{\_}Science Lab Repository Requirements Elicitation Based on Text Analytics{\_}{\_} (2019).pdf:pdf},
isbn = {9789811503986},
issn = {18650937},
journal = {Communications in Computer and Information Science},
keywords = {Business rules,Requirements elicitation,Text analytics,User review,Word cloud,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
pages = {351--360},
title = {{Science Lab Repository Requirements Elicitation Based on Text Analytics}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076115111{\&}doi=10.1007{\%}2F978-981-15-0399-3{\_}28{\&}partnerID=40{\&}md5=f223bc61dde0825423d69682034a6143},
volume = {1100},
year = {2019}
}
@inproceedings{6405278,
abstract = {Large organizations like Microsoft tend to rely on formal requirements documentation in order to specify and design the software products that they develop. These documents are meant to be tightly coupled with the actual implementation of the features they describe. In this paper we evaluate the value of high-level topic-based requirements traceability in the version control system, using Latent Dirichlet Allocation (LDA). We evaluate LDA topics on practitioners and check if the topics and trends extracted matches the perception that Program Managers and Developers have about the effort put into addressing certain topics. We found that effort extracted from version control that was relevant to a topic often matched the perception of the managers and developers of what occurred at the time. Furthermore we found evidence that many of the identified topics made sense to practitioners and matched their perception of what occurred. But for some topics, we found that practitioners had difficulty interpreting and labelling them. In summary, we investigate the high-level traceability of requirements topics to version control commits via topic analysis and validate with the actual stakeholders the relevance of these topics extracted from requirements. {\textcopyright} 2012 IEEE.},
author = {Hindle, Abram and Bird, Christian and Zimmermann, Thomas and Nagappan, Nachiappan},
booktitle = {IEEE International Conference on Software Maintenance, ICSM},
doi = {10.1109/ICSM.2012.6405278},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/HINDLE{\~{}}1.PDF:PDF},
isbn = {9781467323123},
issn = {1063-6773},
keywords = {ieee{\_}inc{\_}nlp{\_}x{\_}re,latent Dirichlet allocation (LDA),requirements,requirements engineering,topics,traceability,version control},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {243--252},
title = {{Relating requirements to implementation via topic analysis: Do topics extracted from requirements make sense to managers and developers?}},
year = {2012}
}
@inproceedings{10.1145/2491627.2491633,
abstract = {If traceability links between requirements and source code are not clarified when conducting maintenance and enhancements for the same series of software products, engineers cannot immediately find the correction location in the source code for requirement changes. However, manually recovering links in a large group of products requires significant costs and some links may be overlooked. Here, we propose a semi-automatic method to recover traceability links between requirements and source code in the same series of large software products. In order to support differences in representation between requirements and source code, we recover links by using the configuration management log as an intermediary. We refine the links by classifying requirements and code elements in terms of whether they are common or specific to the products. As a result of applying our method to real products that have 60KLOC, we have recovered valid traceability links within a reasonable amount of time. Automatic parts have taken 13 minutes 36 seconds, and non-automatic parts have taken about 3 hours, with a recall of 76.2{\%} and a precision of 94.1{\%}. Moreover, we recovered some links that were unknown to engineers. By recovering traceability links, software reusability will be improved, and software product line introduction will be facilitated. {\textcopyright} 2013 ACM.},
address = {New York, NY, USA},
author = {Tsuchiya, Ryosuke and Kato, Tadahisa and Washizaki, Hironori and Kawakami, Masumi and Fukazawa, Yoshiaki and Yoshimura, Kentaro},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2491627.2491633},
file = {:C$\backslash$:/Users/aaberkan/ONEDRI{\~{}}1/SMS-DA{\~{}}1/SMSART{\~{}}1/TSUCHI{\~{}}1.PDF:PDF},
isbn = {9781450319683},
keywords = {acm{\_}inc{\_}nlp{\_}x{\_}re,commonality and variability analysis,configuration management log,traceability recovery},
mendeley-tags = {acm{\_}inc{\_}nlp{\_}x{\_}re},
pages = {121--130},
publisher = {Association for Computing Machinery},
series = {SPLC '13},
title = {{Recovering traceability links between requirements and source code in the same series of software products}},
url = {https://doi.org/10.1145/2491627.2491633},
year = {2013}
}
@incollection{florez_recovering_2019,
abstract = {Software traceability is a necessary process to carry out source code maintenance, testing and feature location tasks. Despite its importance, it is not a process that is strictly conducted since the creation of every software project. Over the last few years information retrieval techniques have been proposed to recover traceability links between software artifacts in a coarse-grained and middle-grained level. In contexts where it is fundamental to ensure the correct implementation of regulations and constraints at source code level, as in the case of HIPAA, proposed techniques are not enough to find traceability links in a fine-granular way. In this research, we propose a fine-grained traceability algorithm to find traces between high level requirements written in human natural language with source code lines and structures where they are implemented.},
address = {Cham},
annote = {Series Title: Communications in Computer and Information Science},
author = {Velasco, Alejandro and {Aponte Melo}, Jairo Hernan},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-030-32475-9_37},
editor = {Florez, Hector and Leon, Marcelo and Diaz-Nafria, Jose Maria and Belli, Simone},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Velasco, Alejandro and {\_}Aponte Melo{\_}, Jairo Hernan{\_} {\_}{\_}Recovering fine grained traceability links between software mandatory constraints and source code{\_}{\_} (2019).pdf:pdf},
isbn = {9783030324742},
issn = {18650937},
keywords = {Healthcare,Information retrieval,Natural language processing,Program slicing,Software maintenance,Software traceability,Static code analysis,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {517--532},
publisher = {Springer International Publishing},
title = {{Recovering fine grained traceability links between software mandatory constraints and source code}},
url = {http://link.springer.com/10.1007/978-3-030-32475-9{\_}37},
volume = {1051 CCIS},
year = {2019}
}
@inproceedings{8049170,
abstract = {Since their introduction over a year ago, Google's TensorFlow package for learning with multilayer neural networks and their Word2Vec representation of words have both gained a high degree of notoriety. This paper considers the application of TensorFlow-guided learning and Word2Vec-based representations to the problems of classification in requirements documents. In this paper, we compare three categories of machine learning techniques for requirements identification for the SecReq and NFR datasets. The first category is the baseline method used in prior work: Na{\"{i}}ve Bayes over word count and TF-IDF representations of requirements. The remaining two categories of techniques are the training of TensorFlow's convolutional neural networks on random and pre-trained Word2Vec embeddings of the words found in the requirements. This paper reports on the experiments we conducted and the accuracy results we achieved.},
author = {Dekhtyar, Alex and Fong, Vivian},
booktitle = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
doi = {10.1109/RE.2017.26},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Dekhtyar, Alex and Fong, Vivian{\_} {\_}{\_}RE Data Challenge{\_} Requirements Identification with Word2Vec and TensorFlow{\_}{\_} (2017).pdf:pdf},
isbn = {9781538631911},
issn = {2332-6441},
keywords = {TensorFlow,Word2Vec,convolutional neural networks,ieee{\_}inc{\_}nlp{\_}x{\_}re,machine learning,requirements identification},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {484--489},
title = {{RE Data Challenge: Requirements Identification with Word2Vec and TensorFlow}},
year = {2017}
}
@inproceedings{8786652,
abstract = {Twitter is an important source for crowdsourcing software requirements and can be used as a communication channel between technical stakeholders and end users of software products. Previous studies have shown that software users share their opinions about the software through short messages called tweets. These tweets might contain worthwhile information (such as bug reports and user requests) which relates to the software requirements. Machine learning and Natural language processing approaches have been applied to elicit these technical tweets. Particularly, when learning techniques are considered, the success rate of the preprocessing process to overcome shortcomings in analyzing tweets becomes crucial. Therefore, in this paper, we propose a method which involves more than 6000 experiments in order to systematically analyze the effects of eleven preprocessing techniques on the classification performance of requirements-related tweets. We train and test three popular supervised learning algorithms on a dataset of 4000 tweets which is a sample of twitter feeds. Finally, we analyze our evaluation results from three different viewpoints to answer the research question. Our findings have demonstrated the crucial role of preprocessing, and also are feasible to provide more insights in designing approaches which can deal more accurately in case of elicitation such tweets.},
author = {Ebrahimi, Amir Mohammad and Barforoush, Ahmad Abdollahzadeh},
booktitle = {ICEE 2019 - 27th Iranian Conference on Electrical Engineering},
doi = {10.1109/IranianCEE.2019.8786652},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ebrahimi, Amir Mohammad and Barforoush, Ahmad Abdollahzadeh{\_} {\_}{\_}Preprocessing Role in Analyzing Tweets Towards Requirement Engineering{\_}{\_} (2019).pdf:pdf},
isbn = {9781728115085},
issn = {2642-9527},
keywords = {Crowd RE,Data-driven Requirements Engineering,NLP4RE,Requirement Elicitation,Software Evolution,Text Preprocessing,Twitter,User Feedback,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {NLP4RE,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
month = {apr},
pages = {1905--1911},
title = {{Preprocessing Role in Analyzing Tweets Towards Requirement Engineering}},
year = {2019}
}
@inproceedings{8920507,
abstract = {Using Machine Learning to solve requirements engineering problems can be a tricky task. Even though certain algorithms have exceptional performance, their recall is usually below 100{\%}. One key aspect in the implementation of machine learning tools is the balance between recall and precision. Tools that do not find all correct answers may be considered useless. However, some tasks are very complicated and even requirements engineers struggle to solve them perfectly. If a tool achieves performance comparable to a trained engineer while reducing her workload considerably, it is considered to be useful. One such task is the classification of specification content elements into requirements and non-requirements. In this paper, we analyze this specific requirements classification problem and assess the importance of recall by performing an empirical study. We compared two groups of students who performed this task with and without tool support, respectively. We use the results to compute an estimate of f for the Ff score, allowing us to choose the optimal balance between precision and recall. Furthermore, we use the results to assess the practical time savings realized by the approach. By using the tool, users may not be able to find all defects in a document, however, they will be able to find close to all of them in a fraction of the time necessary. This demonstrates the practical usefulness of our approach and machine learning tools in general.},
author = {Winkler, Jonas Paul and Gr{\"{o}}nberg, Jannis and Vogelsang, Andreas},
booktitle = {Proceedings of the IEEE International Conference on Requirements Engineering},
doi = {10.1109/RE.2019.00016},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Winkler, Jonas Paul and Gr{\_}{\_}{\_}{\_}o{\_}{\_}nberg, Jannis and Vogelsang, Andreas{\_} {\_}{\_}Optimizing for recall in automatic requirements classification{\_} An empirical study{\_}{\_} (2019).pdf:pdf},
isbn = {9781728139128},
issn = {23326441},
keywords = {Automation,Controlled-experiment,Empirical-research,Machine-learning,ieee{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re},
pages = {40--50},
title = {{Optimizing for recall in automatic requirements classification: An empirical study}},
volume = {2019-Septe},
year = {2019}
}
@incollection{hutchison_ontology-based_2012,
abstract = {The security requirements specification (SRS) is an integral aspect of the development of secured information systems and entails the formal documentation of the security needs of a system in a correct and consistent way. However, in many cases there is lack of sufficiently experienced security experts or security requirements (SR) engineer within an organization, which limits the quality of SR that are specified. This paper presents an approach that leverages ontologies and requirements boilerplates in order to alleviate the effect of lack of highly experienced personnel for SRS. It also offers a credible starting point for the SRS process. A preliminary evaluation of the tool prototype - ReqSec tool - was used to demonstrate the approach and to confirm its usability to support the SRS process. The tool helps to reduce the amount of effort required, stimulate discovery of latent security threats, and enables the specification of good quality SR. {\textcopyright} 2012 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {Series Title: Lecture Notes in Computer Science},
author = {Daramola, Olawande and Sindre, Guttorm and Moser, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33618-8_28},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard and Herrero, Pilar and Panetto, Herv{\'{e}} and Meersman, Robert and Dillon, Tharam},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/daramola Ontology-based support for security requirements specification process.pdf:pdf},
isbn = {9783642336171},
issn = {03029743},
keywords = {information extraction,ontology,requirement boilerplates,security requirements,security threats,springer{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {springer{\_}inc{\_}nlp{\_}x{\_}re},
pages = {194--206},
publisher = {Springer Berlin Heidelberg},
title = {{Ontology-based support for security requirements specification process}},
url = {http://link.springer.com/10.1007/978-3-642-33618-8{\_}28},
volume = {7567 LNCS},
year = {2012}
}
@article{Mahmoud2015281,
abstract = {In this paper, we investigate the potential benefits of utilizing natural language semantics in automated traceability link retrieval. In particular,we evaluate the performance of a wide spectrum of semantically enabled information retrieval methods in capturing and presenting requirements traceability links in software systems. Our objectives are to gain more operational insights into these methods and to provide practical guidelines for the design and development of effective requirements tracing and management tools. To achieve our research objectives, we conduct an experimental analysis using three datasets from various application domains. Results show that considering more semantic relations in traceability link retrieval does not necessarily lead to higher quality results. Instead, a more focused semantic support, that targets specific semantic relations, is expected to have a greater impact on the overall performance of tracing tools. In addition, our analysis shows that explicit semantic methods, that exploit local or domain-specific sources of knowledge, often achieve a more satisfactory performance than latent methods, or methods that derive semantics from external or general-purpose knowledge sources.},
annote = {cited By 32},
author = {Mahmoud, Anas and Niu, Nan},
doi = {10.1007/s00766-013-0199-y},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Mahmoud, Anas and Niu, Nan{\_} {\_}{\_}On the role of semantics in automated requirements tracing{\_}{\_} (2015).pdf:pdf},
issn = {1432010X},
journal = {Requirements Engineering},
keywords = {Information retrieval,Semantics,Traceability,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {scopus{\_}inc{\_}nlp{\_}x{\_}re},
number = {3},
pages = {281--300},
title = {{On the role of semantics in automated requirements tracing}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943348564{\&}doi=10.1007{\%}2Fs00766-013-0199-y{\&}partnerID=40{\&}md5=9aa1fb7bafe7d789f03eefe8e7eab0f9},
volume = {20},
year = {2015}
}
@article{8106888,
abstract = {Natural language processing (NLP) and requirements engineering (RE) have had a long relationship, yet their combined use isn't well established in industrial practice. This situation should soon change. The future evolution of the application of NLP technologies in RE can be viewed from four dimensions: discipline, dynamism, domain knowledge, and datasets.},
author = {Ferrari, Alessio and Dellorletta, Felice and Esuli, Andrea and Gervasi, Vincenzo and Gnesi, Stefania},
doi = {10.1109/MS.2017.4121207},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/Ferrari, Alessio and Dellorletta, Felice and Esuli, Andrea and Gervasi, Vincenzo and Gnesi, Stefania{\_} {\_}{\_}Natural language requirements processing{\_} A 4D vision{\_}{\_} (2017).pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {NLP,Natural language processing,RE,Requirements engineering,Requirements specification,Software development,Software engineering,ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re},
month = {nov},
number = {6},
pages = {28--35},
title = {{Natural language requirements processing: A 4D vision}},
volume = {34},
year = {2017}
}
@article{dalpiaz2018natural,
abstract = {As part of the growing interest in natural language processing for requirements engineering (RE), RE researchers, computational linguists, and industry practitioners met at the First Workshop on Natural Language Processing for Requirements Engineering (NLP4RE 18). This article summarizes the workshop and presents an overview of the discussion held on the field's future. This article is part of a theme issue on software engineering's 50th anniversary.},
annote = {workshop summarisation},
author = {Dalpiaz, Fabiano and Ferrari, Alessio and Franch, Xavier and Palomares, Cristina},
doi = {10.1109/MS.2018.3571242},
file = {:C$\backslash$:/Users/aaberkan/OneDrive - UGent/SMS - data extractie/SMS Artikelen/dalpiaz Natural Language Processing for Requirements Engineering The Best Is Yet to Come.pdf:pdf},
issn = {19374194},
journal = {IEEE Software},
keywords = {NLP,NLP4RE,ieee{\_}inc{\_}nlp{\_}x{\_}re,natural language processing,requirements engineering,scopus{\_}inc{\_}nlp{\_}x{\_}re,software development,software engineering,software requirements,wos{\_}inc{\_}nlp{\_}x{\_}re},
mendeley-tags = {ieee{\_}inc{\_}nlp{\_}x{\_}re,scopus{\_}inc{\_}nlp{\_}x{\_}re,wos{\_}inc{\_}nlp{\_}x{\_}re},
number = {5},
pages = {115--119},
title = {{Natural Language Processing for Requirements Engineering: The Best Is Yet to Come}},
volume = {35},
year = {2018}
}
